%\documentclass{acm-book-v2}
%\RequirePackage[errorshow]{tracefnt}
%%\newcommand{\mpage}[1]{}
%%\newcommand{\indexfn}[1]{}

%%\usepackage{showframe}

%\usepackage{custom-tooltip}
%\usepackage{custom-tooltip-Alt-Text-View}

%\begin{document}

\setcounter{chapter}{6}

\chapter{\label{chap:7}Algorithm Biases and Values}

\noindent An algorithm is coded not only to solve a problem but also to automate a solution to a given problem. In implementing a solution procedure, each algorithm brings certain values and ideals into play. Those values and ideals may influence human beings and the environment every time algorithms run on a computing device. Algorithm objectivity is an open issue today; in fact, algorithms often reflect the values of the designers and programmers. Algorithms that take in large datasets as input also depend on the data they process. For example, the behavior and results of machine learning and data analytics algorithms often depend on the training data they ingest. This chapter addresses the issues of algorithmic biases and ethics principles in software programming and discusses human value alignment of artificial intelligence algorithms that frequently run with increasing autonomy and proficiency in many real-world domains.

\section{\label{sec:7.1}Human Values in Automatic Procedures}

An algorithm coded in a programming language embodies an automated solution to a given problem. Together with the responsible use of algorithms, we must consider the design and implementation of new algorithms that will respect the values and rights of individuals, groups, and, more generally, of the world in which we live. Algorithms should not harm individuals or social groups. They should not promote criminal enterprises or other groups that develop coded procedures to impose their own interests and pursue malevolent goals. Algorithm development can foster many risks that over time could threaten fundamental human rights such as the right to be informed, the right to consent to the use of our own data, the right to preserve our privacy, the rights of the vulnerable and disadvantaged communities, and the right to be forgotten from judicial precedents. In the first decades after electronic computers were built, algorithms and software programs were developed as computation tools to support other scientific disciplines that had a strong need to process very complex operations in a short time. Nowadays, computers and software systems are taking on the (largely unsolicited) task of solving every problem and being used in every human process. They are gaining the right to virtualize the world and replace all human operational processes.

As Cathy \citet{chap:7:ONeil:2016} explained in \textit{Weapons of Math Destruction}, algorithms are no longer pure mathematics, software programs are not neutral mathematical tools. They have become human opinions embedded in formal abstract languages and for this reason they must be studied and used with due care. Algorithms \hbox{cannot} be considered \textit{a priori} as precise, objective, and impartial procedures. In real life, most of the algorithms running in our digital devices increasingly incorporate goals and ideologies, without making them explicit or even visible. They hide ideas and cultural visions behind the apparent objectivity of the logical--mathematical formalism of programming languages. In her recent book, Meredith \citeauthor{chap:7:Broussard:2023} \mbox{[\citeyear{chap:7:Broussard:2023}]} argues that neutrality in technology is not guaranteed, biases can be embedded in software systems, and to avoid these risks, algorithms need to be held accountable. These ideological points of view, in the most general sense that can be given to this term, often are present, explicitly or tacitly, in the instructions that compose the execution flow of algorithms. They are inside software applications that we use often but which, in most cases, we cannot know in detail because we are not able to analyze or understand their code. In many cases, therefore, algorithms implement the opinions, viewpoints, or economic/political interests of those who design and implement them. The ideas, cultural values, and thoughts expressed by software designers sometimes define and condition the present and future of billions of people who use digital systems.

The massive use of data and the pervasive use of algorithms that is today being made in all fields of human life have created many cases where algorithms are used to make decisions and to tackle problems that have moral implications and influence human values. For instance, prediction algorithms used to analyze the health data of millions of patients in many countries may aggravate existing problems, with groups of patients (e.g., rich or white people) given measurably better care than other patients (e.g., Black or poor people). Another significant example is the case where an autonomous car is faced with the choice of having to hit a pedestrian or risk the lives of its passengers. The choice will be left to the algorithms that manage the car and guide its steering wheel. Web search engines are not fully neutral as they collect and analyze very large sets of data and prioritize web pages with the most links, which rely on user preferences and connections. Hence, a web search engine can become an echo chamber that supports biases of the real world and further embeds these prejudices and stereotypes online. \mbox{\citeauthor{chap:7:LambrechtandTucker:2019}} \mbox{[\citeyear{chap:7:LambrechtandTucker:2019}]} published in \textit{Management Science} the results of a real-world experiment by designing a gender-neutral science, technology,\vadjust{\vspace*{-18pt}\pagebreak} engineering, and \hbox{mathematics} (STEM) career advertisement paid to run on Facebook, Instagram, and Twitter as well as through Google's engine for distributing advertisements across different web sites. The social and web platforms' algorithms optimized the advertisement so the most people would see it. As a result of that optimization, men saw the advertisement 20\% more often than women did. Women see fewer advertisements about entering into the science and technology professions than men do because the marketing algorithms set higher prices for their views. An algorithm designed to optimize cost-effectiveness in announcement delivery delivers ads that were intended to be gender neutral in an apparently discriminatory way because of unfair distribution. Lambrecht and Tucker in their work show that this empirical regularity extends to other major digital platforms.

The case we mentioned and other similar cases call for high responsibility from designers and programmers who develop intelligent software systems impacting human ethics and values. These kinds of issues are linked to what is called the ethics of AI. The use of progressively intelligent algorithms, software systems, and networks is changing the frontiers of human that is, they are redefining the areas of actions of people, because they perform many actions/operations/tasks that previously were executed by humans. A paper by \citet{chap:7:BostromandYudkowsky:2014} discusses the issues and challenges of assuring that AI systems operate judiciously as they interact and influence humans through their digital ``acumen.'' Machine learning algorithms and applications must be transparent to inspection, be predictable to those they govern, be robust against manipulation, hold a moral status, and be non-discriminatory (as discussed, e.g., by Ruha \citeauthor{chap:7:Benjamin:2019} \mbox{[\citeyear{chap:7:Benjamin:2019}]} in her book and by several contributors in their essays included in the book \textit{Your Computer Is on Fire} [\citealt{chap:7:Mullaneyetal:2021}]). In summary, they must embody a moral behavior that corresponds to our civilization's ethics. \hbox{Computer} scientists, software designers, and computer programmers must operate toward these main goals by implementing ``new kinds of safety assurance and the engineering of artificial ethical considerations.''

\section{\label{sec:7.2}Human Biases and Algorithms}

The \textit{Encyclopedia Britannica} defines bias as a ``tendency to believe that some \hbox{people,} ideas, etc., are better than others that usually results in treating some people unfairly.'' The \textit{Cambridge Dictionary} definition of bias is ``an unfair personal opinion that influences your judgment.'' These definitions directly connect biases with unfairness. A bias is a human tendency to prefer one person or thing to another, and to favor that person or thing in a way that is unfair or prejudicial. Bias is an unavoidable component of human nature. Human biases are influenced by your environment, family, work, and experience. These can be hard to eradicate. One such solution to minimize the effects of biases is to be aware\vadjust{\vspace*{-18pt}\pagebreak} of possible biases that you have or may face. Among human biases, cognitive biases are systematic deviations from fair or optimal reasoning. They are recurring errors in thinking or patterns of bad judgment occurring in different individuals or contexts. They are the ways a person understands people, events, and facts that are founded on their own particular set of beliefs and experiences and may not be reasonable or accurate. Cognitive biases help humans find mental shortcuts to assist them in their daily lives; however, they can cause irrational interpretations and judgments. According to the \href{https://www.sog.unc.edu/sites/www.sog.unc.edu/files/course\_materials/Cognitive\%20Biases\%20Codex.pdf}{Cognitive~Bias~Codex}, created by John Manoogian III and Buster Benson, there are about 180 cognitive biases. The biases in the Codex are arranged in a circle and are divided into four quadrants. Each quadrant is dedicated to a specific group of cognitive biases:

\def\labelenumi{(\arabic{enumi})}
\begin{enumerate}
\item biases that affect our memory for people, events, and information;
\item biases that affect how we perceive certain events and people;
\item biases that we use when we have too little information and need to fill in the gaps; and
\item biases that affect how we make decisions.
\end{enumerate}

Examples of cognitive biases are confirmation bias, based on looking for or overvaluing information that confirms our beliefs or expectations; gambler's \hbox{fallacy,} a false belief that describes our tendency to believe that something will happen because it hasn't happened yet; and gender bias, the tendency to assign specific behavior and characteristics to a particular gender without supporting evidence. In science and engineering, a bias is a systematic error that may have different origins. For example, in the context of scientific research, a preference for a certain idea (e.g., an approach or a hypothesis) can deviate from fair investigation, truth, or be unjustified by the evidence. For instance, statistical bias results from an unfair sampling of a population or from an estimation process that does not give accurate results on average. Also, researcher's values can become sources of biases. As personal values generally originate corresponding motivations, they may influence a researcher's decisions made during scientific studies. For example, a researcher's values and goals can influence choices about how a hypothesis will be assessed, results are evaluated, and evidence is judged.

Digital technologies reproduce these ways of acting and use the knowledge of engineers, teachers, doctors, drivers, and accountants. As a result, AI systems and machine learning algorithms replicate the biases of the humans who make them and of the humans whose knowledge they are fed. Humans have inclinations and feelings and are not perfect beings. These conditions can be reflected in the computer systems they develop. Designers and programmers must take this possibility into account and must act to avoid or at least to limit the capacity of algorithms to recreate contain personal inclinations and prejudices and produce and automate severe and systematic errors.

\section{\label{sec:7.3}Algorithmic Biases}

In relation to computer science and algorithms, we can define algorithmic bias as a systematic and repeatable error in a computer program, such as a web search engine or a social media news feed, that creates unfair behaviors and results that affect people or events, such as favoring one group over another in ways different from the intended original function of the algorithm. An algorithmic bias can do this even without any programmer intention to discriminate. For example, a health insurance pricing and premium setting algorithm may determine the price of life insurance without being unfair if it is consistently weighing relevant criteria. But if the algorithm sets high prices for one group of clients on the basis of their race or gender but establishes lower prices for another group of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple instances, that algorithm can be designated as biased.

A well-known example of algorithmic bias resulted in more than 50 females and ethnic minorities being denied entry to St. George's Hospital Medical School in South London from 1982 to 1986, based on the application of a new software-based assessment system that denied entry to women and men with non-European ``foreign-sounding names'' based on historical trends in admissions. For those years, the Medical School was found guilty by the Commission for Racial Equality of performing racial and sexual discrimination in its admissions policy. In the software program, details for each candidate were obtained from their admission form; but as this contained no reference to race, this was deduced from the surname and place of birth. The algorithm used that information to generate a score that was used to decide which applicants should be interviewed. Women and candidates from racial minorities had a reduced chance of being interviewed, independent of academic considerations. It is worth mentioning that the program was implemented after a careful analysis of the way in which the Medical School staff were making candidate choices and was continually improved until by 1979 it was \hbox{giving} a 90--95\% correlation with the gradings of the selection panel. This argument is central because the program was not introducing new bias but merely reflecting the selection procedure. Hence this is a case where human biases were transferred to algorithms.

More recently, several similar cases have occurred, such as a Facebook algorithm that displayed gender bias in job advertisements. A study by researchers from the University of Southern California discovered that the Facebook job advert \hbox{algorithm} would promote more technical jobs to men than women. Similarly, in 2015 Amazon realized that their machine learning algorithm used for hiring employees was found to be biased against women. The algorithm was mainly biased because it was trained on the CVs submitted over the past ten years. Since most of the applicants were men, it observed more male applications in a ten-year period and favored applications from men over women. Amazon programmers coded the algorithm to make it neutral toward those particular features. But that was no guarantee that the execution of the algorithm would not find other ways of sorting candidates that could prove discriminatory. This last case is a type of bias that originates from data used by algorithms to learn patterns. Because of the inherent presence of biases within humans, biases are also reflected in the data we use in our decision processes, that is, what we feed our computers. With the increasing availability and use of Big Data and machine learning algorithms, there are many ways that biases influence our automated induction processes and produce machine bias. We must also consider that computer scientists and programmers may have cognitive bias when designing, experimenting with, and implementing algorithms. Simple steps such as not considering certain \hbox{parameters} or features when testing can lead to negative effects and biased results. In these cases, machine bias has real-world consequences that generate problems and reinforces systematic bias.

Some may argue that algorithmic choices and decisions, as opposed to human judgements and conclusions, can be considered as \textit{less} biased without human irrationality, potential discrimination, or failings. However, when algorithms embody human biases, they replicate the same problems. Often, algorithms are seen as limiting human intervention and impact; therefore, they are credited with greater rational rather than emotional abilities. Unfortunately, this is not always true, and they must be very carefully designed to demonstrate that they are more successful than humans in objective tasks than in subjective ones. Sometimes, it may be easier to fix biased algorithms than biased humans. However, algorithms are used every day in all human activities, so the impact of biased software can be immense. Moreover, biased algorithms represent an automated bias that can be repeated every time an algorithm is executed. While a human bias may be fixed each time a decision is made, an algorithmic bias can be repeated many thousands of times before it is identified and the algorithm is modified. In the next sections, we will discuss regulations and laws that impact algorithmic bias and ethics. Here it is implicit that legislators should work to ensure that algorithms used by humans to make important decisions are doing so in a fair and impartial way. As discussed in the literature, to reach this goal some key steps are required. First, regulators must define bias and its real-world effects. Second, once the goals are clearly defined, regulators must use them to provide controls for the software industry. Third, they should require internal accountability offices, documentation protocols, and other needed actions that can prevent or solve algorithmic biases.

However, reducing or eliminating algorithmic bias can be done in different ways; to achieve this goal one key is code transparency. This requires algorithm openness and transparency by software developers, tech companies, public agencies, and other agents involved in the design, implementation, and use of software systems. Industry rights and patents cannot be used as a justification to avoid software transparency and support automated bias. Algorithms must be fair and safe like many other products we use every day. Regulators must act to safeguard users from privacy or human rights threats and avoid unethical bias and discriminatory or unfair code that harms individuals and groups.

\section{\label{sec:7.4}Toward Responsible Algorithms}

Individuals, companies, and public administrations use algorithms with increasing frequency to solve complex tasks and make important decisions. In many cases, algorithms can make their choices without human intervention (even by the person who coded them) while they do not provide us humans with a clear explanation of those choices. The recent developments of AI algorithms based on machine learning strategies indicate that a part of the mediation and decision-making power is being transferred from humans to intelligent machines. This process, which has been undergoing enormous acceleration in recent years, calls for the need to have algorithms be designed with extreme care and that humans be placed at the center. Individuals must always be able to verify their functioning, evaluate their objectivity and fairness, and weigh the accuracy and impartiality of the data they use to make decisions. We need digital technologies that allow human beings to control the logic of machines so as not to end up being controlled by them. In summary, we need to have a sort of ``anthropocentric artificial \hbox{intelligence.''} This is not an easy challenge to overcome because digital acceleration is putting culture and human values into play. Intelligent machines are decisively challenging these essential elements. For these reasons, our response to this challenge will not only condition the world of science and technology but will define the role of human beings in the new millennium.

The responsibility for an algorithm belongs to those who design and use it. Computer scientists, designers, and programmers have the responsibility to develop trustworthy systems, and users have the responsibility to employ them fairly. The ACM Technology Policy Council Statement on Principles for Responsible Algorithmic Systems, compiled\vadjust{\pagebreak} in 2022 [\citealt{chap:7:ACMTechnologyPolicyCouncil:2022}], proposes nine instrumental principles to promote fair, accurate, and beneficial algorithmic decision-making. They include designers' legitimacy and competency, minimizing harm originating from errors or biases, security and privacy practices, data and code transparency, algorithm interpretability and explainability, software maintainability, contestability and auditability, author accountability and responsibility, and limiting environmental impact. In particular, with regard to transparency, the ACM Policy Council asks developers ``to clearly document the way in which specific datasets, variables, and models were selected for development, training, validation, and testing, as well as the specific measures that were used to guarantee data and output quality. Systems should indicate their level of confidence in each output and humans should intervene when confidence is low. Developers also should document the approaches that were used to explore for potential biases.'' Regarding responsibility, the Council recommends that public and private bodies ``should be responsible for entire systems as deployed in their specific contexts, not just for the individual parts that make up a given system. When problems in automated systems are detected, organizations responsible for deploying those systems should document the specific actions that they will take to remediate the problem and under what circumstances the use of such technologies should be suspended or terminated.''

To enforce responsibility in developing and using AI algorithms, in 2023 the European Commission adopted the Artificial Intelligence Act [\citealt{chap:7:EuropeanCommission:2023}]. It is a world-first law to regulate AI systems by differentiating requirements by risk level, introducing prohibitions for harmful AI practices, and enabling societal scrutiny of systems. The AI Act is complex, and it is not easy to recap it \hbox{easily.} However, we can describe its principles and objectives. The Act is a regulatory framework with four main goals:

\begin{itemize}
\item Guarantee that intelligent hardware/software systems placed and used are safe and respect existing law on fundamental rights and EU values.
\item Ensure legal certainty to facilitate AI investments and innovation.
\item Improve governance and enforcement of existing law on fundamental rights and safety requirements applicable to intelligent systems.
\item Simplify the development of a single market for lawful, safe, and trustworthy AI applications and prevent market fragmentation.
\end{itemize}

The proposal lays down a methodology to define high-risk machine intelligence systems that create risks to the health and safety or fundamental rights of persons. As explained in paragraph 33, ``Technical inaccuracies of AI systems\vadjust{\vfill\pagebreak} intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects. This is particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, `real-time' and `post' remote \hbox{biometric} identification systems should be classified as high-risk. In view of the risks that they pose, both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight.'' Expected, proportionate, and well-defined obligations are placed on AI providers and users to ensure protection and respect for existing legislation guarding fundamental rights.

The Act recognizes that the use of AI systems with some particular characteristics such as complexity, opacity, data dependency, and autonomous behavior can adversely affect a number of fundamental rights. Introducing a set of requirements for trustworthy intelligent software systems and fair obligations on all value chain participants, the AI Act aims to enhance and promote the protection of the right to human dignity (art. 1), respect for private life, and protection of personal data (arts. 7 and 8), non-discrimination (art. 21), and equality between women and men (art. 23). The right to a high level of environmental protection and the increase of the quality of the environment (art. 37) are also relevant, including in relation to the health and safety of people. Obligations for algorithm testing, risk management, and human oversight are introduced to enforce the respect for other rights by reducing the risk of incorrect or biased AI-assisted decisions in key matters such as employment, education, public services, law enforcement, and the judiciary. In the case of breaches of primary rights, effective compensation for affected individuals is planned by guaranteeing transparency and traceability of the involved systems and by conducting ex post checks. The EU Commission set up a European \hbox{Centre} for Algorithmic Transparency to support its supervisory role with in-house and external multidisciplinary knowledge. The Centre provides support with assessments as to whether the functioning of algorithmic systems is in line with EU laws and regulations.

We must also mention that the regulation of AI software production and use in the EU is also connected with the Data Governance Act and the Open Data Directive, which aim to facilitate data availability by regulating the re-use of public data, by improving data sharing through the regulation of data mediators, and by encouraging the sharing of data for social reasons. The EU strategy for data aims to establish trustworthy instruments and services for the collection, re-use, and sharing of open data that are essential for the development of data-driven machine learning systems. This strategy is also strongly related to the General Data \hbox{Protection} Regulation (GDPR), which is the most important data privacy and security law in the world. We discuss its principles and effects in the next section.

\section{\label{sec:7.5}Biases in Data}

Many software
applications are based on machine learning algorithms that are data driven. Therefore, a major way to inject bias in those algorithms can be found in the practices of collection, selection, and pre-processing of training data. If training data are not fair, inclusive, and balanced, the inductive procedure of a learning algorithm that uses them could result in an unfair model that will produce biased decisions. As we have discussed, the ability of machine learning algorithms to learn from examples is very effective for solving a large number of problems, but sometimes it can be highly dependent on the analyzed data, which, if they do not represent a significant sample of reality, exposes those systems to phenomena such as bias, preconception, and overfitting (which can be defined as the excessive adaptation of a machine learning model to the examples used in the learning phase). In the case of overfitting, the model adapts only to the specific characteristics of the set of training data, but which are not reflected in the rest of the cases that need to be analyzed. For example, a face recognition algorithm that was trained on images of male subjects' faces will fail to accurately identify female faces.

In the case of data bias, the wrong choices that occur in AI algorithms are generated by biases that are more or less hidden in the training data. For example, this can occur when an algorithm for recognizing potential crime suspects is built using input data that contains a high percentage of photos of Black people compared to photos of white people. Thus, non-objective data affect the algorithm recommendations, which in turn suggest wrong choices and produce unjust, sometimes discriminatory results. This has happened in the US when the police, using these types of algorithms as decision support systems, imprisoned people of color who were not responsible for the alleged crimes. To avoid these kinds of occurrences, after an internal audit that discovered unfair surveillance decisions on Black and Latino people, in 2019 the Los Angeles Police Department suspended the use of LASER, a crime prediction software based on historical crime data analysis to predict crime hotspots, and of Palantir Gotham, an AI-enabled system for defense, intelligence, and law enforcement that analyzes near-real-time data, to assign \hbox{people} criminal risk scores. Often, biased decisions occur without the knowledge of those who use the algorithms to make decisions and of the recipient of the decision itself. This aspect is of particular importance because these are algorithms capable of discovering relationships, trends, and models of knowledge that are very useful in many sectors (document management, face recognition, autonomous driving, judges' decisions, analysis of fraud, or identification of complex pathologies) whose operational choices, in addition to not being understandable to human beings, hide choices based on biases present in the input data.

Today, we are faced with algorithms that are implementing sophisticated learning strategies capable of analyzing enormous quantities of data and of making complex choices that traditional software systems are not capable of. But when they do this starting with data that does not represent reality well, they become tools that produce errors, injustices, or discrimination. A known case in the literature is that of a classification algorithm based on deep learning for image recognition, capable of distinguishing wolves from husky dogs. Analysis made by some researchers led to the discovery that the decision to identify the photos with wolves in a large set of images was based solely on the presence of snow in the background of the photographs. The system error was due to the choice of learning examples in which each wolf was photographed in the snow. Therefore, a husky photographed against a snowy background was automatically classified by the system as a wolf. This is an example of bias in the data being reflected in the learning algorithm. The British journalist and activist Caroline Criado Perez in her book \textit{Invisible Women} discusses how men managing data in a biased manner created the so-called ``\hbox{gender} data gap,'' a gap that discriminated against women in many sectors and created an effective but invisible bias with negative effects on women's lives [\citealt{chap:7:CriadoPerez:2019}].

Another much more serious case occurred in Detroit in 2019 with the arrest of Robert Williams for robbery, suggested by an image recognition algorithm used by the Michigan State Police Department. The system was supplied by the DataWorks Plus company at a cost of approximately US\$5.5 million. After some problems due to identification errors by the face recognition system led to the imprisonment of suspects who were later proven to be innocent, it was discovered that the system used algorithms that erroneously tended to identify to a greater extent the faces of African Americans and Asians compared to other types of faces. Although the system was only meant to be used as a ``hinter'' to potential suspects, the police used it to arrest people identified by the system. The incident that happened to Mr. Williams is unfortunately not an isolated case, and, even if the Detroit police have promised a more careful and more specific use (for cases of violent crimes), it is very likely that similar cases will occur unless greater attention is paid to the cautious use of automatic recognition systems.

Another very serious case is that of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system based on machine learning techniques used until recently in various US courts to support the decisions of judges who have to evaluate requests for release. COMPAS implements a predictive model of the risk of criminal recidivism. The algorithm was shown to be racially biased because it assigned twice the risk\vadjust{\vfill\pagebreak} of re-offending to Black people compared to white people under the same conditions. The model displayed that behavior due to bias from previous sentencing and the fact that the US prison population includes a higher percentage of Blacks than whites. For this reason, COMPAS risk decisions have been argued to violate 14th Amendment Equal Protection rights on the basis of race. These cases clearly show how we increasingly use algorithms to make important decisions, and these algorithms, in many cases, are able to make decisions without humans (even the one who made them), while they do not provide us humans with the rationale for those choices.

The Data and Data Governance article of the AI Act states that high-risk AI systems that make use of techniques involving the training of models with data shall be developed on the basis of training, validation, and testing datasets that meet the quality criteria, and training, validation, and testing datasets must be subject to appropriate data governance and management practices. Furthermore, training, validation, and testing datasets must be relevant, representative, free of errors, and complete. Unfortunately, this does not always happen when large datasets are used for training complex models. They should have the appropriate statistical properties, including, where applicable, with regard to the persons or groups of persons on which the high-risk intelligent system is intended to be used. These properties of the datasets should be met at the level of individual datasets or a combination thereof. Moreover, training, validation, and testing datasets shall consider, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioral, or functional setting within which the high-risk AI system is intended to be used. These and other requirements are to be met to avoid algorithmic biases originating from the data they use.

Together with the AI Act, the other main source of regulation for algorithmic bias defined by European Union is GDPR [\citealt{chap:7:EuropeanUnion:2016}]. GDPR also regulates the transfer of personal data outside the EU and European Economic Area (EEA) areas. It was adopted in April 2016 and became enforceable beginning May 2018. This regulation created a set of administrative rules governing any process undertaken on personal data. Note that GDPR considers data protection and data privacy as human rights. Personal data for GDPR means any information that, directly or indirectly, could identify a living person. Name, phone number, and address are textbook examples of personal data, but also any other numerical or categorical ID that in a dataset is linked or relates to personal information is considered as personal data. Personal interests, information about past purchases, health records, and online behaviors are also considered personal data as they could identify a person. GDPR provides each person with certain rights over their own data. They have the right to gain access to their personal data. They have the right to know how an organization is using the data and to object to the handling and processing of those data. The use of personal data must be in line with integrity friendly principles. People or companies cannot collect personal information ``just in case'' they might need it later. This means that people have the right to know how their data are being used, and they must have a say in this matter. Organizations must only store personal data as long as it is strictly necessary. Finally, the processing of data must be safe and secure.

Article 22 of the EU GDPR refers to automated individual decision-making including profiling. Automated decision-making is a situation where a computer-based system, not a human, makes a decision about a person or a group of people. The purpose of article 22 is that the data ``subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her.'' This article aims to limit algorithmic decisions that do not include any kind of human intervention. The article's main goal is preventing algorithmic discrimination based on data that reveal private facts like an individual's belonging to a particular social group or their particular personal characteristics. It does so by restricting the instances in which personal data processing might be lawful. This norm appears to not be directly concerned with biased decisions because of unfair use of data. However, how some layer observed, data used to train a machine algorithm may be inherently biased against protected people or groups. During the automated decision-making phase, this may engender an unfair treatment of persons belonging to protected groups (having specific religious beliefs or sexual orientations). To address this problem, data that may indirectly reveal personal information used by a machine learning algorithm to build a model used for future decision-making must not be used. This type of data should be excluded from the dataset in order to avoid any discriminatory outcome.

Finally, we must mention that the legislative framework defined in the \hbox{European} Union concerning data protection and machine intelligence regulation also includes the Digital Services Act (DSA) and the Digital Markets Act (DMA), which aim to create a safer digital space where the fundamental rights of users are protected and establish a level playing field for businesses. The DSA defines comprehensive new rules and obligations for online platforms to reduce harm and counter risks online. It also introduces strong protections for users' rights online and places digital platforms under a unique new transparency and accountability framework. The DSA rules include new responsibilities to limit the spread of illegal content and illegal products online, increase the protection of minors, and give users more choice and better information.

Together with the approval of the DSA, the EU approved the DMA in 2022 to address the harmful effects arising from unfair behaviors by online platforms\break acting as ``digital gatekeepers'' to the EU single market. These are digital platforms whose position can grant them the power to act as a private rule maker and create a bottleneck in the digital economy. To address these issues, the DMA defines a series of obligations these digital companies need to respect, including prohibiting gatekeepers from engaging in certain behaviors. Concerning data handling, for example, the DMA bans digital platforms from using the data of business users when they compete with them on their own platform. It also prohibits tracking end users outside of the gatekeepers' core platform service for the purpose of targeted advertising, without effective consent having been granted. Designed as a single, uniform set of rules for the EU, the laws discussed here give citizens new protections and businesses legal certainty across the whole single EU market. Other countries such as Brazil, the UK, and the US are working in the same direction and are planning to introduce laws and regulations that will protect citizens from the negative and unfair effects of discriminating algorithms and biased data.

\section{\label{sec:7.6}Ethics in Software}

Ethics is an old discipline concerned with what is morally good or bad and morally right or wrong. It is also applied to any system or theory of moral values or principles centering on questions such as ``what is a good action?'' and ``what is right to do?'' AI ethics is a sub-field of applied ethics and digital technology focusing on the ethical issues raised by the design, development, implementation, and use of machine intelligence. The goal of AI ethics is to discover how ``intelligent'' algorithms can influence many aspects of the quality of life of persons, their freedom, justice, and rights, both as individuals and as members of groups and communities. Ethical studies in the field of computer science help people in understanding how digital technologies may produce new major rights concerns originating from the design and use of machine intelligence. They stimulate experts and citizens to examine what we should do with these new technologies, with social good as a primary goal. In the context of computing, respect for human dignity requires that algorithms treat people with the respect due to them as individuals rather than merely as sources of information, as data subjects. Intelligent automated systems must respect their essential needs and their cultural and physical identity. Human beings' equality is a critical goal that must be addressed together with non-discrimination. Regarding the design and use of algorithms, human equality demands that the same rules must apply for everyone when accessing data and information. It also requires a fair distribution of the added value being generated by software systems and applications. This requires paying attention to protected communities such as individuals with disabilities, minorities, and \hbox{children,} \hbox{considering} asymmetries of information or power as may occur between companies and buyers.

Several scientists, public bodies, and governments are promoting ethics in algorithms and AI applications. The Vatican, for example, in 2020 issued the Rome Call for AI Ethics, which has been approved by many companies and organizations such as Microsoft, IBM, the United Nations Food and Agricultural Organization, and the Italian government. Also, the EU is very active in this area. The Draft of the AI Ethics Guidelines compiled by the European Commission's High-Level Expert Group on Artificial Intelligence [\citealt{chap:7:High-LevelExpertGrouponArtificialIntelligence:2019}] identified five principles and correlated values that must be observed to ensure that AI systems are developed in a human-centric manner:

\begin{itemize}
\item The \textit{Principle of Beneficence}: ``Do Good''---AI systems should be designed and developed to improve individual and collective wellbeing.
\item The \textit{Principle of Non-maleficence}: ``Do No Harm''---AI systems should not harm human beings. By design, AI systems should protect the dignity, integrity, liberty, privacy, safety, and security of human beings in society and at work.
\item The \textit{Principle of Autonomy}: ``Preserve Human Agency''---Autonomy of human beings in the context of AI development means freedom from subordination to, or coercion by, AI systems.
\item The \textit{Principle of Justice}: ``Be Fair''---Developers and implementers need to ensure that individuals and minority groups maintain freedom from bias, stigmatization, and discrimination.
\item The \textit{Principle of Explicability}: ``Operate
Transparently''---Technological transparency implies that AI systems be auditable, comprehensible, and intelligible by human beings at varying levels of comprehension and expertise.
\end{itemize}

These principles and values will be the drivers for implementing trustworthy AI that respects fundamental human rights, applicable regulation, and core principles and values, ensuring an ethical purpose. Trustworthy AI should also be technically robust and reliable as, even with good intentions, a lack of technological mastery can cause unintentional harm. The EU Draft describes existing technical methods that can be incorporated in the design, development, and use phases of trustworthy AI systems. For example, methods to ensure values-by-design provide links between the principles the system is required to adhere to and the specific implementation choices, in ways that are open and supportable by laws or societal norms. Two examples of those methods are security-by-design and privacy-by-design. Value-by-design methods imply the responsibility of software companies to identify the ethical impact that an AI system can have and the ethical and legal rules that the system should comply with.

Other technical methods implement testing and validating processes that test input data, pre-trained models, and the behavior of the system as a whole. They should be performed by different groups of people to have better validation. Furthermore, traceability, auditability, and explanation methods are important. To make procedures and results traceable, AI systems should record both the decisions they make and the whole process that yielded their decisions. Traceability therefore enables auditability, which entails the enablement and facilitation of monitoring and verification of data, algorithms, and design processes. Explainable AI is trying to understand why a system has a given behavior and why it has provided a given interpretation. In this area, methods are needed to better comprehend and make public the underlying mechanisms and find solutions. A well-known challenge with deep learning systems is the difficulty in providing clear explanations for the decisions and results they produce. This mainly originates from the basic mechanisms of network parameter settings that are difficult to \hbox{associate} with the neural network results. The problem is of major importance not only when explaining to the designer or user the behavior of inexplicable systems such as deep learning networks, but also when deploying reliable and predictable systems.

\citeauthor{chap:7:Mittelstadtetal:2016} [\mbox{\citeyear{chap:7:Mittelstadtetal:2016}}] published a useful review of the ethics of algorithms, discussing the main kinds of ethical issues raised by algorithms and proposing a conceptual map based on different concerns that are jointly sufficient for a principled structure of the field. The proposed map identified six ethical concerns (see Figure~\ref{fig:7.1}), which define the conceptual space of the ethics of algorithms. Three of the ethical concerns refer to epistemological factors: inconclusive, \hbox{inscrutable, and} misguided evidence. Two concerns are explicitly normative: unfair outcomes and transformative effects. The last one, traceability, is related both to epistemic and normative concerns.


\begin{figure}[t!]
\tooltip{\includegraphics{graphics/Chapter_07/Figure1.\image}}{Scheme with six text boxes named, from the top, Inconclusive, Inscrutable, Misguided, Unfair Outcomes, Transformative Effects, and Traceability. The first three text boxes are labeled Epistemic Concerns, and the next two text boxes are labeled Normative Concerns.}[-250pt,3pt]
\caption{\label{fig:7.1}Six types of ethical concerns raised by algorithms [\citealt{chap:7:Mittelstadtetal:2016}].}
\end{figure}

Inconclusive, inscrutable, and misguided evidence are the epistemic factors that emphasize the relevance of the quality and accuracy of the data processed by algorithms for the justifiability of the conclusions (the results) that they reach (compute), and which can shape morally loaded decisions affecting individuals, groups, and the environment. In particular, inconclusive evidence focuses on the way machine learning algorithms generate results that are expressed in probabilistic terms. For example, poor quality of data (i.e., input data with missing values or with noise) generally leads to inconclusive evidence to support final human decisions. Inscrutable evidence refers to problems related to the lack of transparency that may characterize algorithms and the socio-technical infrastructure in which they are designed and/or used---for example, algorithms based on opaque techniques such as deep learning, or algorithms that are not well documented. Misguided evidence focuses on unreliable conclusions originated by algorithmic inequalities lying in the design process, the input data, or elsewhere. The two normative concerns (unfair outcomes and transformative effects) explicitly refer to the ethical impact of actions and decisions based on algorithmic recommendations, including lack of transparency (opacity) of algorithmic processes, unfair results and conclusions, and unintended consequences. Among transformative effects, it must be considered that, as discussed in the previous chapters, algorithms can influence how we conceive and perceive the world in which we live and transform its social, cultural, and political organization. Epistemological and normative concerns, together with the spread of the design, implementation, and deployment of algorithms, make it hard to trace the chain of events and factors leading to a given result (this may occur if neural networks are used). This process may limit the possibility of identifying the cause of a particular outcome, and of attributing moral responsibility for it. This is what traceability, the sixth ethical concern, refers to. In fact, when an ethical problem is identified in an automated procedure addressing any or all of the five discussed concerns, ethical assessment requires tracing both the cause(s) and responsibility for the harm.

The framework defined by the conceptual map of ethical concerns raised by algorithms is useful for studying and reviewing the ethics of AI systems. Regarding normative concerns, their study can be useful, for example, for investigating and limiting the business models of big software companies that in some cases are unfair, inscrutable and ultimately unethical. As Moshe \citeauthor{chap:7:Vardi:2022} [\mbox{\citeyear{chap:7:Vardi:2022}}] signaled in a short article on AI ethics, a major problem that computer scientists and algorithm regulators face today is not only that AI technology in some cases is unethical because of algorithmic procedures and data biases, but that AI and, in particular, machine learning systems are largely used by big and powerful corporations to support business models that are mostly unethical. The ``surveillance capitalism'' defined by Shoshana Zuboff is legal and extremely profitable, but it is not ethical because it is based on the unprecedented collection of personal data with the main purpose of profit-making. For these reasons, Vardi suggested that computer scientists must ``have difficult and nuanced conversations on responsible computing, ethics, corporate behavior, and professional responsibility.''

\section{\label{sec:7.7}Human Alignment of AI}

All the
topics we discussed in this chapter are closely related to the alignment of machine intelligence with human values. Algorithmic fairness, transparency, explainability, data biases, and AI ethics are factors that influence the way algorithms perform the tasks they are designed for and the correctness and quality of results they produce. In discussing the moral and technical consequences of automation, Norbert \citet{chap:7:Wiener:1960} enunciated the algorithm alignment problem as follows: ``If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively [...] we had better be quite sure that the purpose put into the machine is the purpose which we really desire.'' This scenario may occur every time algorithms are not correctly coded by including all the constraints needed to compute only the desired output. AI algorithms based on data analysis may suffer even more from this problem because the models they produce depend on the data they analyze. In the general field of AI alignment that aims to guide intelligent systems toward their designers' intended goals and interests, human--AI alignment plays a very important role. Aligning AI systems with human goals is a complex task that requires coding, training, and using AI systems to implement human values, preventing them from acting in ways hostile to human values, and ensuring that machine intelligence benefits all of humanity. For example, a robot, an autonomous vehicle, or chatbot not designed to consider human values in its operations could make decisions that are harmful for humans. These are just a few examples that show there is an actual risk that if machine intelligence is not carefully designed and implemented it could have negative and sometimes disastrous consequences for humanity.

The book \textit{The Alignment Problem: Machine Learning and Human Values} by Brian \citet{chap:7:Christian:2020} provides an interesting reference about the debate on the \hbox{alignment} problem that originates from incomplete and/or inaccurate coding or use of AI systems. Algorithm alignment guarantees that computing models capture our customs and values so that they perform what we mean and expect. When learning algorithms must be aligned with human values, the task is even more complex as it must address the well-being of large social communities or of all humanity. A paragraph from Christian's book summarizes the global effort of the scientific community on this issue: ``An ecosystem of research and policy efforts to influence both the near and long term is underway across the globe; this is still largely nascent, but it is gathering steam. Research on bias, fairness, transparency and the myriad dimensions of safety now forms a substantial portion of all the work presented at major AI and machine-learning conferences. Indeed, at the moment they are the most dynamic and fastest-growing areas, arguably, not just in computing but in all of science.''

Several researchers and teams are working on finding technical solutions to ensure the human alignment of AI systems. At the same time, philosophers are considering which common human values must be considered and how different cultures and societal values can be integrated to avoid value conflicts. Stuart\break \citeauthor{chap:7:Russell:2020} [\citeyear{chap:7:Russell:2020}], a pioneer in the study of human alignment issues, has proposed three principles for creating a safe and beneficial machine intelligence:

\begin{itemize}
\item \textit{Altruism}: the AI's only objective is to maximize the realization of human values, that is what humans would ``prefer their life to be like.''
\item \textit{Humility}: AI agents are firstly uncertain of what human values are, but they can learn real values and inclinations by observing how human beings act in stable contexts.
\item \textit{Human behavior as a source}: The ultimate source of information about human preferences is human behavior. To achieve value alignment between artificial intelligence systems and humans, people must learn to be better persons, or, perhaps, simpler.
\end{itemize}

A more comprehensive list of principles was defined in the Asilomar Conference on Beneficial AI held in January 2017. More than a hundred scientists discussed how to make machine intelligence systems beneficial for people and formulated a set of principles of beneficial AI called the ``23 Asilomar AI Principles.'' The principles are organized under three categories: (a) related to research, (b) related to ethics and values, and (c) principles concerned with futuristic issues. Examples of principles related to ethics and human values are:

\begin{enumerate}
\item[(6)] \textit{Safety}: AI systems should be safe and secure throughout their operational lifetime, and verifiably so where applicable and feasible.
\item[(9)] \textit{Responsibility}: Designers and builders of advanced AI systems are stakeholders in the moral implications of their use, misuse, and actions, with a responsibility and opportunity to shape those implications.
\item[(10)] \textit{Value Alignment}: Highly autonomous AI systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation.
\item[(11)] \textit{Human Values}: AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity.
\end{enumerate}

The Asilomar principles show that computer science researchers are making advances in understanding the nature and explication of human values and are proposing solutions to the alignment problem. However, several issues remain to be addressed and solved as machine intelligence is playing an increasingly significant role in our lives. The growth of AI systems makes it crucial that human values become an essential part of the processes in which an AI system makes evaluations and decisions in the positive and harmless fulfillment of human goals and values. However, we must acknowledge that is not easy to encode human values in programming languages and tools. Moreover, we, as humanity, do not agree on a fixed set of common values. Therefore, a key issue to be addressed and solved is to accurately define what these values are, because people living in different countries have different cultures and different social and anthropological backgrounds. These and other issues are being studied by the scientific community to theoretically and practically understand how to go from human values that we all agree on, toward coding and embedding them into AI systems that we use more and more often.

\begin{thebibliography}{}

\bibitem[ACM Technology Policy Council(2022)]{chap:7:ACMTechnologyPolicyCouncil:2022} ACM Technology Policy Council. 2022. Statement on Principles for Responsible Algorithmic Systems. Retrieved from \href{https://www.acm.org/binaries/content/assets/public-policy/final-joint-ai-statement-update.pdf}{https://{\allowbreak}www.{\allowbreak}acm.{\allowbreak}org/{\allowbreak}binaries/{\allowbreak}content/{\allowbreak}assets/{\allowbreak}public-{\allowbreak}policy/{\allowbreak}final-{\allowbreak}joint-{\allowbreak}ai-{\allowbreak}statement-{\allowbreak}update.pdf}.


\bibitem[Benjamin(2019)]{chap:7:Benjamin:2019} R. Benjamin. 2019. \textit{Race After Technology: Abolitionist Tools for the New Jim Code}. Polity Press.

\bibitem[Bostrom and Yudkowsky(2014)]{chap:7:BostromandYudkowsky:2014} N. Bostrom and E. Yudkowsky. 2014. The ethics of artificial intelligence. In \textit{The Cambridge Handbook of Artificial Intelligence}. Cambridge University Press, 316--334. DOI:~\href{https://doi.org/10.1017/CBO9781139046855.020}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1017/{\allowbreak}CBO9781139046855.020}.

\bibitem[Broussard(2023)]{chap:7:Broussard:2023} M. Broussard. 2023. \textit{More than a Glitch: Confronting Race, Gender, and Ability Bias in Tech}. MIT Press.

\bibitem[Christian(2020)]{chap:7:Christian:2020} B. Christian. 2020. \textit{The Alignment Problem: Machine Learning and Human Values}. W.W. Norton, New York.

\bibitem[Criado Perez(2019)]{chap:7:CriadoPerez:2019} C. Criado Perez. 2019. \textit{Invisible Women: Exposing Data Bias in a World Designed for Men}. Vintage Books.

\bibitem[European \hbox{Commission}(2023)]{chap:7:EuropeanCommission:2023} European Commission. 2023. Artificial Intelligence Act. Retrieved from \href{https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS\_BRI(2021)698792\_EN.pdf}{https://{\allowbreak}www.{\allowbreak}europarl.{\allowbreak}europa.{\allowbreak}eu/{\allowbreak}RegData/{\allowbreak}etudes/{\allowbreak}BRIE/{\allowbreak}2021/{\allowbreak}698792/{\allowbreak}EPRS\_{\allowbreak}BRI{\allowbreak}(2021){\allowbreak}698792\_{\allowbreak}EN.pdf}


\bibitem[European Union(2016)]{chap:7:EuropeanUnion:2016} European Union. 4 May. 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC. \textit{Off. J. Eur. Union} 59, 294.


\bibitem[High-Level Expert Group on Artificial Intelligence(2019)]{chap:7:High-LevelExpertGrouponArtificialIntelligence:2019} High-Level Expert Group on Artificial Intelligence. 2019. \textit{Ethics Guidelines for Trustworthy AI}. European Commission, Brussels.

\bibitem[Lambrecht and Tucker(2019)]{chap:7:LambrechtandTucker:2019} A. Lambrecht and C. Tucker. 2019. Algorithmic bias? An empirical study of apparent gender-based discrimination in the display of STEM career ads. \textit{Manag. Sci}. 65, 7, 2947--3448. DOI:~\href{https://doi.org/10.1287/mnsc.2018.3093}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1287/{\allowbreak}mnsc.{\allowbreak}2018.{\allowbreak}3093}.

\bibitem[Mittelstadt et~al.(2016)]{chap:7:Mittelstadtetal:2016} B. D. Mittelstadt, P. Allo, M. Taddeo, S. Wachter, and L. Floridi. 2016. The ethics of algorithms: Mapping the debate. \textit{Big Data Soc.} 3, 2. DOI:~\href{https://doi.org/10.1177/2053951716679679}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1177/{\allowbreak}2053951716679679}.

\bibitem[Mullaney et~al.(2021)]{chap:7:Mullaneyetal:2021} T. S. Mullaney, B. Peters, H. Mar, and K. Philip (Eds.). 2021. \textit{Your Computer Is on Fire}. MIT Press.

\bibitem[O'Neil(2016)]{chap:7:ONeil:2016} C. O'Neil. 2016. \textit{Weapons of Math Destruction}. Penguin Random House.

\bibitem[Russell(2020)]{chap:7:Russell:2020} S. Russell. 2020. \textit{Human Compatible: Artificial Intelligence and the Problem of Control}. Penguin Books, New York.

\bibitem[Vardi(2022)]{chap:7:Vardi:2022} M. Y. Vardi. March. 2022. ACM, ethics, and corporate behavior. \textit{Commun. ACM} 65, 3, 5. DOI:~\href{https://doi.org/10.1145/3516423}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}3516423}.

\bibitem[Wiener(1960)]{chap:7:Wiener:1960} N. Wiener. 6 May. 1960. Some moral and technical consequences of automation. \textit{Science} 131, 3410, 1355--1358. DOI:~\href{https://doi.org/10.1126/science.131.3410.1355}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1126/{\allowbreak}science.{\allowbreak}131.{\allowbreak}3410.{\allowbreak}1355}.

\end{thebibliography}
%\end{document}

