%\documentclass{acm-book-v2}
%\RequirePackage[errorshow]{tracefnt}
%%\newcommand{\mpage}[1]{}
%%\newcommand{\indexfn}[1]{}

%%\usepackage{showframe}

%\usepackage{custom-tooltip}
%\usepackage{custom-tooltip-Alt-Text-View}


%\begin{document}

\setcounter{chapter}{4}

\chapter{\label{chap:5}Living with Algorithms}


\noindent {This chapter discusses the pervasiveness of algorithms in our life: when we interact with the operating systems of our PC and smartphone, when we access the web or publish a post or a tweet on social media, in our car and at home, algorithms are with us. They live in our digital devices and interact with us during our daily lives. Algorithms are always ready to serve or to manipulate people. They bring us the morning news, help us use public services, and provide any information available online. AI algorithms influence the lives of billions of people, which make them very powerful and risky at the same time. Here we discuss a set of significant machine learning algorithms utilized in widely used applications and describe the basic principles of their  operation.}

\section{\label{sec:5.1}Algorithms in Our Lives}

Software programs implement algorithms that make computers work and support human beings in most daily activities. Algorithms execute everything that happens all the time on the Internet. Computer programs manage most of the purchases of goods and services and rule most of the daily financial transactions in the world. The enormous wealth and power of Google, Amazon, Microsoft, Apple, and Facebook are based on the algorithms they implement and the data they collect every minute of the day and store in their data centers. Algorithms have led man to the Moon; every day they help fly airplanes and check railway networks; they allow us to be connected to any remote village on Earth. They govern the factories and the production processes in almost all developed nations. It is the Google search algorithm that finds and suggests to billions of people the contents of web pages that may interest them. It is the News Feed algorithm of Facebook that decides which information from our contacts (the so-called friends) is the most important for us. As discussed in the previous chapters, it is data analysis and machine learning algorithms that search through Big Data to discover hidden facts and information. Through those algorithms, Big Data owners learn personal facts and behaviors of people and often use them for their own goals. Some make them available to those who are willing to pay.

Through cryptography algorithms, communications that need to be kept confidential are encoded, and algorithms for image analysis are used to recognize our fingerprints or our faces. Other algorithms control the operation of hospital operating rooms or allow a \hbox{surgeon} to operate on a patient thousands of miles away. In short, algorithms are everywhere, and it will be impossible to do without them in the future. Given their pervasiveness and relevance, we need to understand the importance of mature and responsible use of the algorithms that are performed by billions of computers, smartphones, tablets, and IoT devices that populate our world and affect our daily lives. There are many cases where bugs in software programs have caused major problems and damage, and others where the irresponsible or illegal use of algorithms has harmed people, groups, and \hbox{societies.} Cases of fraud based on illegally used software in the automotive industry, political campaigns, and government agencies show the enormous risks associated with the criminal use of algorithms and the immense social and commercial effects that derive from them.

To understand the importance of computer algorithms in people's lives, we mention here a few algorithms that are used every day by millions or billions of individuals around the world, even if most of them don't know they are using them. Online shopping is continuing to grow worldwide. During the COVID pandemic, around 70\% of the population of the European Union (i.e., around 280 million \hbox{people)} bought or ordered goods or services online. In about five years, from 2016 to 2022, online shopping in the EU increased by 10 percentage points. In the same period, 80\% of people in the US shopped for groceries online, reaching a total of \$1.14 trillion in sales. The trend continues today with about 300 million \hbox{American} consumers shopping online. In the People's Republic of China (PCR), online shoppers increased from 193 million in 2011 to 842 million at the end of 2021 (about 60\% of the PRC's population). Among the reasons for this very large utilization of online shopping portals is the use of data analysis and machine learning algorithms in e-commerce for profiling the different classes of target customers, predicting their preferences and desires, and suggesting new products that may be of interest, even if they often don't know them.

Amazon is one of the world's largest e-commerce portals, and the Amazon Search algorithm (now called A10) powers Amazon's sales. A10's main goal is to connect customers as quickly as possible with the most relevant products they are searching for. It analyzes shoppers' search queries for keywords and then matches customer needs with relevant products. To perform a search, the Amazon search engine determines which items are the optimal matches with the customer's query and then scores them based on the item's level of relevancy to the consumer. This is also done for web pages by the Google search engine, which we will discuss later. Amazon's ranking algorithms automatically learn to combine several relevance features and analyze past search patterns to adapt to what is most important for the customer. The Amazon search algorithm is implemented by a machine learning framework able to rank within categories, blending separate rankings in the search of the global product database. It uses natural language processing techniques for matching queries and products and includes algorithms targeted at unique tasks in specific product categories such as books and electronics.

For training the ranking models, A10 uses labels based on shoppers' actions such as clicks, purchases, or add-to-basket. The system uses the search engine to collect the training sets to be used to improve the ranking model. Many times in each day it computes the set of keywords issued for each context of interest. The context can be a combination of product category, marketplace, user features, and previous searches. To train ranking models of the Amazon ranking engine, training, validation, and test sets are built by collecting data from several days of shopper in general. Test sets are created from dates after the training set dates. Behaviors that resulted in either click or purchase as positive examples are selected for testing the algorithm. At the core of the A10 is a machine learning strategy called gradient boosted trees that extends the decision tree technique to decide the ranking by discovering complex feature interactions. The gradient boosted trees algorithm is based on a collection of decision trees that implement both classification and regression handling categorical or real-valued features. Moreover, this algorithm also works well in the presence of missing values. Boosting algorithms first build a model on the training dataset and then build a second model to rectify the errors present in the first model. The rationale of a gradient boosted algorithm is to build learning models sequentially, with each subsequent model trying to reduce the errors of the previous model. For training ranking models, Amazon A10 uses several features related to products sales, client reviews, query specificity, customer status, and others that provide different measures of textual similarity among similar or different product searches. Furthermore, the algorithm \hbox{considers} that in product searches many goods can share similar descriptions, thus those products that are more popular than others are ranked higher as they can better meet customer desires.

To improve its business, in 2012 Amazon also patented the ``Method and system for anticipatory shipping'' (patent no. US8615473B2) based on a machine learning delivery algorithm that is able to analyze personal and group data to propose and deliver products that customers do not yet know but that, with high probability, they will want. Those products are moved toward customer countries and cities before they have ordered them. By analyzing shopper preferences, their previous orders, and other information about other buyers who have made similar \hbox{purchases,} Amazon has developed a form of anticipated retailing (\textit{anticipatory \hbox{selling}}) that tells its customers what they should buy even if, sometimes, they do not know of the existence of those products. We can say that the Amazon algorithm is able to foresee our needs and desires, even if they are unknown to us. As with the A10 algorithm, anticipatory shipping is made possible by the use of the traces that consumers leave on the Amazon web site when they \hbox{register}, when they are looking for a smartphone or for trousers, when they buy a book by Annie Ernaux or a motorcycle. Every click by customers is monitored and recorded by the machine learning system, to the point that Amazon gets to know its customers better than they know themselves, at least as regards their purchasing wishes.

Scheduling algorithms are used to find efficient ways to carry out a sequence of actions, that is, finding an order to assign resources for performing tasks well. This means, for example, minimizing the total time to complete the actions and/or maximizing the total number of actions completed per time unit. Scheduling algorithms are used for project planning, manufacturing, maintenance, and optimizing transportation. In this last case, scheduling algorithms are used to compile railway timetables, public bus routes and timetables, and routes and times of departure and arrival of planes. In all these cases, algorithms help us to organize our daily lives and sometimes they complicate it. Around the world billions of \hbox{people} use public transportation; therefore, scheduling algorithms are vital for them. Mapping out city bus routes or flight schedules are very tedious and complex tasks for human beings. In all these cases, algorithms are very helpful for finding the best combination of routes or flights. They produce all possible solutions, saving time and fuel. For improving transportation and travel, the Global Positioning System (GPS) is widely used. GPS is a satellite-based navigation system that provides geolocation and time information to a receiver anywhere on Earth. It works by pinging satellites and transmitting a unique signal to each one. With these signals, the GPS employs an algorithm for computing the location of a person, car, truck, plane. It provides three-dimensional user positioning by solving a set of non-linear trilateration equations (trilateration is defined as the process of determining locations of points by measurement of distances, using the geometry of circles, spheres, or triangles) on the signals received from a set of GPS satellites. An equation solution is implemented by a set of algorithms that try to obtain the best possible positioning accuracy by eliminating the impact of errors due to atmospheric delays, receiver vehicles dynamics, satellite clock errors, and received noise. Moreover, GPS receivers also use a tracking algorithm that provides the ability to predict future positions based on the history of the individual positions being reported by the GPS device. Without the use of algorithms, people using the GPS system could easily follow the wrong path or even get lost.

Moving on to discuss a different area where algorithms have been playing an important role recently, we examine how data analysis algorithms are used in streaming television. Netflix has become a giant in the online distribution of television content and media production. In 2022 it had a revenue of 31 billion dollars and more than 200 million customers who daily watched movies, television series, and other content that the platform streamed via the Internet. In 2018, at the Film Festival of Venice, the Leone d'Oro was won by the film \textit{Roma}, which was first released online by the Netflix platform before being shown in theaters. Netflix uses data analysis and machine learning algorithms to study the behaviors and preferences of its hundreds of millions of viewers who are closely monitored in everything they do on the media streaming platform. Data refer to which films they watch, for how long, which ones they quit, which ones they search for, which ones they like, and which ones they don't like very much. All this volume of customer data is used to control user preferences and to decide what to bet on and which productions to invest in, based on customer behavior and not on the decisions of programmers and directors.

This new production and distribution situation is generating unprecedented internal conflicts between the Netflix data analysis team that works in Los Gatos, CA, and the production team based in Los Angeles. The production team has not been completely open to the suggestions that the technology team has provided based on their analysis of consumer behavior. Hollywood doesn't give much credence to data and above all they want to continue to decide on the basis of the skills of cinema professionals and not on the basis of the numbers, data classification, and graphics that come from the IT division. It's a battle between the algorithms that unearth information in the data and suggest what content to invest in, and the producers, screenwriters, and directors who want to be left alone in deciding which films to produce using the media giant's huge profits. The Netflix production team's anger did not even spare the algorithms that analyze Big Data, which were at the receiving end of swear words, treated as if they were people that could be offended. What is happening inside Netflix is a very significant example of the changes that occur in many companies and organizations when data is used as an element on which to base decisions. Even the actors fear that the machine learning algorithms could question their fame and their abilities because Netflix also uses image analysis techniques that have found that some protagonists worry about showing themselves in particular poses without caring about the rest of the (film/TV) set.

The case of Netflix is not isolated. Epagogix is another company that, using algorithms, predicts the success of a movie to be made on the basis of the kind of scenes that screenwriters would like to insert in it. Another interesting \hbox{example} is the British publishing house Unbound, which uses data analysis algorithms to decide which books to publish. Unbound funds its publications through crowdfunding and the text of a potential writer to be published is evaluated by the editors, by some readers, and by a learning algorithm that serves to estimate the potential future sales of the book based on the author's followers on social media. Their aptitudes and their personal and public profile are used to calculate the potential to become famous. In the final decision on whether to publish a book, the opinion of the algorithm is pre-eminent over that of the editors or the publishing managers. In the end, it is the algorithm that analyzes a set of data and chooses which authors to publish according to the tastes of potential readers and not the quality of the text.

Also, colleges and higher education institutions that give scholarships are using predictive algorithms to convince more students to enroll. Learning algorithms for regression and classification analyze student data and on this basis allow colleges to vary the cost of attendance according to students' willingness to pay tuition. Generally, these algorithms first predict how likely future students are to enroll, and then help determine how to allocate scholarships to convince more of those potential students to attend the college. These algorithms are becoming invaluable for colleges in terms of planning and supporting financial stability; however, they often decide the future and sometimes the destiny of students. For this reason, when education institutions do use learning algorithms to assign scholarships, they should proceed carefully and consider how scholarship changes affect \hbox{students'} likelihood to graduate. Colleges should also guarantee a primary role for humans in the selection process performed by algorithms to avoid algorithmic biases and avoid inequities among student population classes. The predictive step of enrollment algorithms estimates how likely an applicant is to enroll in a given college. The algorithm uses data about test scores of the college's past applicants, the amount of financial aid they received, where they live, and other personal information. Using past admissions data, the algorithm builds a predictive classification model that calculates whether each of the accepted applicants will choose to enroll. Typically, this is done with supervised learning methods, decision trees, deep learning, and logistic regression. In some cases, N\"{a}ive Bayes, \textit{k}-nearest neighbors, and gradient boosting algorithms are also used. The output model will be used to predict the likelihood of enrollment for future applicants [\citealt{chap:05:Basuetal:2019}].

Many other AI applications are based on machine learning algorithms. In the next sections, we will discuss some that have had a great impact on the daily lives of many individuals. Here, we briefly mention a few, such as face recognition, online dating systems, and virtual assistants, that many people use. Facial recognition is a biometric identification technique that is based on the detection of a set of features on an individual's face to identify people. Face recognition technology is used everywhere, even when people are not aware of it. Millions of people use facial recognition software to unlock and log onto their smartphones. A similar use is made in many mobile apps to avoid the insertion of textual credentials (username and password). At the same time, with face identification algorithms surveillance workers can pick the faces of criminals out of crowds. While most facial recognition systems work by searching for the face image from a database of known faces (face detection), advanced facial recognition systems use AI methods that learn to identify people's faces even if their appearance has changed, such as if they've grown a beard, wear glasses, or change their facial expression. To do this algorithms carry out other steps for face alignment, measurement, and recognition. The most commonly used machine learning methods for facial recognition are based on deep learning algorithms and in particular on convolutional neural networks (CNNs). Convolution is a procedure that puts the input images through a set of convolutional filters, each of which activates certain features from the images. A CNN can have tens or hundreds of layers that each learn to identify different features of an image. Filters are applied to each training image at different resolutions, and the output of each analyzed image is used as the input to the next layer. In this way, CNNs learn to extract a set of features from facial images and use them to classify the images and compare them on the basis of those unique features (such as distances between eyes, shape of mouths, etc.). A deeper network can learn to identify other facial features such as the kind of skin or other smaller details.

Data analysis and machine learning techniques are used in dating apps to suggest appropriate matches based on people's personality and also to provide advice on first dates based on what a user suggests to the algorithm about the other person. Many couples meet online and dating applications have become increasingly popular. Machine learning algorithms are used in many of the apps that help clients find their soul mate. The apps act as a sort of artificially intelligent matchmaker. Among the algorithms used in dating software, classification and clustering are often used. For instance, clustering may find groups of males who want to meet females who are from two to five years younger and like sports or younger females who prefer males who are four years older than them and are romantic. These techniques are also used in combination with recommendation and/or matching algorithms such as the Gale--Shapley algorithm, which finds a solution to the stable marriage problem, that is, discovers a stable match between two equally sized sets of elements (e.g., \textit{n} men and \textit{n} women) given an ordering of preferences for each element.

Another class of applications that are widely used and are based on artificial intelligent algorithms are virtual assistants, also called AI assistants. Some examples of these are Siri, Cortana, Alexa, Google Assistant, and Bixby. These \hbox{complex} software systems interact with people by using natural language processing and deep learning algorithms to understand natural voice commands or questions and then reply to users. The AI algorithms used in virtual assistants process data input from users (e.g., voice records and texts) and use other information to provide services such as customer metadata, people geolocation, previous discussions, personal information, and other knowledge stored in databases. They implement grammar recognition, semantic interpretation, and speech-to-text and text-to-speech efficient techniques. To solve these complex tasks, deep learning and other classification algorithms are largely used. For this reason, we may say that a virtual assistant is essentially a composite machine learning system that is capable of comprehending human language and responding to people. These characteristics would make Alan Turing happy, as he was the first proponent of software systems that would simulate human behavior and interact with people in a fairly natural way.

\section{\label{sec:5.2}Software Intelligence in Searching the Web}

Operating systems are the basic software programs that manage the hardware and software resources of computers, smartphones, and other computing devices we use every day. An operating system is composed of a collection of software procedures (or processes) that run as soon as we turn on our computer and are the last ones to terminate when we turn off our machine. Operating system tasks are many and concern the management of the CPU, RAM, disks, files, I/O, and interactions with users. Although many of the operating systems we use today employ scheduling and resource management algorithms designed 50 years ago, several machine learning techniques have been designed and added to improve the performance of operating systems, limit power consumption, and improve or simplify user interactions. The latest versions of operating systems use voice interfaces, analyze usage data to optimize hardware use, read our emails, and when they find an appointment or a purchase of a flight ticket, update our calendar. All those operations are assisted by machine learning algorithms. Decision tree-based classification (introduced in Chapter~\ExternalLink{chap:4}{4}) has been studied, for example, as a technique to learn the CPU time-slice utilization behavior in Linux when user programs are executed. Obtaining a model of CPU utilization allows designers to improve CPU usage during the regular use of a computer and reduce the execution time of programs. Deep learning and regression methods (also discussed in Chapter~\ExternalLink{chap:4}{4}) are used to improve the reliability of operating system software. Other learning methods have been examined to provide efficient management of distributed resources in operating systems that manage a network of computers (distributed operating systems). For the implementation of autonomic computing features, that is, the ability of a computer to manage itself automatically through adaptive methods for management and predictive maintenance, some operating systems include self-learning techniques that use unsupervised learning methods to control the hardware system, predict future failures, and schedule software updates and configurations.

Although some machine intelligent techniques have been introduced in the field of operating systems, there is still a lot of space to fill, and new techniques will be introduced in the coming years. An application field where machine learning techniques are widely used with very effective improvements with respect to previous solutions is the domain of web search engines. In this field, Google Search is the top search engine. According to recent statistics, about half of the world's population uses the Google Search engine to navigate the web and find the information and answers they need. When Larry Page and Sergey Brin started the company and deployed the search engine in 1998, there were only 10,000 search queries per day. In 2022, around 8 billion searches were performed daily all over the world through the Google engine. This extraordinary data corresponds to about 3 trillion searches per year. Based on recent statistics, Google controls the web search market with a more than a 90\% share. These data are impressive and demonstrate the advantage that the company founded by two Ph.D. students from Stanford University achieved with respect to its competitors like Bing, Yahoo!, and Baidu. We must also consider that each Google search is executed in parallel on several computers and a response is given in 0.2 seconds on average. To offer this very efficient service, Google data centers run more than 1 million servers, and the Google Search Index includes hundreds of billions of links to web pages. The Google motto explains very clearly the goal of a company that was originated by the development of an innovative web search algorithm, and is based on one of the largest distributed databases of web links in the world: ``Our mission is to organize the world's information and make it universally accessible and useful.''


Google Search is based on three main steps, (i) the crawling stage that downloads text, images, and videos from web pages it finds on the Internet, with automated programs called crawlers, (ii) the indexing stage that analyzes the contents on each page and stores the information in the Google index database, and (iii) the search result provisioning stage that returns information relevant to the user's query. At the base of Google's big success is the web search and ranking algorithm. This is a patented procedure that decides the order of the web links obtained as the result of a search as they appear on the search engine return page. Basically, the page ranking algorithm assigns a score to every searched page. The higher the page score, the higher the page will appear in the search results. The score of a page is determined by hundreds of factors, the first one being based on the number of web pages that link to the page. The rationale is that the more links a page has to it, the more credible/important/significant it must be.

PageRank was the first algorithm developed by Page and Brin [\citealt{chap:05:Pageetal:1999}] that Google used to rank web pages in the results of its search engine, and it is still in use even after many other changes and integrations. PageRank was developed by Google's founders together with other researchers at Stanford University. It is based on the principle that a web page's importance can be expressed by the number of other web pages that link to it. Google was not the first company to exploit link analysis to define the ranking of web pages in search results. Robin Li, who later founded Baidu, developed the RankDex algorithm in 1996; his patent was filed one year before Google's analogous patent. Page and Brin began developing PageRank in the same year at Stanford University; however, the patent for PageRank was filed in January 1997.

The PageRank algorithm works in conjunction with several others to find the best response to a user query; the most important are mentioned here. We must keep in mind that Google updates its algorithm every week, and it releases major updates two to three times a year, which can have a significant impact on rankings. PageRank assumes that more significant web pages will obtain more connections from other websites. In particular, the page rank $R(W)$ of any page \textit{W} is proportional to the number of incoming paths toward \textit{W}. The idea is that the greater the number of paths from various pages that are directed toward page \textit{W}, the higher the probability of accessing that page. Considering that multiple links come out from a page $w{}_{i}$, the page rank $R(W)$ for page \textit{W} is proportional to the ratio of the number of paths that are directed toward page \textit{W} from page $w{}_{i}$ to the total number of outgoing paths $T(w{}_{i}$) from page $w{}_{i}$. Finally, the algorithm takes into consideration whether the page $w{}_{i}$ itself has a high page rank (in other words, it is authoritative). Since the probability to feature an authoritative page $w{}_i$ is high, the same holds for the page \textit{W}. Through this step, the page rank $R(W)$ of a page \textit{W} can then be computed as the total summation of the ratio of the recursive function of all pages $w{}_{i}$ that are featuring page \textit{W} to the total number of outgoing paths from all pages $w{}_{i}$.

This algorithm is at the core of the Google Search engine; however, many other ranking factors are considered in the algorithms used in the Google Search engine. Some of them are web page recentness, topical authority, keyword mentions, and user experience. The recency factor depends on the last time a web page was updated and it matters more for queries that refer to news or recent facts or arguments. Page topical authority is used to promote pages that provide valued content about requests relevant to the one being searched. Keyword mention has been a primary ranking factor used by traditional search engines. In Google Search, it is just one factor to consider, and it concerns the number of times a searched term appears on the page to be ranked. Finally, user experience considers simple page navigation, reduced loading time, website technology, and responsive design. Regarding more specific AI methods used in the search engine, we must mention machine learning-based algorithms such as RankBrain, Bidirectional Encoder Representations from Transformers (BERT), and neural matching that have been integrated into the engine in the past ten years. The RankBrain algorithm is used to understand how words in a user query are related to concepts. Through language processing techniques, it takes a broad query and better defines how that query relates to real-world concepts. By understanding and matching words to their related concepts, RankBrain helps the Google engine to better understand what exactly users are looking for. To do that, the algorithm sorts search queries into word vectors, also known as ``distributed representations,'' which are close to each other in terms of linguistic similarity. RankBrain then attempts to map a query into words or groups of words that have the best chance of matching it. Therefore, it attempts to guess what users mean and records the results. This allows the adaptation of current and future results to provide better user satisfaction. In 2018, the search engine was augmented with neural matching, another machine learning algorithm that is used to understand how words are related to concepts and in particular how queries relate to web pages by looking at the entire query or content on the page and understanding it within the context of that page or query. In summary, we can say that while RankBrain helps the engine better relate pages to concepts, neural matching helps it better relate words to searches. Although this, like all the algorithms developed by Google, is not public, from Google's announcement we can infer about neural matching that ``Neural embeddings, an approach developed in the field of neural networks, allow us to transform words to fuzzier representations of the underlying concepts, and then match the concepts in the query with the concepts in the document. We call this technique neural matching.''



In 2019, a year after the introduction of neural matching, Google implemented BERT, another neural network-based method for natural language processing. BERT better understands the tones and context of words in searches, similar to how humans pick up on these informational signs during a conversation. Using neural networks, BERT analyzes the structure of the text going beyond the strict grammatical rules. A pre-training process provides BERT with billions of questions, which it separates and classifies, putting every word into multiple contexts. This will result in subtle changes to the way searches work. Small elements in user queries, such as ``to,'' ``not,'' or ``no,'' may have been ignored or underestimated by previous Google algorithms. BERT queries are analyzed and assessed in much more detail, teasing out nuances that may have been missed before. For example, in a query like ``\hbox{Italian} tourists to the UK after Brexit needs a visa,'' before BERT the search engine ignored the word ``to'' and showed results for a UK citizen traveling to Italy. With BERT the word ``to'' is put into context, and more accurate search results useful for Italian travelers can be delivered. We can say that the transformers the BERT algorithm uses are deep learning models that adopt the mechanism of self-attention, differentially weighting the significance of each part of the input data (in this case a set of words composing a sentence). Transformers are designed to execute natural language tasks such as text translation and summarization. A transformer processes the entire input all at once to extract the meaning of a sentence by a global evaluation.

As a final topic about the use of machine learning in Google Search, in the most recent versions user profiling techniques are playing a major role. When we use Google Search to find some news, a book review, or any other information, Google uses our searches to learn about us. The search engine implements user profiling algorithms that build detailed profiles of users based on their search history as well as browsing history on Google-owned sites such as Google Maps or YouTube. Users' profiles are exploited to build an advertising profile that serves to provide users with ads that match their interests during their web navigation and personalize the search results. Personalized search results are based not only on the ranking factors we discussed but also on the information that the search engine has about the user at the given time, such as their location, search history, demographics, or interests. The purpose of personalized search is to increase the relevance of the results for the particular user; however, it may create discrimination, disparities, and undesired selection of information with unpleasant effects. Technically, searches on Google Search are associated with a user account or a browser cookie record. When a user performs a search, the search results are based on the user profile (e.g., sex, age, location, preferences) and on which websites they visited through previous search results. Some of the most common machine learning methods used for personalization are linear and logistic regression algorithms, association rules, clustering, and deep learning models.

Together with a more personalized experience that can increase the relevance of the search results, personalization may also have negative side effects such as the creation of a filter bubble. This is a term coined by Eli \citet{chap:05:Pariser:2011} to describe the state of cultural or intellectual isolation created by software systems that limit the variety of information a person may access by providing personalized contents that they would like to read. This kind of scenario can result from tailored searches when a web search algorithm selectively guesses what information a user would like to see based on information about them. Consequently, a user becomes separated from information that disagrees with their viewpoints, effectively isolating them in their own cultural or conceptual bubble. Together with Google \hbox{personalized} search, other examples of personalization systems that may influence hundreds of millions of people are social media news-streams, such as Facebook's personalized News Feed, which we discuss in the next section.


\section{\label{sec:5.3}Social Media Algorithms}

According to the ``Data Never Sleeps'' report, as of April 2022 the Internet reaches 63\% of the world's population, that is, about 5 billion people. Of this total, 4.6 billion were social media users. If we look at the Statista.com infographic, the total amount of data generated, collected, shared, and visualized in 2022 was 97 zettabytes, a number projected to grow to 181 zettabytes by 2025. Every minute of the day, Facebook users share about 1.7 million posts, TikTok users watch about 170 million videos, Twitter users share 350,000 tweets, and Instagram users share 66,000 photos. These numbers demonstrate that social media data are a very rich repository of data to be analyzed by machine learning and data analytics techniques to extract a lot of valuable information and knowledge. Indeed, all social media platforms use advanced machine learning algorithms to classify users for advertising, for sentiment analysis, for marketing purposes, for information personalization, and many other goals. For example, LinkedIn uses machine intelligence techniques to provide job recommendations, suggest people you might like to connect with, and provide specific posts in a user's feed. Snapchat uses computer vision techniques and neural networks to track user features and overlay filters that move with our face in real time. Snapchat also uses deep learning models to intercept hand motions. These hand gesture models can be used to provide augmented reality features. Twitter uses machine learning on its social data to understand what tweet recommendations to suggest on the user's timeline for a personalized experience. Facebook, Twitter, and other social media also use AI techniques to combat inappropriate contents such as hate speech, fake news, and prohibited content. Meta's researchers use AI and machine learning to manage the Facebook Feed to serve users content of major interest. Although many users don't know this, on Facebook they do not read posts in the chronological order they are published by their ``friends.'' It is the Feed algorithm that selects the order in which they are visualized. That algorithm also decides how important a post is for users; it therefore intervenes in the user interaction using a mediation power that may interfere in the relationships between friends, family members, and colleagues.

The learning algorithms used by Facebook survey users about how meaningful they found an interaction with their friends (i.e., a like, a comment, a share) or whether a post is worth their time to make sure the content classification reflects what users say they find meaningful. When a user opens the app, the Facebook learning architecture invokes a Web/PHP layer that queries the Feed aggregator. The Feed aggregator collects all relevant information about a post and analyzes all the features (e.g., how many people have liked this post before, how many times it has been shared) in order to predict the post's value/importance to the user as well as the final ranking score by aggregating all the predictions. This strategy may offer the most appropriate posts for users, but at the same time it can create the above-mentioned filter bubble. Using classification algorithms, probably implemented by deep learning systems with about a hundred thousand parameters, Facebook selectively guesses what posts a user would like to see based on the profile the algorithms were able to derive. As a result, users may become disconnected from posts that disagree with their viewpoints, therefore isolating them in their own cultural or sociopolitical bubbles.

Facebook is a social platform that for many people is useful and for most is also fun. However, unpleasant cases have occurred in past years due also to the use of data analytics and learning algorithms that manage user interactions. In particular, after the Cambridge Analytica case, people have begun to become aware that it is increasingly necessary to know how the platform works and to be able to manage our digital life spaces on social media. Cambridge Analytica was a UK company that developed a ``behavioral microtargeting'' system that implemented a highly personalized advertising system for every single user. Its managers claimed to be able to leverage not only tastes, as do other similar systems for marketing, but also the emotions of people. A machine learning algorithm was developed by the Cambridge researcher Michal Kosinski, who has been working for years to improve it and make it more accurate. The model was designed to predict and anticipate individuals' responses. Kosinski claimed that there is enough information on 70 ``Likes'' put on Facebook to know about the personality of a subject more than his friends, 150 to learn more about the parents of the subject, and 300 to attain more knowledge of his partner. Cambridge Analytica used this software solution to harvest user profiles and data from Facebook and use them for advertisements and political campaigns. The mobile application that they implemented was used to collect data on Facebook related to the online friends of 270,000 of its users, eventually storing rich information on five million Facebook profiles. Cambridge Analytica was then able to build a huge archive, including information on where users lived, their interests, photographs, public status updates, and places where they reported they had gone. This archive was used to analyze people's lives, and the analysis allowed the manipulation of their political and marketing preferences [\citealt{chap:05:Talia:2019}].

In 2016, Donald Trump's campaign committee entrusted the data collection activities for the electoral campaign to Cambridge Analytica. Jared Kushner, Trump's son-in-law, hired Brad Pascale, an IT expert, who was then contacted by Cambridge Analytica to let him try out their technologies. Steve Bannon, at that time manager of the Trump electoral campaign, supported the usefulness of \hbox{having} a collaboration with Cambridge Analytica, of which he was vice president. We don't know how much the company has worked with software tools, but from the surveys carried out so far, we know that the online pro-Trump activity was very organized and large-scale. Large numbers of fake accounts were created and automatically managed (by ``bots'') to spread posts, fake news, and other content against Hillary Clinton. Their activities were scheduled depending on the progress of the election campaign. The operations were almost always in real time, for example, filling social media with comments during the TV debates between Trump and Clinton, the most anticipated events being followed by the voters. Every day, tens of thousands of advertisements were produced on which to measure the response of online users and recalibrate them, favoring those that worked the best. All these are activities Cambridge Analytica has claimed for years to have great skills and knowledge in. This case shows that everything we do on the web and social media can enable someone to manipulate, influence, and misuse our behaviors and decisions. Every action we take on social media is stored in Facebook, TikTok, Twitter, and Instagram data centers. Machine learning algorithms are then used to analyze the data and take actions that may influence our lives. The great merit of Facebook, Twitter, and other social media is to connect people, to build new relationships and rebuild lost relationships, renew old friendships, revive old loves, and fuel new ones. All these things can make people's lives more interesting, less boring, and less lonely. However, the unethical use of data and algorithms, and also of AI algorithms, may cause disinformation, political struggle, dissemination of lies, spreading of hate speech, and hidden advertising. All these risks are generated by the algorithms they use and must be curbed or eliminated by the use of unbiased and fair algorithms.

\section{\label{sec:5.4}Programming Self-driving Cars}

An amazing application domain of machine intelligence is self-driving vehicles. A self-driving car, also called an autonomous car, is a vehicle incorporating hardware devices and software automation that are capable of sensing the surrounding environment and moving safely with little or no human intervention. Self-driving cars include a large set of sensors to recognize their surroundings such as radar, sonar, laser, thermographic cameras, LiDAR, GPS, odometry, and inertial measurement devices. In particular, LiDAR (light detection and ranging or laser imaging, detection, and ranging) is a very important technology used in self-driving vehicles. It is a device that sends out pulses of light that bounce off an object and are returned to a sensor that determines the object's distance. In this way the LiDAR system produces a three-dimensional representation, which is the way the car perceives its physical surroundings. Developing an autonomous system able to drive a car without human intervention is an extraordinarily complex and multifaceted task. Researchers and designers typically tackle such tasks by developing all the necessary components and then designing their combinations according to the principle of divide and conquer. Although the development of self-driving cars dates back to 1920, it was only in 1985 that the Autonomous Land Vehicle project run at Carnegie Mellon University demonstrated a self-driving car with a speed of 19mph (31km/h) on two-lane roads. This vehicle was soon augmented with obstacle avoidance and off-road driving during the day and at night. Later, several car makers, such as BMW, Nissan, Tesla, Ford, Waymo, General Motors, Toyota, Uber, and Mercedes-Benz, produced prototypes and commercial vehicles. In the spring of 2019, Robocar, an autonomous car specifically designed for car racing, set the Guinness World Record for being the fastest self-driving car in the world, reaching 175.49mph (282.42km/h). Together with cars, some companies have developed self-driving vans, autonomous trucks, and public buses. The most important components of self-driving cars are the AI algorithms that analyze a large set of input data coming from the external environment, sensors inside the car, and car processors. Machine learning algorithms are combined with IoT devices, Wi-Fi interconnection, and image processing devices to control and steer autonomous cars that transport people without a human driver. Thanks to AI solutions, these self-driving cars are capable of sensing the road and environment around them, moving safely with only minimal or no human intervention (see Figure~\ref{fig:5.1}).


Learning algorithms based on gradient boosting are used to decide the best actions of a self-driving car. They decide whether the car needs to brake or accelerate, if it needs to take a right or left turn, if it must be ready to stop when a traffic signal appears. The algorithms are continually executed to handle such situations and make accurate predictions on the basis of sensor data and environment recognition. Other machine learning techniques used in autonomous cars for pattern recognition are neural networks, Bayes classifiers, and \textit{k}-nearest neighbors. Raw image data must be filtered to identify the relevant elements containing the objects in the surroundings. This task is executed by pattern recognition algorithms such as the ones we mentioned. The data captured by the sensors by detecting object edges are analyzed to fit contour segments and circular arcs to the edges. Then, pattern recognition algorithms combine in different ways such as rounded arcs and line segments to form the main features needed for recognizing an object. When the images aren't clear, have very low resolution, or contain few data points, it becomes difficult for the car's computer system to detect and locate objects in the car's surroundings. In these cases, clustering algorithms are more effective than classification algorithm in identifying inherent structures in the data and arranging the data into groups having the greatest similarity. \textit{k}-means, neural networks, Gaussian mixture model, and density-based clustering, such as DBSCAN, are the most widely used clustering algorithms. DBSCAN is a clustering algorithm that finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low density so that it can detect outliers between the high-density clusters. DBSCAN is more effective than \textit{k}-means when it comes to working with oddly shaped data. For this reason, it is used in identifying structures belonging to the surroundings of a self-driving car.


\begin{figure}[!t]
\tooltip{\includegraphics{graphics/Chapter_05/Figure1.\image}}{The drawing shows a sketch of a car with a set of digital facilities such as a wireless connection, park assist, a camera, a radar, LiDAR, and an ultrasound sensor.}[-300pt,3pt]
\caption{\label{fig:5.1}A simple scheme showing the main hardware technologies used in an autonomous car.}
\end{figure}


Regression algorithms are also used for event prediction. They exploit the recurring features of an environment to define a statistical model of the relationship between a particular image and the position of a given object within the image. For example, a regression model can provide speedy online detection through image sampling. Bayesian regression, neural network regression, and logistic regression are the three classes of regression algorithms used in autonomous cars. As neural networks can recognize patterns, they are also used within vehicles to monitor the driver. For example, they can be used for facial recognition to identify people in the car and verify if they have the required rights, such as permission to start the car. They are also used to run facial or body recognition to detect the status of passengers during a trip. Finally, reinforcement learning methods are used to learn the driving policy that automatically outputs control signals for the throttle, brake, and steering wheel based on the car's surroundings. In reinforcement learning, an autonomous agent learns to improve its behavior during an assigned task by interacting with the environment. The goal of the agent is to maximize the cumulative rewards it receives during its lifetime. To maximize the reward it receives, an agent must exploit its knowledge by selecting actions that it knows will result in high rewards. It also has to take the risk of trying new actions that may lead to higher rewards than the experienced best-valued actions. Self-driving tasks where this type of learning algorithm can be applied include static and dynamic path planning and trajectory optimization, scenario-based policy learning for highway motion, development of high-level driving policies for complex navigation tasks, and prediction of traffic agents such as pedestrians, animals, and other vehicles [\citealt{chap:05:Kiranetal:2022}].

The implementation of machine learning algorithms in self-driving vehicles has been demonstrated to be effective and efficient. In many aspects, AI algorithms have been proven to be sufficiently smart to substitute humans in driving cars; however, in general, autonomous driving is still a challenging task for researchers and developers. The dream of self-driving cars is significantly maturing, and, because human error accounts for more than 90\% of all accidents, self-driving vehicles may drastically reduce traffic mishaps. Automakers from the US reported nearly 400 crashes involving (partially) self-driving cars, including 273 involving Teslas, from July 2021 through May 2022. This means that although machine learning algorithms are very good at driving cars, they are not perfect. This is expected both because they are designed by humans and because it is impossible for algorithms to prevent every dangerous event that may occur in our daily lives. We have not discussed here the ethical implications of using AI in autonomous cars. But because this is a very important question, we will discuss its implications in the next chapters.

\section{\label{sec:5.5}Intelligent Algorithms at School}

Machine learning and AI models are entering schools and universities and are transforming education, changing teaching methodologies and learning and training processes. Algorithms are used to attract the best students for admissions and predict enrollment to optimize resource use and capacity. There is a global effort to use AI applications in all education processes. Data analysis methods and machine learning tools are, for example, transforming how teaching institutions track pupil performance. They are enabling teachers to adapt learning procedures to different groups of students. All these activities are opening new opportunities and helping to improve the activities of schools and universities. However, the education domain is a complex field where automatic procedures may generate negative effects if they are not carefully developed and applied. AI is used in campuses to improve the teaching efficacy of lecturers and the efficiency of student evaluation and grading. In these areas, there have been several occurrences where learning algorithms were trained with biased or incorrect data or when assessment parameters were not correctly used. An extreme use of image learning technology is the one performed at the Hangzhou No. 11 High School in China. In China, facial recognition technology is widely used for various applications such as predicting crime, monitoring public events, controlling traffic in big cities, and supporting the implementation of the Social Credit System (discussed in Chapter~\ExternalLink{chap:9}{9}). Through this extensive use of facial recognition technology, the Chinese authorities are putting in place comprehensive and pervasive surveillance and behavioral engineering on a large scale that limits the rights and privacy of citizens: a sort of Big Brother capable of controlling every moment and every behavior of individuals and implementing a substantial threat to people's freedom.

The Hangzhou school is using a machine learning application (they called it a ``smart classroom behavior management system'') for student face recognition to monitor and analyze how students behave during lessons and respond to the teacher. The AI system provides real-time data on students' emotional responses to the teachers. The system monitors the faces of students and classifies them into categories such as angry, sad, happy, afraid, and others. The system registers the actions students perform such as reading and writing. The declared goal is to improve the teaching process and adapt it to the student's needs. However, this kind of system has raised concerns about student privacy and rights. The school principals declared that the students accepted the monitoring system; however, many people in China fear that authorities are working to build a surveillance state that keeps track of citizens and will be used to limit freedom and suppress dissent.


Several universities from different countries are using machine learning \hbox{models} for student admission and career monitoring. They analyze data to identify and attract good students, estimate the matriculation rate, and forecast their results. For example, advanced learning methods can support universities in identifying the best students' counties or cities, high schools, or zip codes. They focus on reaching potential students who are most likely to be appropriate for the institution. Moreover, periodically analyzing data from the class tests and intermediate exam results, algorithms can suggest which students are going to perform well during the year and which students may experience challenging times. In fact, learning algorithms can be used to identify students at risk of dropping out or not graduating on time. Learning models to be applied in these cases are decision trees, adaptive boosting classification, and logistic regression, which can be exploited to classify students on the basis of their performance. The main goal of using machine learning to identify students at risk is to predict their final results based on a set of training data that may include student performance results and historical data from previous years. For example, in Illinois, the State Board of Education has developed an early warning system for at-risk high school students that uses some of the mentioned machine learning algorithms, and also to make recommendations on interventions that teachers should implement to get students back on track. This can be done with decision tree-based classifiers that build a model for each group of students and provide the main factors that characterize each group. Another example is from Western Governors University in Utah, where learning models are used to improve student retention by identifying vulnerable students and developing early-intervention projects. According to a university report, initial work increased the graduation rate for the university's undergraduate program by 5\% between 2018 and 2020.

There are other real-world cases that show how machine learning techniques working on student data help education institutions reveal deep understandings of their students and identify and mitigate risks. However, while many higher-education institutes have started the journey to harnessing Big Data and AI techniques, there is still a long and bumpy road to exploiting the full potential of those technologies, trying to avoid the risks arising from the trivial, improper, or biased use of data and algorithms. Data scientists must consider that running machine learning tasks is a complex process, with great potential risks such as the insertion of biases in the analyzed data or in the learning algorithm coming from an improper use of student origin, race, age, or gender. The suggestions or recommendations generated by machine learning methods should not entirely replace existing procedures and approaches. They should be used as additional insights to improve decisions and interventions. Finally, discrimination against students supported by algorithms must definitely be avoided. The performance of each learning model for different student groups must be inspected to ensure it performs similarly for all groups and is not skewed toward any particular subset of them. Unfortunately, this is not always true. A notable example is the 2020 final exam at the International Baccalaureate (IB), a two-year high school in Geneva, Switzerland, that offers an international high school program. Due to the SARS-CoV-2 pandemic crisis, the school decided not to hold the final exam in May of that year. The school informed students that the final results were based on ``student's coursework and the established assessment expertise, rigor and quality control already built into the programmes.'' The student's final scores were conferred by a machine learning algorithm that sadly failed, despite being supposedly based on the coursework and predicted grades of the schools. The main reason for the failure originated in the limited and incorrect data about student careers, teacher-corrected coursework, and data from previous years. Many students declared they received suspiciously low scores, which negatively affected their future education plans, and their parents carried out a furious protest campaign.

The IB case is not isolated as other incorrect uses of prediction algorithms and AI methods have occurred. Another case arose in the UK where, after receiving their grades, many students in London protested outside the Department of Education, claiming their results fell far below expectations and that the results were flawed. Like the IB school, the UK education authorities, due to the urgency of the situation caused by the pandemic, decided to rely on algorithms to compute the students' grades. Machine learning methods were used to fit the distribution of grades awarded in 2020 to the pattern of previous years. A possible source of anomalies came from a decision to try to match the distribution of grades from prior years on a school-by-school basis. This strategy penalized solitary good performers in classes with several poor performers. They encountered large downgrades mainly due to an improper use of data that hadn't been accurately selected and a very weak control of the accuracy of the algorithm results. After numerous complaints, and also due to the risk of many students being denied entry to university, the UK authorities reversed course and decided to use the unmoderated assessments to award grades [\citealt{chap:05:Edwards:2021}]. This was a very negative case originating in the sloppy use of AI methods that resulted in erroneous school decisions and affected thousands of students. Together with other cases, it shows that machine learning techniques must be used very carefully in education, and should not be expected to take the place of teachers.

However, adding AI systems as additional teachers in the classroom is being considered. A team of researchers at Pennsylvania State University developed a natural language processing system that generates a summary report on topics or concepts present in the essays compiled in classrooms, which means teachers can rapidly determine which students have really understood a science lesson. To evaluate essays, the AI system first breaks down students' sentences into individual clauses and then converts them into data structures called semantic vectors. To capture the meaning of clauses in their conversion to vectors, an algorithm called weighted text matrix factorization is used to decompose a big data matrix into two smaller matrices that are easier to compute and minimize the resulting approximation error. The natural language processing algorithm lists the distinct units of content found in several reference summaries, then weights content units by how many reference summaries they occur in, and produces scores based on the weighted content of new summaries. At the end, the system builds a graph where each node is a segment vector and edges connect segments from different summaries with a certain similarity. Node connections represent matches of a student vector to rubric vectors so that the software can find the optimal interpretation of the student essay. This interpretation is provided to\vadjust{\vspace*{10pt}\pagebreak} a human teacher who quickly assigns a grade to the student essay. Even if in this case the final decision will be made by a teacher, the idea behind this approach is to substitute for teachers in evaluation tasks that can be tedious but must be carried out accurately so as to understand the students' ability and gaps. Although students can quickly receive a grade through the algorithm analysis, teachers who did not carefully read the essays would not be able to explain to students how to improve their knowledge.

\section{\label{sec:5.6}Algorithmic Art}

In 1995, Jean-Pierre H\'{e}bert coined the term ``algorist'' to designate artists who create art using one or a set of algorithms that also include their own algorithms. To clarify the term, after the annual SIGGRAPH conference in Los Angeles, H\'{e}bert wrote the following simple algorithm that determines if a person is or not an algorist.

\medskip
{\arrayrulecolor{black}
\begin{tabular}{|l|}
\hline
\texttt{if (creation \&\& object of art \&\& algorithm \&\& one's own} \\
\texttt{algorithm) \{}\\
\quad \texttt{return ``an algorist'';}\\
\quad \texttt{\}}\\
\quad \texttt{else \{}\\
\quad \texttt{return ``not an algorist'';}\\
\quad \texttt{\}} \\
\hline
\end{tabular}}

\medskip
Algorists are creators of so-called ``algorithmic art,'' which can be considered a new visual art in which the creation action is generated with the support of algorithms. From the early years of electronic computers, several artists were interested in using software systems to generate art. However, many artists who used a computer to implement their creative ideas did not necessarily use a script or code to produce their work. The algorists, instead, were fully committed to investigating and using the algorithmic approach to create artwork. During a panel at the 1995 SIGGRAPH conference, H\'{e}bert recalled the mathematician's debate between the algorists and the abacists. This \hbox{dispute}, which lasted through the medieval period and into the Renaissance, focused on the best system to execute basic arithmetic, that is, through a sequence of operations with Indo-Arabic numerals (the algorist approach) or the abacus---an instrument for performing calculations by sliding counters along rods or in grooves---and Roman numerals. The algorists' ideas represented a similar sense of division between artists who coded and artists who \hbox{did not.}

Art created by algorists is a section of generative art in which an artist designs traditional or AI algorithms that are executed by a computer and are able to create complex artwork. This kind of artistic practice has the goal of showing how creativity can be exploited through the use of software and hardware systems. The artist does not materially construct the work, but the algorithm they are able to conceive and code in a programming language produces, for example, a painting or a sculpture. Sometimes, the algorithms designed by an artist are deterministic, so the execution of the algorithm leads to a result that can be predicted \textit{a priori}. In other and more interesting cases, the artists introduce pseudo-random or non-deterministic operations that allow the production of unexpected effects in the artwork during the execution of the program. Examples of algorithms used by artists who code programs to produce artworks are fractals,\footnote{A fractal is a geometric shape generated by mathematical functions containing detailed structure at arbitrarily small scales. Many fractals appear similar at various scales; the repetition of similar patterns at increasingly smaller scales is a property of fractals that is called self-similarity.} genetic algorithms,\footnote{Genetic algorithms are heuristic procedures inspired by the process of natural selection that are commonly used to generate high-quality solutions for optimization and search problems by relying on biologically inspired operators such as mutation, crossover, and selection. They are also used today to solve some machine learning tasks.} and cellular automata.\footnote{Cellular automata are discrete methods of computation based on a grid of cells each in one of a finite number of states. The grid can be in any finite number of dimensions. For each cell, a set of neighboring cells, called its neighborhood, are defined. All cells evolve over time according to an evolution rule that uses as input values the cell and its neighbor's state.} With the emergence of AI, these and other complex computing models are used by artists to create artworks. In particular, as AI and machine learning have been introduced and developed in artistic domains, digital art solutions based on these approaches have become more complex. New software tools and libraries for artists to experiment or enhance their creativity in unforeseen ways have become available. AI-based solutions can be used to help artists better express their creativity; however, they can also be applied to generate something new that an artist would never have imagined before. When machine learning algorithms are coded to compose artistic work, the contribution of the algorithm is more autonomous and its role in art creation is at least similar to or higher than the artist's contribution. This occurs because AI algorithms learn from data and generate a model (i.e., a software program) that creates the artistic result. We can say that a machine trained with a (large) set of input data is able to produce a program that is capable of generating new paintings, sculptures, or original videos without an imperative command from the artist. For example, the use of deep learning networks in art produced very original paintings that, at the same time, raised doubts about this new way of creating art but offered artists new capabilities for expanding the realm of artistic creation known so far.

The examples of artwork produced by using AI algorithms are numerous. An example is the machine learning artwork \textit{Portrait of Edmund Belamy} created by the French arts-collective Obvious and sold in October 2018 at a Christie's auction for US\$432,500. The algorithm was based on a generative adversarial network (GAN) that was trained on a set of 15,000 portraits from the 14th to the 19th century obtained from WikiArt, the online art encyclopedia. GANs are unsupervised deep learning methods that automatically discover and learn regularities or patterns in input data in such a way that the generated model can be used to produce new examples that possibly could have been drawn from the original dataset [\citealt{chap:05:Goodfellowetal:2014}]. These kinds of adversarial networks implement learning algorithms by using two neural nets fighting against one another (thus the ``adversarial'' adjective) to generate new, artificial instances of data (a painting, for example) that are very similar to real data (existing paintings). For this reason, they are widely used in image creation, video production, and voice generation. One of the two neural networks, called the generator, produces new data instances, while the other, the discriminator, evaluates them for authenticity. To do that, the discriminator decides whether each instance of data that it reviews (e.g., an image) belongs to the actual training dataset or not. In more detail, a GAN takes three steps: (i) the generator takes in random numbers and returns an image; (ii) the generated image is fed into the discriminator together with a stream of images taken from the actual, ground-truth dataset; and (iii) the discriminator analyzes both real and fake images and returns a probability, that is, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake. This is a very interesting AI method to create new artwork. It is not the only one; in fact, the list of the so-called AI art generators is long and includes tools such GANBreeder, Magenta, ml5.js, DALL-E2, and AI Painter.

As usual, when new disruptive technologies are introduced, a lot of critics and worries arise. Some people think artistic AI is just a new instrument for artists who like to expand their creativity and are open to novel solutions. Others believe that artworks produced by using AI techniques and models are not real art and will destroy art because automated mechanisms were introduced in this imaginative field. The fear of many people is that this new intelligent technology will replace humans in art production and in the future there won't be any need for human-based art. All these opinions contribute to the public debate; however, we cannot ignore the fact that, through learning algorithms, machines are becoming creative. Perhaps they are not aware of what they are doing, but in some respects they are able to paint, write, and compose as well as humans. In many cases, human artists are not fully aware of the creative process that produces an artwork. This is also true in the case of intelligent art machines driven by learning processes that are coded by humans and give machines new abilities that can also be valuable in the artistic field.


\begin{thebibliography}{}
\bibitem[Basu et~al.(2019)]{chap:05:Basuetal:2019} K. Basu, T. Basu, R. Buckmire, and N. Lal. May. 2019. Predictive models of student college commitment decisions using machine learning. \textit{Data} 4, 2, 65. DOI:~\href{https://doi.org/10.3390/data4020065}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}3390/{\allowbreak}data4020065}.


\bibitem[Edwards(2021)]{chap:05:Edwards:2021} C. Edwards. June. 2021. Let the algorithm decide? \textit{Commun.} \textit{ACM} 64, 6, 21--22. DOI:~\href{https://doi.org/10.1145/3460216}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}3460216}.

\bibitem[Goodfellow et~al.(2014)]{chap:05:Goodfellowetal:2014} I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. 2014. Generative adversarial nets. In \textit{Proceedings of the 27th International Conference on Neural Information Processing Systems} (NIPS'14). ACM, Montreal, 2672--2680.


\bibitem[Kiran et~al.(2022)]{chap:05:Kiranetal:2022} B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. K. Yogamani, and P. Perez. 2022. Deep reinforcement learning for autonomous driving: A survey. \textit{IEEE Trans. Intell. Transp. Syst.} 23, 4909--4926. DOI:~\href{https://doi.org/10.1109/TITS.2021.3054625}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}TITS.{\allowbreak}2021.{\allowbreak}3054625}.


\bibitem[Page et~al.(1999)]{chap:05:Pageetal:1999} L. Page, S. Brin, R. Motwani, and T. Winograd. 1999. \textit{The PageRank Citation Ranking: Bringing Order to the Web}. Technical Report 1999-66, Stanford InfoLab.

\bibitem[Pariser(2011)]{chap:05:Pariser:2011} E. Pariser. 2011. \textit{The Filter Bubble: What the Internet Is Hiding from You}. Penguin Press, \hbox{New York.}

\bibitem[Talia(2019)]{chap:05:Talia:2019} D. Talia. 2019. \textit{Big Data and the Computable Society: Algorithms and People in the Digital World}. World Scientific Publishing Europe, London. DOI:~\href{https://doi.org/10.1142/q0206}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1142/{\allowbreak}q0206}.

\end{thebibliography}

%\end{document}

