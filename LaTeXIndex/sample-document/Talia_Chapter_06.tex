%\documentclass{acm-book-v2}
%\RequirePackage[errorshow]{tracefnt}
%%\newcommand{\mpage}[1]{}
%%\newcommand{\indexfn}[1]{}

%%\usepackage{showframe}

%\usepackage{custom-tooltip}
%\usepackage{custom-tooltip-Alt-Text-View}



%\begin{document}

\setcounter{chapter}{5}

\chapter{\label{chap:6}Working with Algorithms}

{Computers and algorithms have changed the way people work. Through the wide availability and the pervasive use of computers and digital devices and through continuous interaction with software applications, humans can increase work efficiency and results. At the same time, algorithms, computers, and robots are replacing workers in many jobs and are playing a key role in factories and offices, forcing humans to face new challenges. By working every day, and potentially in every place, with computers connected to the Internet, individuals have become ``everytime workers.'' This chapter examines the relationships among algorithms and human work. The chapter also discusses how jobs are changing and how computers and their software systems are influencing the labor sphere and our working  activities.}

\section{\label{sec:6.1}Machine Intelligence for the Fourth Industrial Revolution}

Computers and algorithms have produced a significant change in the way people work in the new millennium. Digital technology is transforming jobs and will continue to rapidly change the working world. Changes in production processes have affected traditional places of production (offices, factories, plants, construction sites), where the use of intelligent digital machines (computers, robots, sensors) is revolutionzing organizational and working methods. Additionally, these changes have generated new professional roles and defined new design and production processes for goods and services in all economic sectors. After steam power, electricity, and automation, the industries of the 21st century will be entirely digital. Computerized machines interact with each other using the IoT paradigm. Robots are now performing many functions that in the past were carried out by humans. Smart sensors, three-dimensional (3D) printers, and AI software will be extensively used, and every machine and object will be interconnected through the Internet and communicate by exploiting edge devices. Cloud data centers store most of the data for production processes, and an entire process can be simulated and monitored digitally.

All these technologies have contributed to creating a fourth industrial revolution. In fact, the term ``Industry 4.0'' was coined to describe this new revolution. It refers to the smart networking of machines and processes for industry with the support of new information and communication technologies such as IoT, cloud computing, advanced robotics, data analytics, machine learning, and other AI solutions. The term was publicly introduced at the Hannover Fair 2011 by German Chancellor Angela Merkel during her opening speech. Using the IoT, cyber-physical systems communicate and cooperate with each other and with workers in real time both internally and across organizational services provided by participants of the Industry 4.0 value chain. The global Industry 4.0 market is expected to grow from US\$130 billion in 2022 to US\$380 billion in 2029. Those numbers indicate the potential impact of digital technologies on industry in the coming years. They will also impact workers, their skills, and job transformations including new job creation and job loss due to smart automation.

Thanks to machine learning and AI techniques, it is possible to introduce adaptive procedures in industrial processes and make machines and robots capable of autonomous learning from data. This approach allows enterprises to improve their performance, optimize processes, reduce faults, improve product quality, and decrease production costs. Indeed, one of the most important contributions of machine intelligence techniques is that they can enable machines and robots to learn from the data they collect from the activities they carry out and from data coming from other similar devices used in analogous production processes. Intelligent data analysis and machine learning allow manufacturing firms to take advantage of data and information generated across their production units or from other partners working in the same area. Predictive maintenance, energy consumption reduction, and robotic assembly line management based on machine learning algorithms are only a few examples of the significant fields where predictive algorithms such as outlier detection, association rules, and deep learning networks are providing significant improvements. Traditional industrial robots are programmed with imperative procedures that are not adaptive and flexible. In some cases, even when they perform complex tasks, they are not able to modify their actions in response to a change in the external environment. Therefore, the robot's operations must be reprogrammed when changes need to be introduced. When inductive learning is exploited in the software controlling a robot, it may adjust its behavior according to the data it receives. Collaborative robotics, where robots work in tandem with human workers, is a significant example of intelligent automation that benefits from the use of machine learning techniques. In this case, data produced in the environment created by robots, sensors, and humans feed the software of robots, which is able to adapt/optimize the movements and \hbox{decisions} of the machines it handles to improve productive processes and/or to avoid risky moves or faults.

In mechanized industry, and also in other sectors such as agriculture and transportation, the use of machine intelligence has introduced many changes. One of the most noticeable technological novelties in transportation comes from the prediction of route transit times provided by navigation applications and journey services. Although the final goal is to have autonomous trucks, today machine intelligence works on connected vehicles together with sensor networks, onboard cameras, and Wi-Fi connections to identify traffic signs, receive data from congested roads, and adjust routes to allow vehicles to reach intermediate and final destinations in the shortest time and in the best conditions. Agriculture~4.0 is the term coined after Industry 4.0 to describe the actions that are performed in agriculture by leveraging Big Data analysis, machine learning, sensor networks, and IoT solutions. Many smart farming machines are taking over fields and carrying out harvesting work with very limited--or entirely without--human intervention. Deep learning, computer vision, and pattern recognition methods are widely used technologies in precision agriculture. Big Data and machine learning algorithms offer farmers a deeper knowledge of all elements that contribute to their production and increase insights that allow growers to have a more precise view of their operations. Agriculture~4.0 supported by large data collection and analysis helps to identify the components of a farm that must be optimized or soil features to be improved. Intelligent software and devices are making it possible for farmers to better understand terrain conditions, their usual practices, and what changes must be implemented to improve processes and results.

\section{\label{sec:6.2}Everytime Workers}

With the use of digital machines, work time has increased to potentially cover the 24 hours of the day and the seven days of the week. The time you work in the digital society becomes work without any time limit. Workplaces have expanded to cover the entire globe and even the sky. The use of digital devices anytime and everywhere allows people to carry out personal (communication, sharing, information, transactions) and work functions with ubiquitous space--time modalities as never before in human history. This is the innovative work of every place and therefore work without a place. Wherever people are, standing still in an office or moving between cities or continents, they can work through their digital prostheses. Sometimes this opportunity brings great benefits to humanity. At other times, pervasive work becomes a high-tech tyranny from which we can't escape, partly because we lack a strong enough mindset and the necessary anthropological maturity to do so.

While the work that people do with digital machines now covers every space and every time, these machines are replacing individuals in different types of work, reducing job and salary opportunities for workers and employees and raising major questions about the future of automated work. Machine intelligence has helped to replace humans in many types of jobs. Automation has made companies more productive, but the productivity gains essentially go to the owners of firms, not to workers. Erik Brynjolfsson, the director of the Stanford Digital Economy Lab, argued that an extreme emphasis on designing and deploying human-like intelligent systems can lead us into a trap. He called it the Turing Trap, coming from the idea that machines must substitute humans, not augment them [\citealt{chap:6:Brynjolfsson:2022}]. As machines become better alternatives for human labor, workers lose economic and negotiating power; they then become increasingly dependent on those who control the technology. In contrast, when AI solutions are focused on augmenting humans rather than imitating them, then people retain ``the power to insist on a share of the value created.'' According to Brynjolfsson, the solution is ``not to slow down technology, but rather to eliminate or reverse the excess incentives for automation over augmentation.'' The question of what roles humans will play in the near future when employment rates will decrease has also to consider the number of robots and AI software tools used in place of humans. The robot employment rate is a measure that is not yet considered by economists and statisticians; however, in the next decades that rate may be a key factor in estimating the gross domestic product of nations.

Professor David Autor, a leading labor economist at MIT, studied the relationships between human work and machine intelligence. Autor analyzed Polanyi's paradox about tacit knowledge and the effects of AI and robotics on the future of work and employment growth [\citealt{chap:6:Autor:2014}]. The basic question can be expressed in very simple terms: Does automation destroy more jobs than it creates? This question has been continuously debated for at least two centuries. The answer given by David Autor is original and interesting. It is original because it really considers the latest actual developments in the field of AI. It is also interesting because Autor chose neither of the two best-known answers: the pessimistic one that argues that digital technologies increase unemployment because digital machines can replace people in many jobs, and the optimistic one that hypothesizes that information technology creates more jobs than it destroys. Autor's thesis is that many scholars have overrated the possibility of replacement of human labor by digital machines and have ignored the strong complementarity between the roles of human work and automated work. This thesis is carefully argued by Autor when he analyzes the so-called Polanyi paradox that declares ``we can know more than we can tell.'' This means that the tacit knowledge we possess of how the world works and of own our capability is beyond what we can explain [\citealt{chap:6:Polanyi:2009}].

As computers, robots, and AI systems are programmed by us to execute simple or complex algorithms, we cannot transfer through algorithms our ``tacit'' knowledge that is interrelated to the personal experience of each individual. From this assumption, David Autor infers that, while digital machines will increasingly execute very long sequences of simple and repetitive functions, they will not receive all our know-how. Therefore, both manual work in which human interaction is fundamental and high-level jobs that require complex knowledge and high intellectual ability cannot be entirely done by robots and software applications. These tasks cannot be done entirely by digital machines, and therefore human workers who do those types of jobs have nothing to fear. ``Humans naturally tackle tasks in a manner that draws on their inherent flexibility, problem-solving capability and judgment. Machines currently lack many of these capabilities, but they possess other facilities in abundance: strength, speed, accuracy, low cost and unwavering fealty to directions'' [\citealt{chap:6:Autor:2014}]. If Autor is right, the risk of job loss is higher for workers and employees of the middle class who carry out routine jobs that will progressively decrease as they are taken over by machines and algorithms. In short, it is mainly the middle class who will have to worry about the advancement of digital automation and the use of machine intelligence technologies in the coming decades. Conversely, the possibility of machines replacing people in jobs requiring adaptation, dexterity, and creativity is still low.

David Autor is aware that universities, research centers, and big IT companies are running many projects to build high-performance computers and machine intelligence software systems that may allow the Polanyi paradox to be overcome through the analysis of massive datasets by machine learning techniques that are able to apprehend from examples and accumulate knowledge from experience to the point of learning tacit knowledge, as we humans do. He wrote, ``The challenges to substituting machines for workers in tasks requiring adaptability, common sense, and creativity remain immense. Contemporary computer science seeks to overcome Polanyi's paradox by building machines that learn from human examples, thus inferring the rules that we tacitly apply but do not explicitly understand.'' If AI techniques and their implementation efforts do reach new successes because of learning algorithms trained on massive amounts of data, not only will industrial workers, employees, secretaries, and truck drivers who do repetitive work risk losing their jobs, but many other categories could be affected by this revolution of the world of work. However, despite impressive advancements, in many complex tasks AI is not that close to humans.\vadjust{\vspace*{10pt}\pagebreak} Therefore, although in some real cases \hbox{algorithms} that analyze large amounts of examples may know more than we can tell, intelligent systems are not yet able to completely emulate human knowledge and behaviors.

\section{\label{sec:6.3}Working with Robots}

The term robot is derived from the word ``\textit{robota},'' which in the Czech language means slave, servitude, or forced labor. The term was proposed in 1920 by the brother of the Czech writer Karel \v{C}apek, who used it in his play \textit{R.U.R.} (\textit{Rossum's Universal Robots}) to indicate humanoid-shaped automatons that rebel against their master to claim freedom. After a century, robots have moved from theater plays, movies, and science books to enter our daily routine. Over the past decades, they have become increasingly advanced because of the complex algorithms they execute and today can be found everywhere from car factories to operating theaters and restaurants. Data from the International Robotics Federation show that the number of industrial robots has tripled over the past decade, with about four million robots in use across various industries by the end of 2022. China is the world's largest industrial robot market and the other four major markets for industrial robots are Japan, the US, the Republic of Korea, and Germany. These five countries account for 78\% of global robot installations.

A robot is an autonomous system that includes sensors to recognize the environment around it and has been programmed to act to achieve some predefined objectives. A robot is an autonomous system because it is not controlled by humans but operates on the basis of its own decisions, which depend on its hardware/software logic and signals coming from the external environment. Through its sensors and the algorithms that it runs, a robot obtains information from the surrounding environment and acts by responding to the needs of the environment. Based on the evolution of robots, we can distinguish three main types:

\begin{itemize}
\item First generation robots: non-autonomous machines configured to perform a sequence of pre-established operations.

\item Second generation robots: autonomous machines that, based on information taken from the environment and received from machine learning software, are able to make decisions for completing their assignments. When unexpected events occur, they are able to find solutions and reach their established goals.

\item Third generation robots: autonomous machines that, through the use of machine intelligence techniques that allow for the autonomous generation of automatic learning algorithms, are able to pursue their goals.
\end{itemize}

Autonomous robots embody learning techniques. Therefore, they are able to adapt and improve their behavior and performance as they learn from data coming from their daily operations. Thanks to environmental and operational data detected by their sensors, autonomous robots are capable of self-learning and behaving in an increasingly advanced manner. Over the years, robotics has specialized in various fields, of which industrial robotics is the most developed. Other application sectors are surgical and service robotics. Industrial robots are the most widespread automata; their appearance dates back to the 1970s and this area is constantly evolving. They are mostly used to automate repetitive work and in recent years have been designed to be functionally autonomous and interconnected. In an even more futuristic perspective, industrial robots, thanks to the use of deep learning and AI models, will be able to learn and make decisions autonomously. The most common operations that they perform are data acquisition and analysis, simulation, monitoring, maintenance, and diagnostics.

In the field of surgical robotics, a doctor has the task of controlling the robotic arms that operate on the patient. This type of robotics is used for urological, cardiological, thoracic, orthopedic, and gynecological surgery. Surgical robots intensively use image processing techniques. For instance, the endoscope in the latest-generation robots has 3D visualization features that allow doctors to scan \hbox{particular} areas in real time. The advantages of robotic surgery are the lower average recovery time and less post-operative pain due to reduced invasiveness. Furthermore, in oncological cases, thanks to the precision adopted in the surgical phases, chemotherapy cycles are accelerated. Shorter hospitalization periods are a positive result; however, some negative aspects of using these robots are the lack of a tactile component, maintenance cost, and the processing cost of the 3D images.

Service robotics uses robots to assist human beings at home, in hospitals, or in laboratories. Application fields that have benefited from using service robots include logistics and agriculture. For example, a service robot can manage a warehouse in an automated manner, thereby minimizing human intervention. These robots are also used in agriculture, such as drones that through special sensors can perceive and analyze field data to plan a more rational use of resources and consequently increase the profitability of the land used for crops.

A class of robots that is gaining a significant role recently is known as collaborative robots or cobots. These are machines used to satisfy the need for collaboration between robots and people. The first definition of a cobot was given in 1996 and then appeared in the 1999 US patent (US5952796A) filing for ``an apparatus and method for direct physical integration between a person and a general-purpose manipulator controlled by a computer.'' In the inventors' goals, a cobot is a machine designed not to replace a worker and take their job but to help them do the job more efficiently and with a reduced risk of injury. Collaborative robots today exploit AI techniques to analyze and collect data. Through these techniques, a cobot is able to perform qualitative functions and decide independently how to behave in a specific situation. AI-based cobots, for example, may detect changing environmental conditions and optimize their operations. They use machine learning algorithms to identify patterns in ongoing operations and apply the acquired insights to reduce risks or obtain a better performance. As an example, industrial cobots use deep learning models to identify coworkers and communicate via gestures; they are also able to learn new tasks by observing and imitating humans.

To increase the level of autonomy of robots, it is crucial to integrate AI techniques into robotic systems. Through the use of machine learning algorithms, a robot can plan a task, touching and manipulating objects based on what it has learned. Robots that have this aptitude are now numerous, and they know how to perform many actions autonomously. Another particular use of AI in robots is to give them perception of the external environment through sensors and artificial vision that allows them to interact with other entities in the environment. This discipline is referred to as social robotics and covers two domains: cognitive robotics and human--robot interaction. The goal of cognitive robotics is to imitate the human cognitive system and therefore be able to learn and acquire knowledge. In the area of human--robot interaction, robots have been developed that are able to understand activities and emotions and are capable of moving in environments together with human beings. Finally, of note is the role that robots play in carrying out high-risk tasks for humans such as defusing bombs or detecting leaks in underground gas pipelines.

The combined use of AI and industrial robotics techniques involves a change in companies and their organizational structure. These techniques could alter the tasks and skills of many occupations, which is why companies are forced to restructure and reorganize themselves. Occupations within companies change as the use of robotics in many activities become automated and many people tend to be replaced by robots. Therefore, to adapt to new skills, the composition of the workforce could undergo changes. An advantage, within companies, of the use of AI combined with robotics lies in the productivity increase, and in the reduction of labor and product costs. However, a disadvantage could lie in the cost because the investment required to implement autonomous systems is excessive compared to the existing labor costs and the unemployment that the use of these intelligent machines could entail. AI is contributing to an improvement in robotics and their use in various fields and sectors. Therefore, it seems difficult to think of a future in which these two do not fully converge.

As robots are doing many jobs in warehouses, factories, and hospitals, and they are trusted with more complex tasks, they need to be able to learn how to interact more with real world environments. As of 2022, Amazon utilizes more than 400,000 mobile drive unit robots that are working together with human employees at fulfillment centers worldwide. In June 2022, Amazon introduced Proteus, its ``first fully autonomous mobile robot'' used to move large carts throughout its warehouses. Proteus can navigate around human employees, unlike some past Amazon robots that were generally kept separated in caged areas. When a human steps into its path, Proteus stops moving, then resumes after the person moves away. Although the company declared it's not looking to build robots instead of hiring people, the hundred thousand robots used by Amazon have replaced human workers in many tasks and created a type of hybrid working environment in which humans and robots work together and share common ground. Amazon claims its new robots such as Proteus could actually help improve safety in its workspaces. However, worker injury rates at the company's warehouses are double the industry rate. According to a report by advocacy group Strategic Organizing Center, in 2021 Amazon suffered 49\% of the injuries for the entire warehouse industry while only making up a third of US warehouse workforce. Amazon employees have declared it is not the work itself that is especially dangerous but rather the hard pace the company's automated systems demand. This scenario of the highly automated warehouse puts humans more at risk for injuries because work rates are influenced by the introduction of a large number of robots.

Although robots have been used in manufacturing for over 50 years, they are now being integrated into workplaces at unprecedented rates, driven by advances in machine learning and smart robotics technology that pose challenges to human workers. In this changing environment, human--robot interaction must be carefully investigated and deployed while also involving workers in the design process of robot--human integration. This should be done to avoid a sort of machine-driven inhumane approach to robotics integration and to eliminate the possibility of having workplaces where humans must defend themselves from a plethora of robots that are free to move everywhere. The use of machine intelligence algorithms in autonomous robots must mainly be devoted to facilitating human work and not to guarantee that workers keep pace with robots.


\section{\label{sec:6.4}AI Algorithms and Gig Workers}

The term ``gig economy'' has emerged in the past decade to describe new forms of ``on-demand'' work in which independent or freelance contractors receive short-term tasks from companies [\citealt{chap:6:Schor:2020}]. A gig worker is a\vadjust{\vspace*{10pt}\pagebreak} sort of on-call worker who is often contracted via smartphone apps and paid per service, generally through formal agreements with online companies. The origin of the ``gig'' word is uncertain, some sources suggesting that it was originally used in the slang of jazz musicians as a synonym for ``job.'' Today most gig workers in the world have no permanent employment contracts and receive none of the traditional benefits of a minimum wage, guaranteed hours, holiday and sick pay, and job security. However, some countries have recently introduced laws and regulations to provide gig workers with some of these benefits. Gig work enables companies to employ labor only when they need it, at particular moments when specific tasks are available for completion. This lowers labor costs for the company but increases the precarity of workers, particularly those in low-skill occupations. In theory, gig workers are freer and more flexible than other traditional workers. Riders and drivers can decide on a minute-by-minute basis whether to provide their work, allowing them more flexibility in scheduling other daily activities. However, the activities carried out by gig workers are greatly influenced by algorithms that compute task assignments, time to complete, rate of pay, and delivery route. All these key elements are defined and assigned through the algorithms that compose the software platforms of gig firms such as Uber, Lyft, Deliveroo, DoorDash, and Just Eat.

In the gig economy, algorithms play a key role because they collect data and information about client requests and service execution, make decisions about work assignment and execution, and evaluate worker performance and service quality. These kinds of algorithms act like automated managers that supervise, control, and govern the activities of a (large) number of workers by continuously tracking, making decisions, and evaluating their behavior and performance [\citealt{chap:6:Mohlmannetal:2021}]. The term ``algorithmic management'' was coined by \citeauthor{chap:6:Leeetal:2015} [\citeyear{chap:6:Leeetal:2015}] to describe the ways algorithms and tracked data are used by Uber and Lyft to make decisions on how workers are assigned, optimized, and evaluated. Algorithmic management is a process that typically includes:

\begin{itemize}
\item Large data collection and monitoring of workers through digital devices.

\item (Semi-)automated decision-making processes based on data analysis and machine learning techniques.

\item Worker performance evaluation based on real-time and historical data and their rating with respect to predefined objectives and/or performance of other workers.

\item Provision of incentives and sanctions to indirectly regulate workers'\break behavior\vadjust{\vspace*{10pt}\pagebreak}.
\end{itemize}

Data are obtained from GPS, accelerometers, cameras, sensors, and text and audio devices that can record workers' movements, behaviors, and speech. All this information is analyzed to provide evidence of worker observance of or departure from working routines. Biometric data are also used to verify employee identities, collect data on emotional and physiological indicators, or screen for drug and alcohol use. Text mining, image processing, recognition techniques, and classification algorithms may be regularly used to assess workers' mood, efficiency, production time, and throughput. Software platforms owned by gig companies can calculate and share data and information with managers in real time and, if needed, also with workers. In this way, continuous reaction and assessment can be integrated into the production processes. For instance, Uber uses personalized data, such as braking and acceleration speed, to analyze whether workers are driving erratically. Then an algorithm is used to recommend when they might need to rest. In many cases, such recommendations come in the form of ``nudges'' that are integrated into software platforms and workers cannot ignore them. Uber has engaged in individualized and real-time active forcing of drivers to go home whenever three passengers in a row have reported feeling unsafe.

In 2016, UPS began to use a software system called ORION (On-Road Integrated Optimization and Navigation). Although the ORION software is not public, it can be assumed that it includes metaheuristic geospatial algorithms, vehicle routing and scheduling algorithms, and a routing strategy that modifies and enriches building metadata to help drivers park and load/unload goods. UPS drivers received driving directives from ORION that optimized delivery routes by suggesting the fastest and cost-effective trip route for a delivery. UPS claims the algorithm has reduced unnecessary delivery truck travel by 100 million miles annually, declaring that truck drivers are not expected to strictly follow the system instructions, but may use their own discretion when necessary. However, we must note that UPS also uses predictive analytics to analyze and forecast optimal practices for routing packages and this analysis can also evaluate the personal decisions of drivers and their performance. The use of the ORION tool is an example of an ``intelligent'' software system that potentially replaces several aspects of human decision-making in a work scenario and provides employees automated instructions on daily tasks.

Algorithms used to collect and analyze gig economy data are usually proprietary and undisclosed to workers and to the public. In particular, most workers do not know what kind of data are being collected about them, nor how they are being analyzed and used in the working processes. We must also consider that in the field of machine learning, and in particular in deep learning, it is particularly difficult to understand algorithm functioning. In all these\vadjust{\vspace*{10pt}\pagebreak} cases, algorithmic procedures are perceived as black boxes making incomprehensible decisions, and workers are often frustrated with algorithmic recommendations that are not intelligible to them. In the field of AI, algorithm explainability requires that a machine learning technique and its decisions or results can be explained at an acceptable level in a way that ``makes sense'' to human beings. Differently from traditional work environments where decisions are delegated only to managers who make choices about workers' activity and organization, in work environments where algorithms are used the authority is delegated to and exercised by an unprecedented mix of ``intelligent'' code and human supervisors. Workers often need to face data, information, and algorithmic disparities in relation to managers and company owners, especially if they do not have digital skills.

Some studies on automatic management and control have revealed that workers sometimes resist control in different ways, from individual strategies of resistance to collective organizing and legal mobilization. This set of resistance tactics has been coined as ``algoactivism.'' Some practical strategies for algorithm resistance are worker non-cooperation, ignoring algorithm recommendations, leveraging algorithms through the analysis of their code, and personal negotiation with clients to bypass algorithms. Other than individual strategies, workers may resist through collective actions using computer technology such as online forums and platforms devoted to workers' emancipation and information sharing. Also, legal mobilization for defending worker privacy, limiting managerial surveillance and discrimination, and claiming data ownership are used as collective forms of algoactivism. Recently, activities devoted to the application of new legal regulations about data collection and algorithm use are increasing in Europe and the US. Examples of algoactivism employing some of the above-mentioned tactics involved gig workers at DoorDash, Uber, Airbnb, TaskRabbit, and Fiverr. The introduction of algorithms into the workplace has changed the ways people work and manage human and machine work. This is particularly true in the gig economy. Algorithms using intelligent techniques and large datasets are becoming efficient tools of worker control and income increase because that is how they were designed to function. However, to be accepted and efficiently used, digital solutions must consider the wellbeing of workers and create opportunities for more emancipatory approaches to work.

\section{\label{sec:6.5}Learning Algorithms for Healthcare}

Another sector where new machine intelligence techniques and tools are changing the way people work is healthcare. Many computing systems and software solutions are successfully used for detecting diseases, supporting doctors, assisting patients, improving healthcare processes, and saving lives [\citealt{chap:6:Talia:2022}]. During the COVID-19 pandemic, machine learning solutions were\vadjust{\vspace*{-16pt}\pagebreak} very helpful for analyzing SARS-CoV-2 pathology and for vaccine production. Algorithms have also been used to predict vaccine hesitancy and optimize vaccine allocation and distribution. For example, a team of researchers in the US has implemented an efficient algorithm to optimize vaccine distribution in an epidemic network that can potentially reduce the percentage of people infected by up to seven times and up to 98\% at the peak of the infection on a city-scale network. The researchers used a simulated social contact network for residents in Oregon and applied a machine learning algorithm they developed by comparing its advised vaccination strategy with a scenario in which doctors randomly vaccinated the same number of people. Graph analytics techniques were used to identify an optimal set of vaccination nodes (``seeds'') that minimized the effective number of network infections. The researchers leveraged principles from a well-known problem in network science, that is, influence maximization, and used a parallel implementation of the algorithm using the Summit supercomputer at Oak Ridge National Laboratory, one of world's fastest such computers, to significantly reduce time to solution from a few hours to roughly three minutes on large networks. This work shows how combining machine learning techniques and a high-performance computing system---in this case, a hybrid CPU--GPU implementation---allows for computing the largest number of possible solutions quickly and accurately so that critical problems such as vaccine distribution optimization can be addressed in as close to real time as possible, and the task of doctors and nurses has been simplified and optimized.

Another extremely dangerous virus outbreak, the Ebola epidemic in West Africa (mainly in Guinea, Liberia, and Sierra Leone) in 2014, became a serious health threat. By the end of the epidemic, 28,000 people had been infected and about 11,000 of them had died, a case-fatality rate of 40\%. To identify the disease dynamics and assess non-pharmaceutical control interventions, a team of US and Italian researchers developed a computer-based model of Ebola transmission that integrated geographical and demographic data to overcome the limitations of previous non-spatial approaches. They used a spatial agent-based model with a Markov chain Monte Carlo approach to estimate virus transmission parameters and evaluate the efficacy of interventions such as readiness of Ebola Treatment Units, safe burial procedures, and household protection kits. The computing model found that the movement and mixing of Ebola and non-Ebola patients in hospitals at the early stage of the epidemic was a sufficient driver of the observed pattern of spatial spread. It was estimated that by August 2014, 38\% of infections were acquired in hospitals, 30\% in households, and 9\% while attending funerals. The model allowed healthcare personnel to evaluate intervention options. Thanks to the availability of an increasing number of Ebola beds in treatment units, hospital transmission drastically decreased over time. In five months, within-hospital transmission declined\vadjust{\vspace*{-16pt}\pagebreak} from 38\% to 17\%.

Other than the pandemic, there are many other fields in healthcare where computing technology is helping humans. Diagnostics, for example, is a sector where new algorithms and software systems are providing innovative solutions. Recently, an international research team investigated the use of machine learning algorithms for Alzheimer's disease identification and prediction. Indeed, there is intense interest in designing machine learning algorithms to investigate syndromes such as Alzheimer's, which is the leading cause of dementia in older adults. Its incidence rates are increasing annually because our aging population is also increasing, and it will have deep social and economic impacts. To solve this problem, different classification algorithms such as decision trees, random forest, support vector machine, gradient boosting, and voting are employed. Classification results reached a validation average accuracy of about 85\% on the test data. This accuracy score is significantly higher in comparison to existing work and shows how AI techniques can be used for diagnosis of symptoms and prediction at early stages of this kind of disease, which is currently a major health concern. The analysis of data allowed researchers to identify the stage of the patient's Alzheimer's disease. Detecting the stage helps doctors better understand how the disease is affecting patients and put in place the appropriate treatment.

Regarding software models of the health of older people, a team of researchers from the US and Hong Kong implemented an AI recommendation system based on self-organizing maps (unsupervised neural networks used for data clustering) that can estimate one's psychological age and future well-being. The system exploits a deep learning model to provide a two-dimensional map of human psychotypes and identifies the regions that are most vulnerable to depression. The map is then used to provide personalized recommendations for maximizing one's future well-being. The learning algorithm splits all respondents into clusters depending on their likelihood of developing depression and determines the shortest path toward one of the clusters that express respondents mental stability. This model of human psychology can be used in self-help digital applications and during therapist sessions. To demonstrate the system's potential, the team has released a free web application (\href{https://www.futurself.ai/}{www.futurself.ai}) that lets people take the psychological test the team developed. This is another interesting case that shows how learning algorithms can change the way doctors investigate diseases and change treatment protocols.

Cardiac arrest is the third leading cause of death in industrialized countries, resulting in more than 700,000 deaths in Europe and the US annually. In the US, more than 500,000 cases of cardiac arrest occur a year, with 61\% happening outside a hospital and 39\% in a hospital. The percentage of people who survive out-of-hospital cardiac arrest with care by medical services is, regrettably, only about 8\%. Machine learning techniques can be used to predict cardiac arrest. A recent study designed and tested an algorithm based on the gradient\vadjust{\vspace*{-18pt}\pagebreak} boosting technique to \hbox{predict} the risk of out-of-hospital cardiac arrest by combining timing and meteorological data. Previous studies have shown an association between lower temperature and the incidence of cardiac arrest. Researchers developed and trained an algorithm on more than 500,000 of roughly 1.2 million cases of out-of-hospital cardiac arrests using either weather or timing data, or both. To test the model's accuracy, the results they obtained were compared with 135,000 such cases occurring in 2014--2015. By combining meteorological and chronological variables in the predictive model, the predicted values fitted the observed values with high precision. Sundays, Mondays, holidays, winter, low ambient temperature, and large interday or intraday temperature difference were strongly associated with the incidence of cardiac arrest. This kind of predictive model can be used to support doctors in improving the prognosis of patients with out-of-hospital cardiac arrest through a warning system for citizens and emergency medical services on high-risk days that will contribute to minimize the effects of an arrest and save patients' lives.

Together with machine learning solutions, public health professionals have another key source to consult when looking for data, information, and techniques that help them to protect the health of people. Many thousands of apps are available for handling numerous public health issues and users. Computer scientists and physicians have developed many smartphone applications that collect data to enhance therapy and help therapists make more timely interventions. Several apps have been downloaded many thousands of times and are utilized many times during the day by patients. Regarding heart problems, wearable devices such as smartwatches can be used today to monitor and learn about an individual's vital signs. Using the collected data, a smartwatch can alert a person to signs of an impending cardiac problem or heart attack. For example, researchers at the Beth Israel Deaconess Medical Center in Boston have developed mindLAMP, an app to provide therapists with an overview of patient behavior and mental status. mindLAMP uses smartphone sensors to collect behavioral data such as screen time, movement and sleep, and information from patients through surveys and cognitive tests. Doctors review the data to evaluate patients' mental health and then tailor therapy routines based on the evaluation's results. Boston University researchers created the Motivation and Skills Support app to deliver targeted social-goal assistance to patients based on location, movement, and audio data collected by their phones. Finally, researchers at the University of Washington are exploring the use of online search-history data to better understand suicide risk and improve medical processes to detect and prevent it. An algorithm they used in the attempted suicide retrospective study identified about 63\% of attempts that had been made as early as six months before the event. As declared by John Torous, a psychiatrist and director of the \hbox{Division} of Digital Psychiatry at Beth Israel Deaconess Medical Center, these software systems are not used ``to replace or be a substitute for care, but to\vadjust{\vspace*{-18pt}\pagebreak} augment it, by partnering with patients to bring valuable new data into a therapy visit and offer increased support for patients between those visits.''

As with other safety-critical application domains, healthcare is a crucial field where intelligent computing technologies must be introduced with extreme caution. In Western countries, people of color sometimes face disparities in access to healthcare and the quality of care. Implicit attitudes and behaviors that exist outside of conscious awareness of healthcare professionals have been identified as one of many factors that contribute to health disparities. In other cases, algorithm bias generates unfairness and discrimination. However, there are cases where algorithms may help in improving health diagnosis and treatment by correcting the doctors' decisions. A recent study found that a deep learning algorithm used to analyze the experienced pain of patients suffering from osteoarthritis of the knee was able to explain better than therapists the level of the pain the patients were feeling. Researchers started from the fact that underserved people experience higher levels of pain. This disparity persisted even after human physicians used medical images controlled for the objective severity of diseases such as osteoarthritis. Scholars used a deep learning technique to measure the severity of osteoarthritis by using knee X-rays to predict patients' experienced pain. This algorithm significantly reduced unexplained racial disparities in pain by considering supplementary undiagnosed features that would be overlooked by doctors. Because patients who reported severe pain and scored highly on the algorithm's measure, but low on the official grading systems, were more likely to be Black, the system suggested that traditional diagnostics may be inadequate when serving that community. The study appears to be particularly interesting because, while AI methods have often been accused of being unfair, this case shows that there are cases where an algorithm can do a better job than a human to improve the health of patients of lower socio-economic status. The example we discussed here shows how the integration of machine intelligence into medicine and clinical practice is changing healthcare. As this integration continues, the assistive role of computers in solving real-world problems can be fulfilled by enhancing physician and ``practitioner'' skills as opposed to replacing them. The intensive use of AI technologies, however, requires careful screening, ethical behavior, legal frameworks, patient protections, and intensive practitioner training.

\section{\label{sec:6.6}AI for Energy and Green AI}

Digital technologies are key elements for mastering some of the greatest challenges of our time. Among these challenges, we must consider climate change and energy management. Machine intelligence tools and applications can accelerate climate solutions, for example by reducing the energy needed to heat and cool buildings, developing accurate climate and weather models, modeling emissions reduction in energy and industrial infrastructures, or optimizing freight transportation. Machine learning algorithms have also been used for reducing emissions from cement plants, forest fire prevention and control, and tracking animal conservation outcomes. AI and machine learning solutions are available to provide real-time forecasts of solar power generation to support balanced electrical grids, and to predict how extreme weather threatens food security. The digitalization of the energy sector is resulting in more efficient use of energy and in an impressive collection of large data volumes that may be analyzed to understand complex energy systems and to make predictions that can be used to make energy production and commerce more efficient and secure.

Also, client data providing information about personal and family behaviors and time windows of energy consumption can be exploited to save energy. In smart homes, the connected devices can react to price changes in the electricity market and adapt to household usage patterns to save electricity and reduce costs. A heating system in a family house or in a large office building can be driven by a machine learning system that on the basis of electricity market prices can adjust and boost its performance when electricity is cheaper and more abundant. Smart grids are also a significant sector for application of data analysis and machine learning algorithms. For instance, with a combination of dynamic power generation plants such as solar, hydroelectric, and wind, it is very important for power generation to react intelligently to consumption. Machine learning can help analyze the data coming from the various actors (producers, storage facilities, and consumers) connected to each other via the grid and make forecasts that can optimize power grid management, energy distribution, and client consumptions. Machine learning solutions are also used in microgrid adaptive control. A microgrid is a local energy grid that can operate as a single independent electric grid. Microgrid control systems can use machine learning to manage energy flow and optimize energy practices. Microgrids are becoming popular because they can provide energy security during emergencies and can integrate renewables into large or national power grids more easily than traditional grids. For these reasons, the use of data-driven management systems in microgrids may bring significant benefits in energy shortage periods and for saving energy for consumers.

Predictive algorithms are widely used for stock trading, and they are greatly changing this sector. Algorithm-based stock trading uses artificial advisors that analyze masses of data items and executes trades at the optimal price. Algorithmic traders also analyze forecast markets with incredible accuracy and trade firms efficiently, mitigating risks and providing better returns. Energy trading is also impacted by machine learning algorithms. They are used to predict energy demand and provide traders with real-time information about energy prices and other information that may influence the market such as weather or seasonal data. With these data, human and robot traders can then make more informed decisions about when to buy and sell energy. For example, when a market opportunity is identified that fits within a predefined strategy, robot traders embodying machine or deep learning algorithms start to operate and through them trades can take place at very high frequency, much faster than is possible through manual trading.

Automatic learning systems are also used by telco companies for saving energy in data transmission. A report from Nokia clarifies that around 80\% of telcos are using Big Data and machine learning software systems to reduce energy consumption. Nokia reports that only 90\% of the energy produced by power plants reaches the network, which means there is an initial loss of 10\% during transmission. From that energy, about 80\% is consumed by radio access, the rest is used for transport, core, and operation support systems. Thirty percent of that network energy is consumed by passive components such as air conditioning and power systems; only 65\% of the original energy is consumed by the network elements itself. Given that scenario, we may deduce that smart site solutions for telecommunication energy are extremely important. Therefore, machine learning strategies that perform dynamic shutdowns and activate sleep modes of unused resources are key solutions. For instance, algorithms such as reinforcement learning, decision trees, and auto-regression models are used to dynamically forecast workloads and save energy by turning off unused hardware devices.

Regarding the use of AI methods for saving energy at large, an orthogonal but important issue is related to the reduction of the computational costs of machine learning algorithms and the research efforts toward the so-called green AI that considers the carbon footprint of machine intelligence. Many of the results of machine intelligence techniques and applications used today are achieved by analyzing very large datasets and running computationally intensive machine and deep learning models. Research studies have estimated the carbon footprint of these models and argued this trend is ecologically unfriendly and excessively expensive. According to a simple metric [\citealt{chap:6:Schwartzetal:2020}], the energy cost (\textit{EC}) of the execution of a learning method is proportional to the cost of executing the method on a single tuple (\textit{T}), the size of the training dataset (\textit{D}), which determines the number of times the method is executed during training, and the number of hyperparameter experiments (\textit{H}), which controls how many times the model is trained during model development:\vspace*{2pt}
\begin{equation*}
\textit{EC} \propto T \times D \times H.
\end{equation*}
\removelastskip{\vfill\pagebreak}

\noindent AI systems are often computationally expensive; however, most of them provide opportunities for efficiency improvements and energy consumption reduction. Those goals are pursued by Green AI, a research field promoting AI solutions that are energy aware and more environmentally friendly and inclusive. Green AI fosters technical approaches that have advantageous performance--efficiency tradeoffs. The rationale of this vision is to consider measures of algorithm efficiency and accuracy together with the positive impact on their inclusiveness and the natural ecosystem. In this context, energy-aware machine learning algorithms have received attention in the research community, and energy efficient solutions of Big Data analysis especially on mobile and edge devices have been designed and evaluated [\citealt{chap:6:ComitoandTalia:2017}]. Practical solutions are also aimed at designing more energy-efficient data centers, which consume less energy and, consequently, pollute and cost less. Furthermore, approximate machine learning methods have recently been proposed as another practical approach to energy-efficient AI systems. These are based on the ability to tolerate some loss of quality or optimality in the computed model. By relaxing the need for totally precise learning results, approximate techniques allow significantly improved energy efficiency.

\section{\label{sec:6.7}AI for Finance and Trading}

Algorithms that work on Wall Street have transformed how business in finance is done and who profits from it. Among those algorithms, rule-based systems, machine learning, and data analytics procedures are playing a key role in finance and trading. In particular, machine learning algorithms in finance are now considered a key element of many financial services such as risk-level evaluation, credit score calculation, loan approval, and asset management. As in the case of energy trading we mentioned before, AI stock trading largely uses artificial advisors to analyze millions and millions of data items and perform trades at the optimal price. Software traders also predict market trends with great accuracy to mitigate risks and afford higher returns. A short article from \textit{MIT Technology Review} [\citealt{chap:6:Byrnes:2017}] reports that in 2000 the cash equities trading desk at Goldman Sachs's New York headquarters employed 600 traders to buy and sell stocks for their many clients. After 20 years, there are just two equity traders left. Automated trading software has taken over the rest of the work, supported by 200 computer engineers. This is not an isolated example; in fact, other firms operating on Wall Street and on other stock exchanges have moved to AI-based trading that has replaced human experts and achieved good profits.

The case of the trillion-dollar Flash Crash that occurred in 2010 shows the huge consequences of using algorithm trading and the importance\vadjust{\vfill\pagebreak} of including all the needed input parameters and correctly setting/{\allowbreak}calculating their appropriate values. The story is easy to tell. On May 6, 2010, Wall Street was impacted because of the Greek debt crisis. The euro was falling against the dollar, and no one had expected the near 1,000-point dive in share prices. In a few minutes, the Dow Jones index lost almost 9\% of its value. Hundreds of billions of dollars were wiped off the share prices of companies such as General Electric and Proctor~\& Gamble. But the financial bloodbath fortunately did not last long. In less than one hour, Wall Street regained its serenity and eventually closed just 3\% lower. A lengthy investigation was carried out, and at the end of September 2010, the Securities and Exchange Commission and the Commodity Futures Trading Commission issued a joint report that depicted a highly fragmented and fragile market in which a single large transaction generated by Waddell \& Reed Financial's high-frequency trading (HFT) software caused a large change in the share price. The order would have involved an unusually large quantity of E-mini S\&P 500 futures contracts, which, due to the massive presence of HFT algorithms, immediately extended the \hbox{effects of} the transaction to the entire list, causing a significant and rapid loss of value of the stock exchange. A similar crash case occurred with the Knight Capital loss of US\$440 million on August 1, 2012, due to unpredictable behavior of its trading algorithms.

These disruptive cases demonstrate the large impact of the use algorithms in finance and trading and the substitution of human traders with these algorithms. The growth of high-tech and high-frequency trading has resulted in a significant decline in the role of human market makers and increased the financial market's dependency on algorithms. In particular, in HFT the execution of algorithmic trading strategies is characterized by extremely short position-holding periods of a few seconds or milliseconds. Moreover, Ultra HFT, also called low-latency trading, refers to HFT execution of trades in sub-millisecond times through co-location of computing servers and stripped-down strategies, direct market access, or individual data feeds offered by Exchanges and others to minimize network and other types of latencies [\citealt{chap:6:Treleavenetal:2013}].

Composite trading algorithms, including statistical computations, rule-based software, and machine learning strategies, are used in complex trading sectors such as credit and currencies. To manage these kinds of multifaceted trades, intelligent algorithms are generated through very long training sessions based on massive amounts of data examples. Executed on high-performance computing servers, they are able to make complex decisions as a human trader would do, and they make those decisions in a shorter time. The algorithmic trading process can be split into four main steps: \textit{pre-trade analysis}, \textit{trading signal generation}, \textit{trade execution}, and \textit{post-trade analysis} [\citealt{chap:6:Nutietal:2011}].

{\baselineskip=14.35pt Pre-trade analysis is the most common use of algorithms within a trading scenario. It includes any system that uses financial data or news to analyze certain properties of an asset. It can be an automatic method to value a company, or it can involve data analytics or machine learning algorithms that scan news or social media posts to forecast asset price volatility. Pre-trade analysis includes three computational models. The alpha model predicts the future behavior of the financial instruments to trade, the risk model evaluates the levels of exposure/{\allowbreak}risk associated with the financial instruments, and the transaction cost model calculates the potential costs associated with trading the financial instruments. Human traders use the output to make trading decisions that are most likely based on a selection of trading signals and some discretionary input. The second step in automating the trading process is trading signal generation. It consists of the portfolio construction model. This model takes as input the results of the alpha, risk, and transaction cost models and decides what portfolio of financial instruments should be owned going forward and in what quantities. Human traders can execute the generated signal if they require further discretionary input. This level of automation is generally applicable to all but HFT, where complete automation is a prerequisite. The third step is trade order execution. Algorithmic trading can execute trades and place orders in one or more exchanges. At trade execution, the execution model executes the trades, making decisions with constraints on transaction costs and trading duration. Often, the trading decision is made algorithmically; therefore, human traders are excluded from the actual trading choice, giving algorithms a new role and stronger power. According to the UK research firm Coalition, nearly 50\% of the revenue from cash equities trading comes from algorithmic trades.

Trading is not the only professional sector where machine intelligence software is deeply embedded. Financial fraud detection is another area where data analytics and machine learning algorithms are efficiently applied. Fraud is a major problem for financial and banking companies and services. It accounts for billions of dollars in losses each year, and millions of people every year file a fraud complaint. Fraud detection is a complex task that is hard to do for humans. While in the past financial fraud detection systems were implemented through rule-based expert systems, which could be sidestepped by modern fraudsters, today computerized systems are based on the analysis of transaction data. Most corporations and banks today exploit machine learning techniques, such as association rules, deep learning, and anomaly detection algorithms, to identify or prevent fraudulent financial transactions. Machine learning software scans through ``Big transaction Data'' to find specific operations or anomalies that must be declined or flagged for further investigation. The huge amount of data to be analyzed makes human scanning\vspace*{-14pt}\break\par}

{\noindent impossible and offers algorithms the opportunity to learn fraud patterns that are useful for identifying fraudulent financial practices.}

\section{\label{sec:6.8}AI in the Public Sector}

Similar to or more than other computer science technologies, AI solutions can have a significant impact on public policies and services in many ways. The OECD Observatory of Public Sector Innovation observed that in a few years it is expected that the potential will exist to make available nearly one-third of public servants' time, allowing them to shift from routine tasks to higher value work. The average civil servant spends up to 30\% of their time on documenting information and other elementary administrative tasks. By automating or avoiding even a fraction of these tasks, the public sector can save a remarkable amount of money as well as devote civil servants' work toward more valuable activities, resulting in more attractive jobs and improving public service quality. Governments are also increasing the use of machine intelligence to design better strategies and make better decisions, improving relationships and engagement with citizens. This process is improving the speed and quality of public services. A mapping initiative carried out in 2019 by the OECD identified 50 countries (including the EU) that have launched, or have plans to launch, national AI strategies. They are at different stages of development, and these include some key themes such as economic development, trust and ethics, security, and enhancing the talent pipeline. Of those 50 countries, 36 either have separate strategies in place for public sector AI or a dedicated AI focus embedded within a broader strategy.

An example of automated tasks in the public sector is the case of the US Bureau of Labor Statistics at the Department of Labor, which each year is tasked with analyzing hundreds of thousands of surveys related to workplace injuries and illnesses in businesses and public sector organizations across government. This analysis is time-consuming and boring and takes up 25,000 employee hours each year. In the last ten years, the Bureau experimented with the use of an intelligent software system to code surveys starting with the easiest and most clearcut responses. Over time, the use of that software increased and is now used in half of all surveys. The Bureau has found that the system can code as much in one day as a trained employee could do in a month, with a higher level of accuracy. This is an impressive result that may create some problems for employees. However, the Bureau leaders explained to employees that the purpose was not to replace them but rather to allow them to focus on more complex and valuable tasks. They also provided training sessions on machine learning and how it can add value to their work [\citealt{chap:6:ChenokandYusti:2018}].

Another example is the use of machine learning for land mapping in Australia. The Queensland Government Department of Environment and Science has adopted a learning algorithm to automatically map and classify land use features in satellite imagery. Identifying different land uses (e.g., agriculture, grazing, or housing) is crucial for natural disaster monitoring and conserving biodiversity. It can also be useful in providing a near real-time analysis of the impact large disasters such as floods and tropical cyclones will have on harvests. The procedure implemented in Queensland was 97\% accurate; thus, it is reliable. In traditional methods performed by human workers, mapping land uses for the whole state took years, whereas the same process now takes only six weeks when leveraging machine learning technology.

Another significant example comes from Denmark, where a machine learning software has been developed to help civil servants make decisions about whether citizens and businesses receive financial and other assistance from the government, such as assistance for poor families and housing assistance. The goal of the Danish government is to generate more accurate and impartial decisions free from human bias. It can also help address the problem of an ageing population with only a limited number of civil servants available to process an increasing number of welfare requests. To make this possible, the government worked to address two new challenges: how to put in place appropriate legislation to enable automated decisions, and making underlying citizen data and offices decisions flows readable and understandable by computers.

There are many other real-world examples. They show how public services are boosted by machine intelligence solutions that are able to analyze very large amounts of data and perform tasks significantly faster than human employees.


\begin{thebibliography}{}
\bibitem[Autor(2014)]{chap:6:Autor:2014} D. H. Autor. 2014. \textit{Polanyi's Paradox and the Shape of Employment Growth}. National Bureau of Economic Research.

\bibitem[\hbox{Brynjolfsson}(2022)]{chap:6:Brynjolfsson:2022} E. Brynjolfsson. 12 January. 2022. The Turing Trap: The Promise \& Peril of Human-Like Artificial Intelligence. [Online]. Retrieved November 12, 2022 from \href{https://digitaleconomy.stanford.edu/news/the-turing-trap-the-promise-peril-of-human-like-artificial-intelligence/}{https://{\allowbreak}digitaleconomy.{\allowbreak}stanford.{\allowbreak}edu/{\allowbreak}news/{\allowbreak}the-{\allowbreak}turing-{\allowbreak}trap-{\allowbreak}the-{\allowbreak}promise-{\allowbreak}peril-{\allowbreak}of-{\allowbreak}human-{\allowbreak}like-{\allowbreak}artificial-{\allowbreak}intelligence/}.

\bibitem[Byrnes(2017)]{chap:6:Byrnes:2017} N. Byrnes. 7 February. 2017. As Goldman embraces automation, even the masters of the universe are threatened. \textit{MIT Technology Review}. [Online]. Retrieved November 12, 2022 from \href{https://www.technologyreview.com/2017/02/07/154141/as-goldman-embraces-automation-even-the-masters-of-the-universe-are-threatened/}{https://{\allowbreak}www.{\allowbreak}technologyreview.{\allowbreak}com/{\allowbreak}2017/{\allowbreak}02/{\allowbreak}07/{\allowbreak}154141/{\allowbreak}as-{\allowbreak}goldman-{\allowbreak}embraces-{\allowbreak}automation-{\allowbreak}even-{\allowbreak}the-{\allowbreak}masters-{\allowbreak}of-{\allowbreak}the-{\allowbreak}universe-{\allowbreak}are-{\allowbreak}threatened/}.

\bibitem[\hbox{Chenok and Yusti}(2018)]{chap:6:ChenokandYusti:2018} D. Chenok and C. Yusti. 2018. \textit{The Future Has Begun: Using Artificial Intelligence to Transform Government}. IBM Center for the Business of Government and Partnership for Public Service. Retrieved from \href{https://www.businessofgovernment.org/report/using-artificial-intelligence-transform-government}{https://{\allowbreak}www.{\allowbreak}businessofgovernment.{\allowbreak}org/{\allowbreak}report/{\allowbreak}using-{\allowbreak}artificial-{\allowbreak}intelligence-{\allowbreak}transform-{\allowbreak}government}.

\bibitem[Comito and Talia(2017)]{chap:6:ComitoandTalia:2017} C. Comito and D. Talia. 2017. Energy consumption of data mining algorithms on mobile phones: Evaluation and prediction. \textit{Pervasive Mob. Comput.} 42, 248--264. DOI:~\href{https://doi.org/10.1016/j.pmcj.2017.10.006}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1016/{\allowbreak}j.{\allowbreak}pmcj.{\allowbreak}2017.{\allowbreak}10.006}.

\bibitem[\hbox{Lee et~al.}(2015)]{chap:6:Leeetal:2015} M. K. Lee, D. Kusbit, E. Metsky, and L. Dabbish. 2015. Working with machines: The impact of algorithmic and data-driven management on human workers. In \textit{Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15)}. ACM, 1603--1612. DOI:~\href{https://doi.org/10.1145/2702123.2702548}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}2702123.{\allowbreak}2702548}.

\bibitem[M\"{o}hlmann et~al.(2021)]{chap:6:Mohlmannetal:2021} M. M\"{o}hlmann, L. Zalmanson, O. Henfridsson, and R. W. Gregory. 2021. Algorithmic management of work on online labor platforms: when matching meets control. \textit{MIS Q.} 45, 4, 1999--2022. DOI:~\href{https://doi.org/10.25300/MISQ/2021/15333}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}25300/{\allowbreak}MISQ/{\allowbreak}2021/{\allowbreak}15333}.

\bibitem[Nuti et~al.(2011)]{chap:6:Nutietal:2011} G. Nuti, M. Mirghaemi, P. Treleaven, and C. Yingsaeree. 2011. Algorithmic trading. \textit{Computer} 44, 11, 61--69. DOI:~\href{https://doi.org/10.1109/MC.2011.31}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}MC.{\allowbreak}2011.31}.

\bibitem[Polanyi(2009)]{chap:6:Polanyi:2009} M. Polanyi. 2009. \textit{The Tacit Dimension}. University of Chicago Press.

\bibitem[Schor(2020)]{chap:6:Schor:2020} J. Schor. 2020. \textit{After the Gig: How the Sharing Economy Got Hijacked and How to Win It Back}. University of California Press, Berkeley.

\bibitem[Schwartz et~al.(2020)]{chap:6:Schwartzetal:2020} R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. December. 2020. Green AI. \textit{Commun. ACM} 63, 12, 54--63. DOI:~\href{https://doi.org/10.1145/3381831}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}3381831}.

\bibitem[Talia(2022)]{chap:6:Talia:2022} D. Talia. October. 2022. Machine intelligence for human health: A few noteworthy cases. \textit{Computer} 55, 10, 82--86. DOI:~\href{https://doi.org/10.1109/MC.2022.3190786}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}MC.{\allowbreak}2022.{\allowbreak}3190786}.

\bibitem[Treleaven et~al.(2013)]{chap:6:Treleavenetal:2013} P. Treleaven, M. Galas, and V. Lalchand. November. 2013. Algorithmic trading review. \textit{Commun. ACM} 56, 11, 76--85. DOI:~\href{https://doi.org/10.1145/2500117}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}2500117}.
\end{thebibliography}

%\end{document}

