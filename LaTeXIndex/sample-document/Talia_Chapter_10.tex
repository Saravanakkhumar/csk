%\documentclass{acm-book-v2}
%\RequirePackage[errorshow]{tracefnt}
%%\newcommand{\mpage}[1]{}
%%\newcommand{\indexfn}[1]{}

%%\usepackage{showframe}

%\usepackage{custom-tooltip}
%\usepackage{custom-tooltip-Alt-Text-View}



%\begin{document}

\setcounter{chapter}{9}

\chapter{\label{chap:10}Transparency and Accountability of AI Algorithms}

\noindent Often, decisions made by algorithms are neither clear nor visible to users. At the same time, some computing models (such as deep neural networks) are inherently opaque in their procedures. This gives responsibility and power to algorithm designers and impedes users from knowing how algorithms work and the real effect of algorithm execution on their lives. This chapter discusses issues and methods related to algorithm transparency, accountability, explainability, and openness, with a special focus on machine learning algorithms.

\section{\label{sec:10.1}Introduction to Algorithm Accountability}

In 2020, due to the COVID-19 pandemic, thousands of students in England and Wales received their A-level (their final school exam) grades. That year, instead of scoring actual exams, grades were determined by an algorithm. Almost 40\% of students received grades that were lower than they had anticipated, sparking public protest and legal actions. The algorithm looked at the historical grade distribution of a school and then decided students' grades on the basis of their ranking. Experts criticized the low accuracy of the algorithm and the use of historical data that generated a case of algorithmic bias. Faced with complaints and outcries, the government withdrew the grades. Students received grades based on their teachers' estimate of what their grades would have been had the exams gone forward as planned.

On June 25 and July 7, 2018, the City of Rotterdam used the System Risk Indication (SyRI) system to perform a risk analysis of welfare fraud on 12,000 addresses in a disadvantaged neighborhood. The risk analysis used an algorithm that was fed by several datasets containing personal data on people's fiscal, residential, scholastic, and employment situation. The city never published the algorithm's parameters and decision rules, nor were residents informed that they were being examined for welfare fraud. Residents and activists complained, and in 2020 a Dutch Court prohibited the government from using SyRI. A core reason for this, according to the verdict, was a lack of transparency of the algorithm used by this system. Other critical features of the SyRI system are discussed in the next section.

In 2023, police in the Basque Country, an autonomous region in the north of Spain, used a software tool to predict gender-based violence. The accuracy of the algorithm used in that tool was unclear, and it left a lot of room for the personal opinions of police officers. The results of a study carried out in 2022 on the performance of the algorithm show that, when assessing cases that had been catalogued as high risk, more than half of the time (53\%) the algorithm established that the risk was low. The study argued that ``the assessment tool is more likely to classify severe cases as non-severe at this score, which could imply the underestimation of cases.'' Additional analyses were needed to better understand why the algorithm made inaccurate classifications of gender-based violence cases, as its decision processes were not clear.

These cases are just a few of the real-world examples of a long list of opaque algorithms that have a large impact on people's lives and call for new approaches and regulations to implement algorithm transparency and accountability. The decision of the European Union in April 2023 to open the European Centre for Algorithmic Transparency (ECAT) is further confirmation of the importance of algorithms and data management in our lives. The European Commission created this new Centre to contribute to a safer, more predictable, transparent, and trusted digital environment for people and companies. The Centre aims to carry out several activities such as testing algorithmic systems to better understand their functioning, analyzing transparency reports, risk assessments, and independent audits, and defining relevant procedures to ensure data access for regulators and researchers, among others.

As we discussed in the previous chapters, algorithmic systems regulate and influence many aspects of our daily experience. With the ever-increasing societal impact of digital platforms, Big Data analysis, and AI applications (the most recent is the case of ChatGPT), there is an urgent need for public oversight of the processes on which those systems are based. Algorithm transparency and accountability are key elements in this situation. As we discussed in Chapter \ExternalLink{chap:7}{7}, measures adopted under the EU Digital Services Act (DSA) call for algorithmic accountability and transparency audits. Several real cases, a few of which have been discussed previously, occurred in recent years and have shown that the societal and ethical impact of algorithms is an area of growing concern.

In Chapter \ExternalLink{chap:7}{7}, we discussed the responsibility and ethical issues that inevitably arise in the development and use of algorithms. These are key issues for developers and users because of the wide use of algorithms in decision-making processes. In particular, many AI algorithms are used to make decisions in the private and public sectors. For example, algorithms are used to assess and assign people insurance and benefits, decide who should be hired or fired in companies, select who can see news or advertisements, and estimate a person's risk of committing crimes or of paying a loan. Significant, and sometimes vital, decisions that affect citizens' lives are made by algorithms whose instructions are not known by users and guided by data that are large and complex and often are not open to the public. Algorithms used for supporting decision-making can be extremely complex and can generate unexpected biases, risks, and harms for people who are subject to (semi-)automatic decisions. Some factors, such as designer biases and mistakes, incomplete data and/or inadequate input parameters, biased training processes, and biased data, can contribute to introducing unfair procedures and produce incorrect results. As we discussed, algorithms are as capable of bias as humans as they are entrenched with subjective values. Often, algorithms replicate social values and can also embed them into computerized systems, creating new expectations for what is important in a given context. Moreover, faulty engineering design choices or low explainability of algorithms can produce opacity in the design, usage, evaluation, and control of those algorithms. For these reasons, algorithms that are used to make decisions that affect people should be monitored, assessed, and amended during their use. Otherwise, they tend to strengthen the values and processes they were created with and in this way extend the negative effects of their decisions.

We can define accountability as the quality or state of being accountable, an obligation or willingness to accept responsibility or to account for one's actions. Mark \citet{chap:10:Bovens:2007} provided a more detailed definition of accountability: ``a relationship between an actor and a forum, in which the actor has an obligation to explain and to justify his or her conduct, the forum can pose questions and pass judgement, and the actor may face consequences.'' If we apply this general definition to the field of algorithms, we may say that algorithmic accountability is the process of assigning responsibility for harm when algorithmic decision-making results in discriminatory and inequitable outcomes [\citealt{chap:10:Caplanetal:2018}]. In 2020, Maranke \citeauthor{chap:10:Wieringa:2020} [\citeyear{chap:10:Wieringa:2020}] provided a more detailed definition: \hbox{``Algorithmic} accountability concerns a networked account for a socio-technical algorithmic \hbox{system,} following the various stages of the system's lifecycle. In this accountability relationship, multiple actors (e.g., decision makers, developers,\vadjust{\vfill\pagebreak} users) have the obligation to explain and justify their use, design, and/or decisions of/concerning the system and the subsequent effects of that conduct. As different kinds of actors are in play during the life of the system, they may be held to account by various types of fora (e.g., internal/external to the organization, formal/informal), either for particular aspects of the system (i.e., a modular account) or for the entirety of the system (i.e., an integral account). Such fora must be able to pose questions and pass judgement, after which one or several actors may face consequences. The relationship(s) between forum/fora and actor(s) departs from a particular perspective on accountability.'' Algorithmic accountability basically refers to the identification of responsibility for how an algorithm is designed and implemented and for its impact on people that use it or are subject to it. If harm is not avoided, mechanisms for reparation and compensation must be provided. For automatic decisions made by an algorithm, algorithmic accountability must be supported by clear policies that include system auditing procedures and standardized assessments for potential harm.

The lack of accountability in algorithms poses serious challenges to their employment in critical sectors such as justice, policing, healthcare, and welfare. Citizens cannot respond to or contest a decision made about them by a computer program if they are not notified or made aware of the decision, if they are unable to understand how a decision has been made, or if they cannot understand the reasons why it has been made. The lack of algorithmic accountability may have important consequences: reduced control of operations producing biased or unfair results, inefficient expenditure on poorly verified software systems, and discrimination against groups of citizens. Sometimes, for the affected persons, the consequences of the lack of accountability of the algorithms used to make decisions on crimes or health can in fact be a matter of life and death. In general, reliance on algorithms that are not sufficiently explained and not appropriately evaluated may weaken citizens' confidence in software solutions and applications. Algorithmic accountability requires software companies to implement additional appropriate controls during the software's lifecycle; however, it has taken hold mainly due to the attention of users and the changed legislative and regulatory context of digital systems as provided, for example, by the EU GDPR, the EU Artificial Intelligence Act, and the US Algorithmic Accountability Act (AAA). In particular, the AAA, introduced in both the US House and Senate in February 2022, requires tech companies using algorithmic decision systems to conduct critical impact assessments of their automated systems in accordance with Federal Trade Commission (FTC) regulations. The Act proposes to address discrimination and privacy concerns related to the use of computer programs in corporate decision-making. When the bill becomes law, the FTC will have the authority to conduct bias impact assessment of algorithmic decision support systems within two years. Software systems used in healthcare, banking, justice, employment, welfare, and education sectors would be probable targets for inspection. The US AAA and the EU Artificial Intelligence Act have several principles in common. For example, they both aim to establish the governance infrastructure needed to hold poor decision systems accountable and allow systems with good intent to ensure and explain that their decisions are legal, ethical, and safe.

Professional associations are also active in supporting initiatives and regulations for algorithmic accountability. For example, the ``Statement on Algorithmic Transparency and Accountability'' of the ACM US Public Policy Council and the ACM Europe Policy Committee lists seven principles for algorithmic transparency and accountability. The third principle is devoted to accountability, and it requires that: ``Institutions should be held responsible for decisions made by the algorithms that they use, even if it is not feasible to explain in detail how the algorithms produce their results.'' This principle has been extended in the ACM Technology Policy Council ``Statement on Principles for Responsible Algorithmic Systems,'' which includes an accountability and responsibility principle declaring that ``Public and private bodies should be held accountable for decisions made by algorithms they use, even if it is not feasible to explain in detail how those algorithms produced their results. Such bodies should be responsible for entire systems as deployed in their specific contexts, not just for the individual parts that make up a given system. When problems in automated systems are detected, organizations responsible for deploying those systems should document the specific actions that they will take to remediate the problem and under what circumstances the use of such technologies should be suspended or terminated.'' The Institute of Electrical and Electronics Engineers (IEEE) also worked toward the development of algorithm design and regulation recommendations. In particular, the IEEE developed the ``Standard for Algorithm Bias Considerations,'' which provides a framework to support developers and those responsible for the deployment of algorithms and software systems to detect, eliminate, or mitigate biases in the algorithmic systems' behavior, including biases in data used by the algorithms. The IEEE standard defines detailed methodologies to help designers and developers certify how they operated to address and remove mistakes and biases from their software systems.

Some algorithmic accountability issues related to data management are considered in the EU GDPR. This regulation requires data controllers to inform data subjects, that is, citizens, upon the collection of their data, whether their data are subjected to processing and analysis by an algorithm and how each algorithm manages their data, including the implication and consequences of this automatic data management. Under GDPR, individuals are then informed that their data will be handled by an algorithm so that they can access the data to ensure its correctness and accuracy. These regulations allow citizens to monitor how software systems exploit their data and ensure they do it in a ``correct'' way. However, as discussed by Sandra Wachter and her co-authors [\citealt{chap:10:Wachteretal:2017}], GDPR only dictates that citizens (data subjects) receive significant information about the logic involved as well as the significance and the foreseen consequences of the processing of their data by algorithms. According to Wachter et~al., GDPR lacks precise specification as well as clear and well-defined rights and safeguards against algorithm-based decision-making. Therefore, GDPR runs the risk of being ineffective if additional legislative and policy decisions are not taken to improve the transparency and accountability of algorithms and their effects on people whose data they process.

As a final consideration of this introductory section, we must recognize that we are in the first era of algorithmic accountability and the designed methods of accountability for the possible harms are not easy to apply. Some reasons for that come from the little knowledge individuals have about the algorithm design and development processes and because accountability implementation requires a good level of algorithm transparency and explainability.

\section{\label{sec:10.2}Algorithm Transparency and Opacity}

Although transparency of algorithms and software programs alone is not sufficient to provide accountability, it is an important prerequisite for accountability. If you do not know the sequences of instructions that compose an algorithm and compute its results, you cannot be responsible for it. Without a sufficient degree of transparency, it is hard to know whether an algorithm does what it declares, whether it is fair, or whether its results are trustworthy. Contrary to how it may appear at first glance, transparency is not a simple concept. In \textit{The Oxford Handbook of Public Accountability}, Albert \citet{chap:10:Meijer:2014} defines transparency as ``the availability of information about an actor allowing other actors to monitor the workings or performance of this actor.'' Moving from this general definition of transparency to a specific one for algorithmic systems, we can say that transparency of an algorithm refers to the principle that the static and dynamic factors that influence the decision of an algorithm (the actor) should be available---or visible---to the \hbox{people} (the other actors) employing or affected by the outcomes of that algorithm [\citealt{chap:10:DiakopoulosandKoliska:2017}]. Algorithm transparency includes availability of information about its design, its instructions (its code), the results it produces, and the processes it enacts or that individuals enact in terms of action applied during the operation of a software system based on the algorithm. The ACM ``Statement on Algorithmic Transparency and Accountability'' encourages systems and institutions that use algorithmic decision-making ``to produce explanations regarding both the procedures followed by the algorithm and the specific decisions that are made. This is particularly important in public policy contexts.'' Transparency of algorithms would require those who design and implement software systems to explain them to users or particular audiences. However, several factors, such as corporate confidentiality regulations and system patents, are likely to limit what algorithm designers and owners can be required to reveal. Furthermore, it must be considered that transparency always involves decisions about what to disclose and what information to omit. The decisions taken about the degree of information on algorithms made available to the public, governments, or a given group of individuals do not guarantee that they are appropriate in providing the right and complete information needed for guaranteeing algorithmic accountability.

Indeed, although transparency of algorithms requires that the input data, the instruction logic, and the corresponding output of an algorithm must be known, it does not necessarily imply that its results will be fair and unbiased. For this reason, algorithmic transparency must be coupled with accountability. That is, whoever designed, implemented, and deployed an algorithmic system that makes decisions must be accountable for the results and effects of those decisions. And yet the work of evaluating the results and decisions of a computerized system for assessing the liability of its designers and makers cannot be done without considerable transparency of the code of that system. Therefore, ensuring transparency leads to greater accountability. However, making algorithms transparent in a \hbox{simple} and intelligible way is not a straightforward process. As we discussed before, the information made available about an algorithmic system should permit the examination of potential biases inside the algorithms and/or in the input parameters, or even in the training data if data analysis or a machine learning algorithm is used. However, we should consider that this information will never be complete, and will never be able to provide full transparency for algorithms that are composed of thousands of code instructions. Even if algorithm documentation is supportive of providing transparency, human experts may find it difficult or sometimes impossible to provide perfect and clear elucidations of their decisions and of decisions included in the algorithms they design. While simple or rule-based algorithms that are coded using conditional instructions such as:
\begin{center}
\textbf{\textit{if}} $<$\textit{condition}$>$ \textbf{\textit{then do}} $<$\textit{operation}$>$
\end{center}
or plain loops such as:
\begin{center}
\textbf{\textit{while}} $<$\textit{condition}$>$ \textbf{\textit{do}} $<$\textit{operation}$>$
\end{center}
are easy to read and understand, it is extremely hard, for example, to comprehend the code of deep learning algorithms because they operate as a black box that receives some input data and produces outcomes according to a rearrangement of the values associated to the several connections of artificial neurons that is not readable to humans. Therefore, the outputs of deep learning systems can be impossible to interpret on the basis of their internal algorithms. A similar situation occurs, for example, with clustering algorithms based on thousands of parameters that are updated and optimized in a way that is difficult to understand. For this kind of machine learning system, therefore, transparency is a hard goal that can in some ways be simplified by paying special attention at the design stage and assessing results in terms of accuracy and fairness. In this way, individuals or social groups affected by the algorithm decisions (the output they produce) can be made aware of the characteristics of the training data, the principles coded into the software, and the description of the output data.

{The DSA, approved by the European Commission in July 2022, introduced new regulations that obligate large digital platforms to be transparent about the main factors that influence the algorithmic decision-making systems they use to distribute and moderate content. For example, novel transparency duties for digital platforms will allow European citizens to be better informed about how digital content is recommended to them and how they can monitor the way platform algorithms interact with them. Therefore, under the DSA, online platforms will have to improve their transparency and, at the same time, will also be held accountable for their role in spreading unlawful, harmful, and disrespectful content. In particular, the DSA defines rules for online platforms that must scrutinize the risks related to the diffusion of illegal content they can produce and the negative effects on voting processes, gender-based treatment, and civil rights. The DSA introduces new requirements for intermediaries to be more transparent when algorithms are used, and for what purpose software systems are run on their online platforms. The Act requires assessment of the extent to which such algorithmic systems are under human control and also consideration of content moderation. DSA is just one of the public regulations issued in recent years. In other countries such as Brazil, the UK, and Portugal, laws have been approved to regulate the adoption of algorithmic systems in online systems and in public administrations, implementing measures to enhance algorithmic transparency and clarity during their purchasing, development, and use. Indeed, the algorithm development stage is also a critical step for providing transparency. At this stage, the documentation describing the design and implementation work is compiled. It should also describe the used components, the choices made about software semantics (how results are computed), and the data used for algorithm training. For example, Timnit Gebru and her colleagues proposed the creation of repositories\vadjust{\pagebreak} to make explicit the motivation, collection,\hfilneg}

{\noindent\baselineskip14.4pt and composition process behind the \hbox{selection} of training data in machine learning algorithms [\citealt{chap:10:Gebruetal:2021}]. As each machine learning algorithm uses datasets for training and/or evaluation purposes, the features (the attributes and the raw values) of these datasets influence the behavior of a learned model. If these datasets reproduce unfair opinions or biases, their use can have negative consequences especially if the resulting machine learning models are used in critical sectors such as healthcare, justice, finance, human resource management, and banks.

Some algorithms cannot be examined because the data, instructions, or outcomes they rely on are not (or cannot) known or well documented. This creates a sort of algorithm opacity that hinders people from knowing how algorithms work and how algorithms make their decisions. Algorithms are opaque in the sense that if an individual is a recipient of the results of an algorithm (for instance, a classification of credit risk or a loan decision), seldom do they receive detailed information about why and how the classification or the decision has been reached. Additionally, the inputs themselves may be entirely unknown or only partially known.

In short, we can say that the logic and instructions that algorithms implement and execute to generate their results are opaque to individuals or organizations who use the algorithmic system for decision-making. They can also be opaque to people who are affected by the algorithm decisions. Moreover, speaking of AI, algorithms become opaque to experts who have designed and implemented the algorithms. As we will discuss, opacity is to a limited extent due to the complexity, variety, and size of input data an algorithm must process and from the potentially high number and heterogeneity of data attributes (numerical and categorical) and input parameters. In the case of machine and deep learning algorithms, it must also be considered that algorithms learn from data as they run. Therefore, the programmer may not comprehend how or what its learning algorithm discovers and how this modifies its behavior.

As we discussed, there can be different reasons for this state of ignorance of the users. Sometimes, the algorithm is too complex to explain, is proprietary, and, as we will discuss in the next section, cannot be explained because its technical design is hidden in the mathematical model it uses (this often occurs with deep neural networks). The ACM ``Statement on Algorithmic Transparency and Accountability'' we mentioned before identifies several factors that may generate opacity in algorithmic systems, among them there are (i) technical factors: ``the algorithm may not lend itself to easy explanation''; (ii) economic factors: ``the cost of providing transparency may be excessive, and may include compromising of trade secrets''; and\vspace*{-14pt}\break\par}

\noindent (iii) social factors: ``revealing input may violate privacy expectations.'' Moreover, the Statement clarifies that ``Even well-engineered computer systems can result in unexplained outcomes or errors, either because they contain bugs or because the conditions of their use changes, invalidating assumptions on which the original analytics were based.'' Jenna \citet{chap:10:Burrell:2016} proposed an accurate classification of algorithm opacity that considers three main classes:

\bgroup
\def\labelenumi{(\arabic{enumi})}
\begin{enumerate}
\item Opacity as intentional corporate or state secrecy.

This type of algorithmic opacity is a deliberate form of self-protection implemented by hardware/software companies for maintaining their trade secrets and competitive advantage. Indeed, often principles and code of algorithms are kept confidential for business or commercial reasons. For example, Google has never published details of how the ranking algorithms of its search engine work, like Facebook did with the algorithms that compute the News Feed of its app. Many other similar cases can be mentioned. Moreover, sometimes it is required to keep confidential the main components of a decision policy of an algorithm, the code that implements it, as well as input data for preventing planned ``gaming'' of a software system. However, we must also consider that this kind of opacity is often used as a means to implement new ways of manipulating users, sidestepping laws and regulations, or discriminating against individuals or social groups. Here we must mention that the approaches based on ``open source'' or ``free software'' remove this opacity and disclose the source code of algorithms that can be read, distributed, executed, and modified according to widely accepted rules.

\item Opacity as technical illiteracy or inadequate education.

Undoubtedly the design and implementation of algorithms requires specialized expertise. Also, reading software code is a very complex (or impossible) task for many people. This type of opacity that comes from this condition excludes the vast majority of people from understanding the languages used to code algorithms such as Java, Python, JavaScript, or C++. To achieve this skill, advanced courses are needed as programming languages and code design paradigms are created to be interpretable by other machine languages or directly by computers while at the same time being hard to read and understand by people who are not expert programmers. This type of opacity cannot be totally solved. However, extensive educational programs in coding and computational thinking would increase the number of people able to read and understand algorithms and make citizens more\vadjust{\vspace*{14pt}\pagebreak} educated about \hbox{algorithms,} allowing them to discern their logic and evaluate their functioning and results.

\item Opacity generated by the complex features of AI and machine learning algorithms and the scale required to apply them usefully.

Several software systems are composed of many components that embody complex and long algorithms that are built by teams of programmers. This scenario is common in complex software systems development, which generates a sort of opacity that coders who contribute single components are also not able to solve. In her paper, Burrell argues that there are some challenges of scale and complexity that in machine learning algorithms add up to the ones mentioned before. Those challenges originate from the difficulty of understanding the algorithm execution when it works on the training data. Machine learning models are generated from the analysis of large data examples where each data tuple may include hundreds or thousands of (heterogeneous) attributes (features). Then the logic of an algorithm that implements a machine learning model is shaped from what it learns from complex training data. Handling such a large number of data features produces complex code that also becomes opaque to algorithm experts and even more so to its users. The complex interaction between some learning algorithms (e.g., convolutional neural networks, singular value decomposition, or principal component analysis) and the large data sets they analyze to build a learning model generates software code that holds a certain degree of opacity that cannot be easily solved.\vspace*{-1pt}
\end{enumerate}
\egroup

In summary, several machine learning systems are opaque to some degree because they are based on instructions not explicitly programmed by human experts but automatically generated by a machine learning algorithm on the basis of the data it ingests and uses to learn a model. The more complex and high-level a learning algorithm is, the harder it is to explain and describe, even by experienced algorithmic designers or programmers.

In her paper, \citet{chap:10:Wieringa:2020} mentions the case of the SyRI system (see the previous section) used by several municipalities in the Netherlands to identify which citizens are more likely to commit fraud on social benefits. The SyRI system officially reports persons suspected of fraud to the Ministry of Social Affairs and Employment, which delegates this task to the municipalities. When several technical specifics of the SyRI system were not disclosed, civil society organizations formed the coalition ``Suspect by Default,'' demanding that the authorities reveal the system logic. The main\vadjust{\vspace*{12pt}\pagebreak} argument of the society organizations was that the SyRI decision system has no place in a democratic environment because civilians were required to share their data to be used for profiling measures of which the citizens were unaware and which they cannot question due to the system's opacity. They were partially successful. In 2020, a Dutch court decided the SyRI legislation was unlawful because it did not comply with the right to privacy under the \hbox{European} Convention of Human Rights. The most important motivations provided by the court were that the SyRI system was opaque and also because the reasons for collecting the data were not clear and precise enough. In response, the government argued that exposing the operating details of the system would \hbox{create} gaming effects, thus it would be imprudent to reveal the critical properties and procedures of the system.

In the past, techniques and methods have been studied and developed that can help limit or eliminate the main sources of opacity in algorithms. The analysis of input parameters and training data provided to the algorithm is one of those \hbox{useful} methods. Input analysis does not provide a complete understanding of how a system works, but it can be a useful tool toward understanding a system and its software modules. By inspecting the information provided to a software \hbox{system} for making a decision, one may guarantee that the input data are not biased, flawed, or inadequate. We must consider, however, that input analysis may involve private or copyrighted information. In this case, it can be hard to do it without the cooperation of the company that designed and implemented the system. Another methodology to augment the transparency of algorithms, in particular in the context of statistical data mining, is the statistical analysis of results. This process can include the use of data tests to decide whether a system behaves correctly throughout different use cases and does not generate unwanted or incorrect results/decisions. This can help in understanding whether there are serious issues that deserve deeper examination.

A final methodology for reducing algorithm opacity is sensitivity testing or analysis. The goal of this method is to better understand how a learning system behaves by providing several input data or test cases that show (very) small variances. This process can be carried out, for example, by an expert who wishes to infer information about a learning system that manages loans. Through the analysis of the outputs of a computerized loan scoring algorithm, after providing many very marginally different loan applications, it is possible to understand how the algorithm works. This methodology has been shown to be useful in the context of learning algorithms where sometimes researchers and developers of a given algorithmic system have limited awareness of how it produces specific results, as happens, for example, in several deep learning algorithms.

\section{\label{sec:10.3}Explainability of AI Algorithms}

Understanding how machines work is often a demanding task for humans. It especially becomes difficult when individuals have to deal with complex machines such as an airplane, a computer, a DNA analyzer, or a nuclear submarine. We can certainly consider AI systems among the most complex machines that humans have invented. They often perform very complex tasks and generate sophisticated results through complex computations that are not easy to understand even by experts. In fact, understanding their functioning and explaining their results is sometimes very difficult because their internal logic is not fully transparent to humans. In the past, the focus on AI systems has often been on explaining their results and decisions during their operation stage. Recently, researchers and designers have been asked to generate domain knowledge understandable by users in the AI system design phase.

The term explainable AI (XAI) was first coined in 2004 by Van Lent and his colleagues to describe the skill of the system they developed to explain the behavior of AI-controlled agents in simulation games. XAI is a scientific field that aims to study, design, and implement AI systems that are understandable to humans [\citealt{chap:10:AdadiandBerrada:2018}]. XAI is a set of methods and techniques that include learning algorithms that humans can comprehend and trust together with the results/{\allowbreak}decisions they generate. XAI is the opposite of ``black box'' systems in machine learning where users and sometimes even system designers cannot explain why an AI algorithm reached a certain decision or a specific result. Black box AI systems designed for decision-making are often based on machine learning over Big Data. They map data features into a class by predicting events or the behavioral traits of citizens without providing the reasons why they did it. This opaque approach is elusive not only for the lack of transparency but also for possible biases the black box algorithms inherit from human discrimination and/or errors originating (often hidden) in the training data, which may lead to unjust or erroneous decisions. As we discussed, machine learning techniques include supervised algorithms and unsupervised ones. In supervised learning, humans must train algorithms to distinguish particular features or patterns by using previously classified data. The controlled training step and the use of predefined classes increases the explainability of this class of algorithms, although it does not fully solve the black box problem. In unsupervised learning, an algorithm receives unlabeled data, and it needs to autonomously discover patterns or categories for itself. In this case, human intervention is limited or not required. Therefore, in this case algorithms may suffer from low explainability problems, thus more attention must be paid to avoid black box solutions that are not explainable.

For example, artificial neural networks encode training examples of a function in their connection matrix and extrapolate them to estimate the outcomes for data not in the training examples. Therefore, in real deep learning systems the matrix of connection weights among neurons can amount to gigabytes or terabytes of data. In a traditional software system based on explicit programming, an expert can locate the code segment responsible for the output and, if necessary, change or fix it. In contrast, in a neural network, if the human operator wants to know why the network generated an unexpected output, they see no specific algorithmic steps or methods but a very large matrix of weights. How the weights relate to the unexpected output is opaque. For this reason, finding effective solutions to make neural networks and deep learning systems less opaque and make their outcomes and decisions fully explainable is an open research topic that is receiving a lot of attention.

Generally, we can say that XAI is a real case of the social right to an explanation of how machines work and act on our lives. To be more specific, XAI helps humans describe machine learning model accuracy, fairness, transparency, and outcomes in decision-making. According to a definition provided by DARPA, XAI ``aims to create machine learning techniques that (i) produce more explainable models, while maintaining a high level of learning performance (prediction accuracy); and (ii) enables human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.'' The EU GDPR that we already discussed includes a right of explanation for citizens to achieve ``meaningful information of the logic involved'' when automated decision-making occurs with ``legal or similarly relevant effects'' on individuals. The implementation of the right of explanation cannot be successful if we do not have technologies and processes capable of explaining the logic of AI-based decision systems and in particular of software applications based on black box decision-making systems. The OECD Recommendation of the Council on Artificial Intelligence provided a set of principles and recommendations issued to promote an AI-powered crisis response that is responsible and based on human-centered values. This Recommendation includes a specific article (1.3) on transparency and explainability that calls on all relevant stakeholders to ``commit to transparency and responsible disclosure regarding AI systems.'' To this end, the Recommendation appeals to all the ``AI actors'':

\bgroup
\def\labelenumi{(\arabic{enumi})}
\begin{enumerate}
\item to foster a general understanding of AI systems;

\item to make stakeholders aware of their interactions with AI systems, including in the\vadjust{\vspace*{14pt}\pagebreak} workplace;

\item to enable those affected by an AI system to understand the outcome; and

\item to enable those adversely affected by an AI system to challenge its outcome based on plain and easy-to-understand information on the factors and the logic that served as the basis for the prediction, recommendation, or decision.
\end{enumerate}
\egroup

\citet{chap:10:Guidottietal:2019} correctly argue that a missing step in the production of machine learning models is the explanation of their logic, communicated in a comprehensible (human readable) manner that clarifies the model rationale and the potential biases learned by the model. The use of this approach allows machine learning designers, deployers, and users to understand and validate the system decision logic so as to understand the reasons an outcome/{\allowbreak}decision is produced/{\allowbreak}calculated. This gap impacts not only transparency but also accountability, ethics, and safety.

Multiple services and products include machine learning components. This occurs in complex systems such as autonomous cars, robotic systems, human judges' assistants, and personalized healthcare equipment; however, it also occurs in simpler devices such as smartphones, household appliances, and home automation. Approaches based on ``explanation by design'' and ``black box explanation'' may help in both application scenarios. In the first case, given a dataset of training decision records, a machine learning model must be developed together with its explanation. The second approach requires that, given the decision records produced by a black box AI model, an explanation on how to reconstruct the decisions must be associated with them. In the ``explanation by design'' scenario, the main goal is to deliver a transparent machine learning decision model by providing a readable explanation of the model's logic by design. In the ``black box explanation'' approach, the algorithm opacity problem can be solved by implementing methods for auditing machine learning algorithms and finding a clear explanation for their behavior. Considering the different efforts researchers and developers are putting in place to make algorithms explainable and comprehensible, they can be categorized into four different classes:

\bgroup
\def\labelenumi{(\arabic{enumi})}
\begin{enumerate}
\item global interpretability, or explaining the model;

\item local interpretability, or explaining the outcome;

\item black box inspection; and

\item transparent box creation.
\end{enumerate}
\egroup
\removelastskip{\vspace*{14pt}\pagebreak}

Explaining an AI model requires compiling a description of the global logic of its decision system, while, explaining the outcome is a form of ``local'' interpretation related to a specific use of the model. For example, in the case of personalized \hbox{decisions,} a personal interpretation is required. Black box inspection can take many forms, such as reconstructing how the black box system works internally and associating that functioning to the results. Finally, in transparent creation the goal is to construct a transparent box system that uses only ``visible'' predictors and does not include opaque or hidden ones.

\citet{chap:10:JovanovicandSchmitz:2022} proposed a list of explainability-related attributes that are useful for describing the learning system domains and to drive their design and implementation. Those attributes include:

\begin{itemize}
\item \textit{User}, which concerns main target users' levels of domain knowledge and skills of a learning algorithm.

\item \textit{Risk}, which describes the undesirable consequences that an algorithm decision can cause with users, such as a failure in task execution or behavior contrary to user goals.

\item \textit{Timeline}, which defines the timeliness of automated learning decision-making that considers if a system is asked to make real-time decisions or expose a slow response time.

\item \textit{Automation}, which defines the level of autonomy of a learning algorithm in decision-making; it corresponds to the degree of human assistance or intervention needed to enable algorithm decisions.
\end{itemize}

Explainability is also a desirable feature for the ethics of AI. \citet{chap:10:Floridietal:2018} proposed \textit{explicability} as a principle for AI ethics. They use the term to synthesize both explainability (i.e., the question ``how the system works?'') and accountability (i.e., the question ``who is responsible for the way it works?''). This may be debatable; however, explainability is surely a significant requirement for ethical AI as it has practical importance for stakeholders. Finally, in offering explanations about its logic and outcomes, a machine learning system must take into consideration to whom, why, and how its technical details should be communicated, which is critical for exposed and sensitive individuals or groups of people.

As discussed, explainability is a key factor for supporting accountability and transparency in AI systems. Explaining how AI systems make decisions helps users and controllers identify and moderate or eliminate biases that do not respect the ethical principles AI systems must respect. Without explainability,\vadjust{\vspace*{14pt}\pagebreak} it is difficult to identify the sources of unfair decisions or mistakes that may produce ethical consequences for users of machine learning systems.

\section{\label{sec:10.4}Some Final Remarks}

AI algorithms are very complex artifacts that involve human skills and machine learning capability. Algorithms do not make mistakes by themselves, it is humans who may make mistakes when designing, implementing, and using algorithms. These mistakes are included in algorithms and generate unfair behaviors that harm people. Numerous software systems released by Google, Microsoft, Amazon, and many other software corporations are composed of hundreds of thousands of lines of code. Maintaining full control of these extremely complex software systems is difficult. The same holds for reading this impressive amount of code and understanding the semantics of methods, procedures, or functions. Whereas algorithmic transparency requires that the input data and the mechanisms must be known, transparency does not necessarily demand that the outcome of the system be fair. For this reason, transparency must be combined with accountability and explainability. To make this combination effective and incisive, it needs to come with suitable governance organizations.

In many cases, learning systems based on collection and storage of large datasets, which extract from them models that are applied to individuals, are corporate trade secrets. In these cases, AI systems are out of the control of people and authorities. To avoid system opacity and enforce owners' responsibility, agencies or public bodies must be created to impose design principles, standards, and audits, and to implement the necessary policies. In many countries, legal systems are not able to adapt to very rapid technological change. With the acceleration of the development of AI systems, digital technology has become the de facto ruler affecting and influencing the lives of millions of people. This is a scenario that asks for public policies and bodies working on algorithmic accountability to guarantee that these powerful systems do not harm people and ensure they will be fair and transparent.

Governments, public bodies, and parliaments are requested to act proactively in setting up governance structures on algorithm transparency, accountability, and explainability. Some laws have been passed and some organizations have been created in recent years to achieves these goals. For example, in 2022 the EU DSA allowed researchers and users to conduct algorithm auditing. This will allow third parties to access AI systems for the first time. In the same year, a committee of the Portuguese parliament voted that\vadjust{\vspace*{14pt}\pagebreak} software companies must share with workers' councils the details of their algorithmic systems that affect employees. That decision follows a mass layoff at the TAP airline in 2021. The company ran an algorithm to choose which workers would be fired, but the process was ruled illegal in April 2022. The Alfred Landecker Foundation funded the project ``Auditing Algorithms for Systemic Risks,'' led by the AlgorithmWatch organization, to inspect algorithms and evaluate their impact on people. The project's goal is to generate more transparency and increase democratic control of automated decision-making systems. Audits are performed to study machine learning systems and identify risks and potential harm. Methods and governance proposals are developed to enable algorithm auditing on a large scale. The final goal of the project is to make the auditing process an effective tool that increases the transparency and accountability of software platforms. Other similar initiatives will be activated in the next few years around the world to address algorithm transparency and accountability and raise the awareness of people on these issues.

\begin{thebibliography}{00}

\bibitem[\hbox{Adadi and Berrada}(2018)]{chap:10:AdadiandBerrada:2018} A. Adadi and M. Berrada. 2018. Peeking inside the black-box: A survey on explainable artificial intelligence (XAI). \textit{IEEE Access} 6, 52138--52160. DOI:~\href{https://doi.org/10.1109/ACCESS.2018.2870052}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}ACCESS.{\allowbreak}2018.{\allowbreak}2870052}.

\bibitem[Bovens(2007)]{chap:10:Bovens:2007} M. Bovens. 2007. Analysing and assessing accountability: A conceptual framework. \textit{Eur. Law J.} 13, 4, 447--468. DOI:~\href{https://doi.org/10.1111/j.1468-0386.2007.00378.x}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1111/{\allowbreak}j.{\allowbreak}1468-{\allowbreak}0386.{\allowbreak}2007.{\allowbreak}00378.x}.

\bibitem[Burrell(2016)]{chap:10:Burrell:2016} J. Burrell. 2016. How the machine `thinks': Understanding opacity in machine learning algorithms. \textit{Big Data Soc.} 3, 1. DOI:~\href{https://doi.org/10.1177/2053951715622512}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1177/{\allowbreak}2053951715622512}.

\bibitem[Caplan et~al.(2018)]{chap:10:Caplanetal:2018} R. Caplan, J. Donovan, L. Hanson, and J. Matthews. 2018. \textit{Algorithmic Accountability: A Primer}. Data \& Society.

\bibitem[Diakopoulos and Koliska(2017)]{chap:10:DiakopoulosandKoliska:2017} N. Diakopoulos and M. Koliska. 2017. Algorithmic transparency in the news media. \textit{Digit. Journal.} 5, 7, 809--828. DOI:~\href{https://doi.org/10.1080/21670811.2016.1208053}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1080/{\allowbreak}21670811.{\allowbreak}2016.{\allowbreak}1208053}.


\bibitem[Floridi et~al.(2018)]{chap:10:Floridietal:2018} L. Floridi, J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, R. Madelin, U. Pagallo, F. Rossi, B. Schafer, P. Valcke, and E. Vayena. 2018. AI4People---An ethical framework for a good AI society: Opportunities, risks, principles, and recommendations. \textit{Minds Mach.} 28, 4, 689--707. DOI:~\href{https://doi.org/10.1007/s11023-018-9482-5}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1007/{\allowbreak}s11023-{\allowbreak}018-{\allowbreak}9482-5}.

\bibitem[Gebru et~al.(2021)]{chap:10:Gebruetal:2021} T. Gebru, J. Morgenstern, B. Vecchione, J. Wortman Vaughan, A. Wallach, H. Daum\'{e} III, and K. Crawford. December. 2021. Datasheets for datasets. \textit{Commun. ACM }64, 12, 86--92. DOI:~\href{https://doi.org/10.1145/3458723}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}3458723}.

\bibitem[Guidotti et~al.(2019)]{chap:10:Guidottietal:2019} R. Guidotti, A. Monreale, and D. Pedreschi. January. 2019. The AI black box explanation problem. \textit{ERCIM News }116, 12--13.

\bibitem[Jovanovi\'c and Schmitz(2022)]{chap:10:JovanovicandSchmitz:2022} M. Jovanovi\'c and M. Schmitz. February. 2022. Explainability as a user requirement for artificial intelligence systems. \textit{Computer} 55, 2, 90--94. DOI:~\href{https://doi.org/10.1109/MC.2021.3127753}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}MC.{\allowbreak}2021.{\allowbreak}3127753}.

\bibitem[Meijer(2014)]{chap:10:Meijer:2014} A. Meijer. 2014. Transparency. In M. Bovens, R. E. Goodin, and T. Schillemans (Eds.), \textit{The Oxford Handbook of Public Accountability}. Oxford University Press, Oxford, 507--524.

\bibitem[Wachter et~al.(2017)]{chap:10:Wachteretal:2017} S. Wachter, B. Mittlestadt, and L. Floridi. 2017. Why a right to explanation of automated decision-making does not exist in the General Data Protection Regulation. \textit{Int. Data Priv. Law} 7, 2, 76--99. DOI:~\href{https://doi.org/10.1093/idpl/ipx005}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1093/{\allowbreak}idpl/{\allowbreak}ipx005}.


\bibitem[Wieringa(2020)]{chap:10:Wieringa:2020} M. Wieringa. 2020. What to account for when accounting for algorithms: A systematic literature review on algorithmic accountability. In \textit{Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20)}. ACM, 1--18. DOI:~\href{https://doi.org/10.1145/3351095.3372833}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}3351095.{\allowbreak}3372833}.
\end{thebibliography}

%\end{document}

