%\documentclass{acm-book-v2}
%\RequirePackage[errorshow]{tracefnt}
%%\newcommand{\mpage}[1]{}
%%\newcommand{\indexfn}[1]{}

%%\usepackage{showframe}

%\usepackage{custom-tooltip}
%\usepackage{custom-tooltip-Alt-Text-View}



%\begin{document}

\setcounter{chapter}{2}

\chapter{\label{chap:3}Algorithms and Data---{\allowbreak}The Pillars of Our Future}

This chapter examines the relationships and synergies between algorithms and data. In particular, we introduce and discuss data structures, databases, web data, and Big Data concepts. The data science discipline created for exploiting Big Data in scientific discovery is also introduced. Large data storing/{\allowbreak}management/{\allowbreak}processing by smart and efficient algorithms is also illustrated through practical use cases in significant application domains such as digital health, smart cities, social media, and science.

\section{\label{sec:3.1}Data Storing and Abstract Structures}

The Merriam--Webster dictionary defines data as ``factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation.'' This very basic and general definition recalls that data are a main element in reasoning and computation. A second definition given by that dictionary is more relevant to the field of computing we are discussing here: data is ``information in digital form that can be transmitted or processed.'' Although raw data, also in digital form, should be considered as simple units that compose information, this latter definition is very useful for understanding the importance of data for computers and algorithms. Without input data that express facts, symbols, words, and numbers, computers cannot produce any result and algorithms cannot execute their operations. Upon completion, the ENIAC computer had 20 accumulators that operated as computational and storage devices. Each accumulator could store one signed 10-digit decimal number. That was the original way to represent and store data in the pioneering electronic computer. At that time, the ENIAC had no main memory; only in 1953 was a 100-word static magnetic-memory added. It was based on a binary-coded decimal number system and can be considered as the progenitor of the random-access memory (RAM) used today in all computers and digital devices.

While the term ``data'' is now very often associated to the dictionary's second definition that we mentioned, this word is much older than computers. It comes from Latin where \textit{data} is the plural of \textit{datum} meaning ``(thing) given or granted,'' the past participle of \textit{dare}, ``to give.'' In English, this word was rediscovered in the 17th century from its Latin roots and used in statistical studies, engineering, and other scientific fields. In the first decades of the 20th century, scientists and engineers worked on techniques and devices for storing data and information. Among others, we should mention an Austrian engineer, Fritz Pfleumer, who invented a technique for storing information magnetically on tape. Pfleumer, who was an expert in industrial processes for producing special papers, used thin cigarette paper to create a coated paper recording tape by replacing the bronze with a magnetizable medium. He patented the first audio tape recorder, which piqued the interest of German General Electric (AEG) who a few years later signed a contract with Pfleumer and afterward produced the world's first practical tape recorder, called the Magnetophon K1. The principles Fritz Pfleumer developed are still used today, with the vast majority of digital data being stored magnetically on computer hard disks.

Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This could be acceptable for devices such as desk calculators, digital signal processors, and other specialized devices, but it is an unacceptable limitation for general-purpose computers. Machines based on the von Neumann architecture include a memory in which they store both instructions and data. By exploiting that memory, digital computers are versatile in that they do not need to have their hardware reconfigured for each new operation/{\allowbreak}program to be executed. That memory is used to store the instructions/{\allowbreak}programs and data they must process every time it is needed.

A computer includes different types of memory where data are stored, of which three are the most important and popular: main memory, secondary storage, and tertiary storage. Main memory is the fastest and the only one directly accessible by the CPU, which, when running a program, reads the instructions and data stored there. Whereas main memory is volatile, secondary storage is persistent in storing data. Hard disk drives or solid-state drives are usually used as secondary storage. They can store much more data than main memory, typically two orders of magnitude more. Secondary storage is less expensive but thousands of times slower than main memory. Finally, tertiary storage is sometime used for archiving very large rarely accessed data as it is much slower and cheaper than secondary storage.

While in human history the most commonly used data storage medium was the paper used in books, journals, newspapers, and other similar sources of \hbox{information,} in the last ten years the most used data storage media are magnetic, semiconductor, and optical devices. The current limited usage of paper has been caused by the very wide diffusion of computers and digital devices where data and information are mainly stored and circulated. A significant effort was carried out by computer designers and scientists to define and implement appropriate mechanisms and formats for storing and accessing data in computers. Other than implementing the different types of hardware memory devices, abstract data types and data structures have been defined to model and represent the conceptual organization, structuring, and operations associated with data stored in a computer.

A data type is a feature of data that defines the values it can take and the operations that can be performed on it. Therefore, a data type is a formal way to specify what to store and how to handle data. Programming languages support a set of basic data types such as byte, integer (to represent integer numbers), floating-point (for real numbers), characters, and Booleans (to represent the two truth values true and false). A data type is a very useful way to assign a meaning to each data item that is stored in a computer memory and will be processed by the CPU. It is a practical concept to express and limit the possible values and the operations that can be done on data. For example, the Java programming language defines basic data types that include \textit{byte}, \textit{short}, \textit{int}, \textit{long}, \textit{float}, \textit{double}, \textit{boolean}, and \textit{char}. One of them will be associated to each single data element (a variable or a data record) stored in computer memory. For instance, the following data declarations specify a numeric variable (named \textit{index}) and a character (named \textit{reply}):

\begin{center}
\texttt{\textbf{\MonoBold int} index};

\texttt{\textbf{\MonoBold char}} \texttt{reply};
\end{center}

\noindent Arithmetic operations can be executed on the integer \textit{index} that stores a 32-bit signed integer, which can take a minimum value of $-2{}^{31}$ and a maximum value of $2{}^{31}-1$. Typical operations used to operate on the character variable \textit{reply} are copy, print, and concatenate.

In general, languages, operating systems, and other software systems allow programmers and users to utilize sets of data structures for storing and processing their data, which may be built from the combination of basic data types. A data structure is a collection of data values that may be of different types, relationships among them, and operations that can be applied to its data elements. For example, non-primitive data types in Java that are used to implement complex data structures are \textit{String}, \textit{Arrays}, and \textit{Classes}. Most high-level programming languages also allow developers to define additional composite data types. This can be done by combining multiple elements of basic types and defining the new operations (through functions or methods) of the new data type. Examples of composite data types are array, lists, records, sets, and unions. An array is a fixed-length or resizable collection of elements in a specific order, typically all of the same type. Array elements are accessed using an integer index to specify which element is required.\break A list is a linear collection of data elements of any type, called nodes, where each node contains a value and a pointer to the next node in the list. A list is more flexible than an array because elements can be inserted and removed without relocating the rest of the list. A record is an aggregate data structure whose elements, called fields, can have different basic types. A record element, for example, may be composed of three fields: an integer, a string, and a Boolean. The set data type is an implementation of the mathematical concept of a finite set; thus, it is a data type that stores unique values without any particular order. Operations such as \textit{union}, \textit{intersection}, and \textit{subset} are defined on variables of the set type. Finally, the union data structure defines which of a number of permitted primitive types may be stored in its instances, for example, \textit{float} or \textit{short} integer. Unlike a record, which can contain a \textit{float} and an integer, a union stores only one value at a time. In its implementation, adequate memory space is allocated to contain the widest member\break data type.

Data structures provide a means to efficiently compose, store, and manage large amounts of data that can be used in complex software applications such as scientific simulations, file systems, databases, information retrieval systems, and web services. They are used to organize storage and retrieval of information kept in the different types of memories used in computers, in particular in the main or secondary memory. As the size and complexity of data increase, data structures play a very important role in the development of software systems and applications. Effective data structures are vital for developing efficient algorithms. The most recent programming languages emphasize the role of data types and data structures, with respect to algorithms, as the crucial organizing element in the design of scalable software systems. Smart storage mechanisms and strategies of data processing are vital for the development of large-scale applications and services that deal with distributed data repositories, as occurs in cloud computing infrastructures, in several application fields.

\section{\label{sec:3.2}Data in Files}

A very important abstract structure to store and manage data is provided by files. A file is a collection of data handled by a computer as a unit and identified by its name. Files are used for storing data in secondary or tertiary storage and for purposes of managing input and output. Files are stored on disks or magnetic tapes and represent the used data abstraction for keeping data persistent in computers. Indeed, data and information stored in a file survive the termination of the program using them or when the computer is switched off. Files can be shared with and transferred between computers via removable media or the Internet. A file can contain a few data elements (words, numbers, program code, etc.) or it can be very large up to gigabytes or terabytes of data. Files enable users and applications to save, read, write, and modify data; they use different internal structures that may also help to find the data that matches certain criteria. Different types of files are designed for different purposes. However, on most current operating systems, files are organized into one-dimensional arrays of bytes. This simple structure allows for encoding different data formats using an appropriate combination of characters to define, for example, GIF or HTML files. A file may store an executable program, an image, a short or long text document, a video, or any other kind of combined data elements. The main goal of developing file structures and organization is to minimize the number of transfers from the disk to the main memory and vice versa, limiting disk access as much as possible.

The word ``file'' is derived from the Latin \textit{filum}, in English a thread. In modern times, a file defines a collection of  papers or publications, one that is usually arranged or classified. After the development of computers, the term file has been used in the context of computer storage to define a collection of data archived on a digital storage device. From the early 1950s, this term denoted information stored on computer hardware devices, including punched cards, vacuum tubes, and disk drives. About ten years later, operating systems, such as the Compatible Time-Sharing System (CTSS) developed at MIT and the Master Control Program (MPC) implemented at Burroughs included a module devoted to managing files on storage devices that was called a file system. Today, all operating systems include one or more file systems that organize the way files are stored on hardware devices---how they are named, stored, and retrieved.

A file system consists of two distinctive parts: a pool of files, each one storing data, and a set of directories that organize and provide information about all the files in the system. Directories are abstract structures that connect and group files in folders (each folder is called a directory) that virtually represent a wrapper for files. Directories then allow a user to group files into separate collections. Originally, directory structures were flat. The first implemented file system supporting hierarchies of directories was the MULTICS operating system. All operating systems nowadays include file systems that define hierarchies (such as trees or graphs) where directories may contain sub-directories. For example, Windows uses the FAT32 and NTFS file systems while Linux uses ext3, FAT32, and exFAT. Although file systems are data structures devoted only to storing and accessing data in secondary storage, they play a very important role in the management of data. In the era of Big Data, new file systems for parallel computers and distributed systems, such as NFS, HDFS, Google File System, and GPFS, were developed and used to store very large amounts of information in files residing on different disks and for allowing a set of programs to access the information stored in files concurrently.

Distributed and parallel file systems are used today to store large amounts of data in files distributed across multiple networked computers and to enable fast access through simultaneous, concurrent input/output operations between software applications and storage servers. A high-performance file system splits a large file and distributes, or stripes, its portions to several storage drives, which can be located on different remote servers connected in a local area network or through the Internet. Users do not need to know the physical location of the partitions to read or modify a file as the system uses a global namespace to facilitate file access. Storage capacity and bandwidth can be scaled to store huge amounts of persistent data in files, and services may include high availability,  fault-tolerance, and replication.

\section{\label{sec:3.3}Data in Databases}

A big step in storing and
processing data has been the development of databases. A~database is a well-structured collection of data and information specifically organized for efficient search and retrieval by a computer. Databases are computer-based archives structured to facilitate the storage, retrieval, modification, and deletion of data carried out by a set of data-processing operations. Apart from storing the data itself, a database also includes the relationships among the data elements that enrich data and support complex retrieval operations. Data, structures, and relations of a database can be stored on a set of files. However, databases, through data abstractions and their relations, offer a more efficient and better structured model for storing and querying data. Databases are designed and implemented by using a database management system (DBMS), which is a software program that allow users to define, create, query, update, and maintain access to a database. DBMSs encompass all the core facilities that have been provided to generate and administer a database. When a database is in operation, a DBMS serves as the interface between users or their applications and the database, permitting data retrieval, updates, optimization, and the overall management of data and information stored in the database.

Early database management systems appeared in the 1960s, and they were designed and used in business sectors such as banking, airline ticketing, commerce, telecommunications, and in complex scientific projects such as the Apollo program for the Moon landing. The Integrated Data Store (IDS) was one of the first database management systems; it was designed in the 1960s at the computer division of General Electric and became the basis for the CODASYL Data Base Task Group standards. The CODASYL model offered software applications the capability to navigate around a linked dataset. They can find data records by using a unique key or navigating data relationships from one record to another or scanning all the records in a sequential order. For this reason, databases using this approach were called navigational databases. The CODASYL model became, for about ten years, the paradigm used for implementing commercial database systems such as the IDMS database from Computer Associates and the multidimensional MDBS2 database system.

When using the CODASYL model, designers realized that the internal data structures were not transparent, which required programmers to navigate database structures to obtain information (i.e., specify the access path, describing all of the records and the sets that would be used). This problem was solved in a new DBMS model called the relational model, invented in 1970 by Edgard F. Codd, a computer scientist working at IBM. Codd published a set of scientific papers where he described a new approach to database definition and implementation. The most well-known is ``A relational model of data for large shared data banks'' [\citealt{chap:3:Codd:1970}], where Codd wrote ``This paper is concerned with the application of elementary relation theory to systems which provide shared access to large banks of formatted data. {\dots} The relational view (or model) of data appears to be superior in several respects to the graph or network model presently in vogue for non-inferential systems. It provides a means of describing data with its natural structure only---that is, without superimposing any additional structure for machine representation purposes. Accordingly, it provides a basis for a high-level data language which will yield maximal independence between programs on the one hand and machine representation and organization of data on the other.''


In contrast to CODASYL, where data records are stored in some sort of linked lists, Codd proposed to structure data in a number of tables, each table being used for a different type of entity (e.g., employee, salary, job, and company). Each table would contain a fixed number of columns containing the attributes of the entity. For the employee entity, we may have attributes such as surname, age, city, school degree, and so on. One or more columns of each table were designated as a primary key by which the rows of the table could be uniquely identified. Primary keys are used for cross-references among tables and queries would join tables based on these key relationships using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of tables (or relations) aimed to ensure that each item of information was only stored once, thus simplifying update operations.

After Codd's complete definition of the relational model, IBM worked on a prototype DBMS, called System R, which was based on those concepts. To manipulate and retrieve data stored in the System R database management system, Donald D. Chamberlin and Raymond F. Boyce developed the Structured Query Language (SQL) for programming and querying relational databases. SQL operations are based upon the relational algebra and tuple relational calculus used by Codd in his model. Among the programming concepts of SQL, the most important are \textit{queries}, which retrieve the data based on specific criteria, and \textit{statements}, which can have a persistent effect on schemata and data, or can be used to control transactions, program flow, connections, sessions, or diagnostics. An SQL query consists of three pieces, or blocks: the \textit{select}, \textit{from}, and \textit{where} blocks. The \textit{select} block tells the database which columns of data we want it to return. The \textit{from} block specifies which table(s) we want to search. The \textit{where} block allows us to search for records with certain characteristics. For instance, Figure~\ref{fig:3.1} shows the SQL query to be run if we want to retrieve all the names of people living in Rome who receive a salary of {\texteuro}100,000 or greater, from a database containing a table \textit{Persons} composed of five columns storing, respectively, the first name, surname, age, residence town, and salary of a list of people.

\begin{figure}[!b]
\begin{tabular}{l}
SELECT firstname, surname\\
FROM Persons\\
WHERE (town = `Rome') AND (salary >= 100,000)\\
\end{tabular}
\caption{\label{fig:3.1}An example of a simple SQL query on a relational database containing the table \textit{Persons }for retrieving the names of people living in Rome and receiving an annual salary higher than {\texteuro}100,000.}
\end{figure}

After the implementation of this relational DBMS, others such as INGRES, \hbox{Oracle} Database, and PostgreSQL were implemented and intensively used in the late 1970s. Examples of other well-known and widely used relational database systems developed in the 1980s are IBM Db2, MySQL, PostgreSQL, Microsoft SQL Server, and Microsoft Access. Indeed, by the early 1990s relational systems dominated all professional and significant data management applications. Moreover, as of the first two decades of the 2000s, they remain dominant, and the SQL database language for the relational data processing has influenced other database languages based on different data models. Among them are the object databases designed in the 1980s to overcome the difficulties of object--relational impedance mismatch, encountered when a relational RDBMS is being served by an application program written in an object-oriented programming language, such as Java or C++, because objects or class definitions used on object-oriented programs must be mapped to database tables defined by a relational schema. In some cases, the solution of this problem led to the development of hybrid object--relational databases.

The post-relational database systems in the late 2000s became known as NoSQL (for Not only SQL). Those database systems introduced fast key--value stores, graph-oriented, and document-oriented databases. They provide data models and mechanisms for storage and retrieval of data that are different from the tabular relations used in relational databases. The main goal of these new database management systems is to implement scalable databases in parallel and distributed computers for supporting Big Data applications that need to access and query very large datasets. The term NoSQL was coined around the year 2000 and in the succeeding years some NoSQL systems were developed mainly by private companies such as Amazon, Facebook, and Google and by open-source communities such as those working in the framework of the Apache Foundation, a non-profit corporation that supports several hundred open-source software projects. For example, in 2007 Amazon introduced its Dynamo distributed NoSQL system for internal use that was the inspiration for the implementation of the DynamoDB database service that supports key--value and document data structures. In fact, Amazon was one of the first big companies to store much of its important corporate data in non-relational databases.

As mentioned previously, different NoSQL database systems use different data models and methods. Their main advantages are that they can efficiently manage unstructured or semi-structured data such as social media posts, emails, videos and pictures, document files, geographical points and metadata, and other formats. NoSQL database systems are designed and used mainly for storing and querying unstructured data in efficient ways that require scalability when large amounts of data must be handled. Indeed, with relational databases users must convert all data into tables, and when the data doesn't easily fit into a table, the database's structure can be complex and slow to work with [\citealt{chap:3:Leavitt:2010}].

Several types of NoSQL database systems have been developed in the past 20 years; some of them overlap. Some of the most widely used are document-oriented, key-value, graph-based, and column-oriented databases. Document-oriented databases store and manage data as collections of documents rather than as tables having uniform-sized fields for each record. In document-oriented databases, users can define any number of fields of any length to a document. CouchDB is an example of this class; it was developed by the Apache Software Foundation as an open-source database system. MongoDB, RavenDB, and IBM Domino are other well-known document-oriented database systems. Key-value store database systems store data indexed for retrieval by keys. These systems can manage structured or unstructured data. They store the data as a single dense collection that may have different fields for every record. Using this unstructured approach, they need far less memory than relational systems to store databases, which can lead to performance gains. For example, Couchbase Server is a key-store system designed to provide scalable key-value and JavaScript Object Notation (JSON) document access. It is designed to be configured on a single computer or on several distributed servers when it needs to be used to store Big Data repositories.

Graph databases were designed by exploiting the mathematics of graphs, which models relations between entities that are represented as a set of vertices connected by edges. They use graph structures for semantic queries with nodes, edges, and properties to represent and store data. A graph database consists of a set of objects---each one can be a vertex (or node), an edge, or a property. Nodes represent data entities. They are comparable to a record, relation, or row in a relational database, or to a document in a document-store database. Edges connect a node to other nodes and represent the relationship between them. Edges are then the key concept in graph databases, representing an abstraction that is not directly implemented in a relational model or a document-store model. Properties are attributes associated to nodes. For example, if \textit{person} were one of the nodes, it might be associated to properties such as \textit{man}, \textit{age}, and \textit{home address}. Apache Giraph and Neo4j are two well-known graph database management systems.

Lastly, column-oriented databases store data tables by column rather than by row. They contain columns of closely related data rather than, as in relational databases, storing sets of data in a structured table of columns and rows with uniform-sized fields for each record. This structure assures more efficient data access when only querying a subset of columns and more choices for data compression. However, data insertion is typically less efficient. HBase is a distributed, open-source column database developed by the Apache Software Foundation that emulates Google's Big Table. Facebook created the high-performance Cassandra DBMS to store data from its social media platform.

Overall, we can say that NoSQL database systems are flexible enough to better enable developers to design databases in a way that meets their needs. In fact, NoSQL databases are often faster than relational ones because their data models are simpler. They don't have all the requirements that relational databases have. For example, they relax data consistency constraints that require the data values in the different copies of a data store to be kept consistent, enabling better performance and scalability for certain classes of applications. For more traditional applications, NoSQL databases won't replace relational ones. However, the many NoSQL solutions allow users to look at their data and select the most \hbox{appropriate} database management system they need, finding the best tradeoff between functionality and performance.

\section{\label{sec:3.4}Data in the Web}

Despite the big effort by database experts and users to model and organize data in a very structured way and provide efficient query mechanisms for finding the right information stored in those systems, in the early 1990s a momentous event significantly changed the path of computer data storing and processing technologies. In the 1960s, the American scholar Theodor H. Nelson introduced the hypertext concept as text shown on a computer display with references, called hyperlinks, to a number of other texts that a reader can directly access. This concept can be considered as a particular application of the hypergraph mathematical structure, which is a generalization of a graph in which an edge can link any number of vertices. In December 1968, Douglas C. Engelbart demonstrated at Stanford Research Institute the first hypertext software interface. From then until the end of the 1980s, several hypertext software systems were implemented and run on traditional sequential computers. Let us consider that when stored on a computer hypertext documents are interconnected by software hyperlinks, which are typically activated by a mouse click, a screen touch, or pressing a key on the keyboard. Hypertexts are often used to link plain or formatted text, tables, images, videos, and data of different formats.

The real change in the exploitation of hypertext systems occurred in 1989 in Geneva, where Tim Berners-Lee, an English computer scientist working at CERN, proposed and implemented a new distributed hypertext system for addressing the need for a simple information-sharing facility raised by physicists working at CERN and connected to many research centers and universities in the world. Tim Berners-Lee called the project ``WorldWideWeb.'' It was the first time that someone proposed to connect hypertexts with the Internet, which had already existed for around 20 years. Through this intelligent combination, Berners Lee created the web, the most used Internet service that offered a very simple interface to access the largest source of data and information created by humans and stored on billions of computers. ``Most of the technology involved in the web, like the hypertext, like the Internet, multifont text objects, had all been designed already. I just had to put them together. It was a step of generalizing, going to a higher level of abstraction, thinking about all the documentation systems out there as being possibly part of a larger imaginary documentation system'' [\citealt{chap:3:Berners-Lee:1990}].

On December 1990, Berners-Lee published the first website that was accessible to the Internet from the\vadjust{\vspace*{12pt}\pagebreak} CERN network. That website provided an explanation of what the World Wide Web (WWW) was and how people could use a browser or set up a web server. The WWW grew very quickly and after about thirty years the estimated active websites were about 1.2 billion, containing more than 50 billion web pages. As a curiosity, in June 2021 the web's source code developed by Berners-Lee was auctioned by Sotheby's in London and sold for US\$5,434,500 to fund benefit initiatives by him and his wife. As declared in the Sotheby's website [\citealt{chap:3:Sotheby:2021}], it includes the ``Original archive of dated and time-stamped files containing the source code, written between 3 October 1990 and 24 August 1991. These files contain code with approximately 9,555 lines, the contents of which include implementations of the three languages and protocols invented by Sir Tim; HTML (Hypertext Markup Language); HTTP (Hyper Transfer Protocol); and URIs (Uniform Resource Identifiers), as well as the original HTML documents that instructed early web users on how to use the application.'' This description contains an error in the HTTP explanation. Indeed, it is not the ``Hyper Transfer Protocol,'' but the Hypertext Transfer Protocol defined by Berners Lee as the application protocol for the distributed and collaborative exchanging of hypertext documents that composed the web pages. Together with it and the syntax of URIs, which are unique sequences of characters that identify resources used by web technologies (i.e., web pages), the third pillar of the web infrastructure is the HTML language.

HTML is the standard markup language for composing hypertext documents designed to be downloaded from a website and displayed on a web browser. The HTML components, including tags, text-based data types, characters, and entity references, have been defined to describe the structure of a web page. HTML tags, tables, images, videos, and other objects may be embedded into a rendered page. Therefore, HTML provides a linguistic tool to create structured documents by composing hypertext elements such as headings, paragraphs, lists, links, quotes, and other elements. HTML elements are delineated by tags, written using angle brackets. Tags such as $<$img$>$, $<$link$>$, and $<$table$>$ directly introduce content objects, such as images and tables, and hyperlinks into a web page. Other tags such as $<$p$>$ and $<$br$>$ are used to format document text and may include other tags as sub-elements. Web browsers are able to read and interpret the HTML source pages and visualize content of a page in a graphical format.

From the point of view of data storage and representation, HTML defines many data types as specializations of character data. They include script data, stylesheet data, names, URIs, numeric values, language types, media descriptors, colors, character encodings, dates and times, and many others. Following this approach, the web soon became the main repository of unstructured and semi-structured data, making it difficult to automatically process and query the great vastness of its billions of pages. In particular, semi-structured data stored in web pages do not follow the tabular structure of data stored in relational databases or other data structures used in NoSQL databases. In semi-structured data repositories, data items belonging to the same class can have different attributes even though they are grouped together and the order of attributes is not significant. The combinations of semi-structured and unstructured data have increased since the advent of the web and require different and new ways to browse, extract, pre-process, and analyze web data with respect to those stored in databases. For this reason, new markup languages such as JSON and Extensible Markup Language (XML) have been developed together with new techniques and algorithms for data processing such as web crawlers or spiders and web data scrapers. With the advent and extensive use of social media, the use of these techniques has been extended from web pages to the extraction and organization of social data items such as posts, tweets, and likes that billions of users publish on social media platforms.

Although database specialists have worked to design and implement techniques and systems for the management of well-structured data, the very wide use of Internet services and applications, and in particular the web and social media, has led to the creation of very large amounts of data stored on our computers and in the cloud in semi-structured and unstructured formats that are more complex to access and manage.

\section{\label{sec:3.5}The Era of Big Data}

The World Economic Forum estimated that at the beginning of 2020 the number of bytes in the digital universe was 40 times bigger than the number of stars in the observable universe. According to the Statista Research Department, the total amount of data created, stored, and exchanged globally in 2020 reached about 65 zettabytes, which is $65\times10{}^{21}$ bytes, or 65 billion terabytes. This extraordinary number is forecast to increase rapidly, and in 2025, the total amount of data stored in our computers and digital devices around the globe is projected to grow to about 180 zettabytes. The Statista specialists reported that the growth of data was higher than previously estimated because of the COVID-19 pandemic, since more people worked and studied from home and used home entertainment services more often. More than 90\% of data available in the world was generated within the past ten years. From 2020 to 2025, the IDC forecasts new data creation to grow at a compound annual growth rate of 23\%, resulting in approximately 175 zettabytes by 2025, a value that is similar to the Statista forecast.

Some people contend that data are the oil of the third millennium. Actually, data have created an immense market that is reaching impressive values. According to the research report ``{`Big Data Market'} information by technology, by organization size, by deployment, by end-user and region---forecast to 2030'' by Market Research Future, the data market size will reach US\$297.28 billion, growing at a compound annual growth rate of 14.52\% by 2030. By 2025, more than a quarter of data created will be real time in nature, and Internet of Things (IoT) real-time data will represent over 95\% of it. Actually, data are present wherever there is a memory device: in the disks of our computers, in smartphones, on the web, in social networks, in the private and public cloud platforms, in sensor networks, in digital cameras, in laboratory instruments, and in IoT devices. They are Open Data, Personal Data, Hyperdata, Linked Data, and other data models that will come in time. As mentioned, data hold a great economic potential and are responsible for creating the most valuable brands in the world such as Google, Facebook, and Amazon. Companies and organizations who will exploit data by analysis and learning tools will be able to benefit from a better understanding of phenomena, processes, behaviors, and trends. The IDC estimates that the amount of data subject to data analysis will grow by a factor of 50 to 5.2 zettabytes in 2025. However, these numbers say that a very small percentage of available data is used as input source for data analysis and machine learning tasks, thus dispersing most of its value.

Many definitions have been proposed for the Big Data term. Most of them, however, refer to the main features identified by Doug Laney, an analyst of the META Group, now Gartner, in its report published in 2001: large volume, high velocity, and large variety [\citealt{chap:3:Laney:2001}]. The term Big Data was not actually used in the Laney report; however, he clearly identified the rising trend of large repositories of data stored on digital devices and the ``management challenges along three dimensions: volumes, velocity, and variety. \dots\ In 2001/02, IT organizations must compile various approaches to have at their disposal for dealing with each.'' In more detail, Big Data refers to massive, heterogeneous, and often unstructured digital content that is difficult to process using traditional data management tools and techniques. The definition provided by the Gartner glossary is as follows: ``Big Data is high volume, high velocity, and/or high variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation.'' The NIST Big Data Public Working Group added variability as a fourth dimension [\citealt{chap:3:NIST:2015}] and wrote that ``Big Data consists of extensive datasets---primarily in the characteristics of volume, variety, velocity, and/or variability---that require a scalable architecture for efficient storage, manipulation, and analysis.''

Volume is about the impressive amount of data produced, stored, and exchanged through the Internet. As computers, networks, and human interactions on digital systems generate data, the volume of available data today is massive. Data velocity deals with the pace at which data flow in from their many sources. The flow of data on the Internet is continuous and extremely fast. Variety refers to the several different sources and types of structured and unstructured data. In the past, we used to store data from sources such as spreadsheets and databases; now data are generated through web pages, emails, social posts, photos, likes, PDFs, memes, videos, and voice recordings. This variety of data presents complications for collecting, storing, and analyzing digital data. Another characteristic that has been considered for Big Data is veracity, which deals with how accurate or truthful a dataset can be and includes how trustworthy the data source, type, and processing of it is. Therefore, dealing with data veracity also means removing bias, errors, inconsistencies, and null values.

While we need to deal with the proliferation of databases, web portals, audio and video streams, tweets, posts, and blogs that generated the Big Data phenomenon, we should recognize that data themselves are not necessarily important per se. Data become very important when we are able to extract value from them, thus adding a new ``V'' to Big Data properties: value. This new feature is obtained by reducing data volume and by identifying patterns, trends, and hidden information in large data repositories that may bring value to analysts and users. Big Data are the ideal companions for data analysis algorithms that without data could not compute results, make decisions, and solve problems. They represent very precious collections of contexts to be deciphered, hypotheses to be exhibited, and incessant traces that are very useful for people who know how to use them, for experts who are capable of sophisticated computations and, when necessary, of acute manipulations.

The Big Data scenario required a shift in data processing architectures from traditional systems based on vertical scaling, which exploit faster processors, main memory, or disks, to horizontal scaling, which can be implemented by adding more processors, computers, or machines to be used concurrently for running in parallel several programs/{\allowbreak}threads that may analyze different partitions of data simultaneously. This parallelization strategy in the past was mainly used for computationally intensive applications in scientific simulation and modeling; now it has been extended to data analysis and has created a new research and development field called high performance data analysis. Through the splitting and mapping of algorithms and data across independent processors, data scientists can greatly improve Big Data analysis, both managing very large datasets and reducing global processing time. Big Data analysis needs parallel computing infrastructures such as HPC systems, cluster computers, and clouds to run parallel algorithms for scaling data processing across a large number of storage and computing resources.

Big Data computing considers the world as the biggest digital laboratory. The task, which appears increasingly evident, is to compute real phenomena by exploiting those gigantic amounts of data, thus offering professionals, research teams, companies, and organizations extraordinary new opportunities. Nevertheless, it contains within itself the risk of marginalizing the processes and decisions related to actual quality, to the real meaning of things. To avoid this purely measurable horizon, the use of Big Data necessarily requires that we focus on the description of qualitative and not only quantitative elements of life and of natural phenomena.

\section{\label{sec:3.6}Data Science}

The Big Data phenomenon has prompted the
scientific community to reexamine research methods and processes by reorienting them toward insights obtained from the analysis of data. In 2007, Jim Gray, a Turing Award winner, addressed the separation of data-intensive science from computational science. He called for a paradigm shift in the computing architecture and large-scale data processing platforms known as the Fourth Paradigm [\citealt{chap:3:Heyetal:2009}]. Experiments, study of theorems and laws, and simulation were in chronological order in the previous three scientific paradigms. Tony Hey and co-authors argued that this new paradigm not only represents a shift in the methods of scientific research but also a shift in the way people think. He declared that the only way to deal with the challenges of this new paradigm is to build a new generation of hardware and software systems to efficiently manage, analyze, and visualize large amounts of data produced by this impressive data deluge. Stimulated by continuous and extraordinary advancements in processing power, communication bandwidth, storage, and an unprecedented wealth of data, Big Data processing solutions based on scalable algorithms and large datasets were developed to deal with the increasingly complex data science tasks.

The intensive use of computers, algorithms, and Big Data for scientific discovery processes has led to the development of \textit{e-science}, in which scientific methods have changed significantly through the use of computational methods and new data management and analysis strategies. A thousand years ago, science was empirical, and its main goal was describing natural phenomena. A few hundred years ago, the theoretical branch was born, which started to use models and generalize. Thanks to computers, in the last century a computational branch of science was created based on simulation of complex phenomena. Today, e-science unifies theory, experiments, and simulations to support data-intensive scientific discovery. Very large sets of data are produced and shared through digital machines and instruments or are generated by simulators. They are then processed by data analysis or machine learning techniques, and the information/{\allowbreak}knowledge they produce is stored in computer archives and on the Internet. In e-science, frameworks specialists analyze data using data science methods, powerful computers, and large-scale distributed platforms. The final paragraph of the edited version of the last talk by Jim Gray included in \textit{The Fourth Paradigm} [\citealt{chap:3:Heyetal:2009}] summarizes in a very clear way how computing changed science: ``I wanted to point out that almost everything about science is changing because of the impact of information technology. Experimental, theoretical, and computational science are all being affected by the data deluge, and a fourth, `data-intensive' science paradigm is emerging. The goal is to have a world in which all of the science literature is online, all of the science data is online, and they interoperate with each other. Lots of new tools are needed to make this happen.''

A very recent key discipline for this new paradigm that supports scientific discovery by the intensive use of computers, algorithms, and data is \textit{data science}. Data science combines computer science, applied mathematics, and data analysis techniques to provide insights based on large amounts of data. Data science improves discoveries by founding decisions on insight extracted from large datasets through the use of scalable algorithms for collecting, cleaning, transforming, and analyzing (Big) data. Although data science is a young discipline, its history began in the 1960s. In 1966, Peter Naur coined the term ``Datology'' as the ``science of nature and use of data,'' and in 1974, he used the term data science as the ``The science of dealing with data, once they have been established, while the relation of the data to what they represent is delegated to other fields and sciences.'' However, the first scientific conference mentioning data science in its name was the IFCS Conference in Data Science, Classification, and Related Methods held in 1996. Seven years later, the \textit{Journal of Data Science} was established to advance and promote data science methods, computing, and applications in all scientific fields.

In data science, algorithms play the same role that equations play in mathematics. Data gathering and collection is the first step in a data science process; however, it is not limited to the traditional data collection performed in statistics. Data science not only uses traditional data collection methodologies but it also uses data that are already available; that is, data produced for other goals that are accurately selected and pre-processed can be used for the analysis of scientific and business processes. In general, having more data helps. However, having the right data is the most important requirement. For this reason, most of the time spent in the data science processes (around 60--70\%) is for collecting and preparing data. In his survey on data science, Longbing Cao summarized the most used definitions for data scientist [\citealt{chap:3:Cao:2017}]. The US National Science Board defines data scientists as ``the information and computer scientists, database and software engineers and programmers, disciplinary experts, curators and expert annotators, librarians, archivists, and others, who are crucial to the successful management of a digital data collection.'' A report from the US Committee on Science of the National \hbox{Science} and Technology Council defines data scientists as ``Scientists who come from information or computer science backgrounds but learn a subject area and may become scientific data curators in disciplines and advance the art of data science. Focus on all parts of the data life cycle.'' Finally, the Joint Information Systems Committee defined data scientists as ``people who work where the research is carried out, or, in the case of data center personnel, in close collaboration with the creators of the data and may be involved in creative inquiry and analysis, enabling others to work with digital data, and developments in data base technology.''

A Venn diagram proposed by Drew Conway (see Figure~\ref{fig:3.2}) is often used to describe the overlapping skills needed by data scientists. The Mathematics and Statistics sphere includes knowledge of methods and tools such as probability theory, algebra, and stochastic analysis. The diagnosis of the problem is performed by applying various mathematical and statistical approaches to the available data. Math expertise is important because it helps scientists choose the correct procedure to solve their problems depending on the data they have. The Computer Science sphere refers to coding and programming skills needed for gathering and preparing the data and for applying learning and statistics algorithms to the problems. Finally, the Domain Expertise sphere completes the required skills as it implies the knowledge of the particular field in which people are working. Data scientists must know about the goals of that field, several methods, and the constraints that they will be dealing with.

\begin{figure}[!b]
\tooltip{\includegraphics{graphics/Chapter_03/Figure2.\image}}{Venn diagram showing three intersecting circles, one labeled Computer Science, another labeled Math \& Statistics, and the third one labeled Domain Expertise. The area of intersection is labeled Data Science.}[-240pt,-280pt]
\caption{\label{fig:3.2}The data science Venn diagram. It is based on the combination of three set of skills that contribute to provide the needed expertise of data scientists.}
\end{figure}

Data science processes aim to transform a problem into a solution by exploiting data, computational techniques and infrastructures, and analysis techniques. Typically, a data science process includes a set of steps as follows:

\bgroup
\def\labelenumi{(\arabic{enumi})}
\begin{enumerate}
\item \textit{Framing the problem}---Understanding and formalizing a problem will help data scientists build an effective model that can be appropriate and successful. At the end of this step, all of the information and context needed to address and solve the problem must be acquired.

\item \textit{Collecting the needed data}---This step involves realizing what data are needed and finding ways to gather that data from one or more sources such as file systems, databases, data warehouses, data repositories, social media, and web pages.

\item \textit{Processing the data for analysis}---This step is used to pre-process and clean raw data to ensure that the data is in the correct format for the analysis step. Inconsistencies and errors must be identified and dealt with appropriately.

\item \textit{Exploring the data}---Exploratory data analysis looks at and analyzes datasets and summarizes their main features. This step defines how to process and represent data, making it easier for data scientists to discover patterns, identify anomalies, or test hypotheses.

\item \textit{Performing in-depth analysis}---This step aims to produce the data model. Data mining, machine learning, statistical techniques, and mathematical algorithms are used to extract insights and predictions from data. Predictive and/or descriptive data models are eventually produced.

\item \textit{Communicating results of the analysis}---The final step is carried out to describe the results of the entire process. Proper communication in data science processes will put into action the discovered models and solutions of the addressed problem. Thus, communicating the findings in a clear way will highlight their value.
\end{enumerate}
\egroup

A major issue to be faced in the data exploration, processing, and analysis tasks used in data science applications is the time-consuming process of identifying and training an adequate model. Therefore, data science is a highly iterative exploratory process where most scientists work hard to find the best model or algorithm that meets their data challenge. In practice, there is no one-model-fits-all solution. Indeed, there is no single model or algorithm that can handle all dataset varieties and changes in data that may occur over time. All data analysis and machine learning algorithms require user-defined inputs to achieve a balance between accuracy and generalizability. This task is referred to as parameter tuning; it is a long and complex process that must be managed.

As we already mentioned, distributed systems, clouds, and HPC infrastructures provide the appropriate computing systems that underpin Big Data analysis by exploiting parallel programming algorithms and tools. Multidisciplinary skills and knowledge in both scalable computing and data science help to unlock the knowledge contained in the increasingly large, complex, and challenging datasets that are now generated across many areas of science and business. The integration of scalable computing resources, software tools, networking, data, information management, and expertise of data scientists using machine learning and data analysis algorithms provides the required combination for solving new problems and building new scientific disciplines around data.

The world is progressively moving toward a data-driven society where data are the most valuable asset. The proliferation of Big Data and big computing have boosted the adoption of machine learning and data science across several application domains. Efficient and optimized distributed and parallel in-memory and disk-based computing platforms for complex data analysis jobs are crucially required to tackle this challenge. Moreover, data science and data-oriented programming languages are needed to provide complex abstract structures and operations that must be close to problem formulation and data format and organization. Exploitation of efficient machine intelligence and data analysis is of great importance for solving complex and challenging problems. In this way data science is creating a new age of discovery.

\section{\label{sec:3.7}Data-intensive Applications}

Different studies report that in 2020 more than 2.5 quintillion bytes of data were generated every single day. People exchanged about 306 billion emails per day in 2020 while 294 billion were sent in 2019. In 2020, 3.5 billion searches were made on Google; in 2021, the searches were about 6 billion. In 2020, 432,000 hours of video were uploaded on YouTube per day, whereas in 2022 YouTubers uploaded around 720,000 hours of video content per day. These are just a few examples of the data deluge we are experiencing.

Because of this continuous and explosive growth of data, many data-intensive applications require exploiting scalable data storage and data analysis solutions. A well-known example of a Big Data application is the ATLAS detector at the Large Hadron Collider at CERN in Geneva. The ATLAS infrastructure has a capacity of 200 petabyte of disks and 300,000 cores, with more than 100 computing centers connected via 10 gigabits-per-second links. The data collection rate is very high and only a portion of the data produced\vadjust{\vspace*{-18pt}\pagebreak} by the collider is stored. Several teams of \hbox{scientists} run complex applications to analyze subsets of those huge volumes of data. The collection and analysis tasks would be impossible without a high-performance infrastructure that supports data storage, communication, and parallel processing. Also, computational astronomers are collecting and producing larger and larger datasets each year that without scalable infrastructures cannot be stored and processed. One significant example is the Energy Sciences Network (ESnet), the US Department of Energy's high-performance network managed by Berkeley Lab, which in late 2012 rolled out a 100 gigabits-per-second national network to accommodate the growing scale of scientific data. From 1990 to 2019, ESnet's average traffic has grown by a factor of 10 every four years.

If we go from science to society, social data and e-health are good examples to examine. Social media platforms, such as Facebook, Instagram, TikTok, and Twitter, have become very popular and are receiving increasing attention from the research community because, through the huge amount of user-generated data, they provide valuable information concerning human behavior, habits, and travels. When the volume of data to be analyzed is of the order of terabytes or petabytes (billions of tweets or posts), scalable storage and computing solutions must be used, but no clear solutions exist today for the analysis of very large datasets. The situation is the same in the e-health domain where huge amounts of patient data are available and can be used for improving therapies, for forecasting and tracking of health data, and for the management of hospitals and health centers. Very complex data storage and analysis in this area will need novel hardware and software solutions.

The widespread diffusion of sensors, webcams, and mobile devices and the availability of networking infrastructures is enabling the collection of big and heterogeneous data that are produced daily in urban spaces and the development of smart city services. Such data pertain to the mobility of people or vehicles, air quality, safety issues, water/electricity consumption, and so on. They represent useful resources for improving urban services and environments. This is stimulating the implementation of several applications and systems in the field of urban computing, an interdisciplinary scientific field that pertains to the study and application of computing technology in urban environments. The smart city paradigm aims to plan and design future urban territories in a more efficient way. Smart cities rely on the adoption of sensing technologies, databases, IoT systems, ubiquitous devices, wireless networks, and all those frameworks that are used for sensing cities and territories, for collecting data, and for acting on the basis of the application's logic. In order to process the large amount of data being produced, smart city applications are supposed to use multiple hardware and software technologies including parallel system architectures and clouds. Due to the specific nature of smart city software solutions, data and\vadjust{\vspace*{-18pt}\pagebreak} objects are strictly related to the space or territory in which they are defined and used. Indeed, environmental information extracted from sensors, data inherent to the neighborhoods and residential units in a city, and information about traffic and mobility are specifically descriptive of the city area where they have been gathered and stored.

The innovative organization of smart cities is one of the most important application scenarios where Big Data management is combined with IoT technologies. The main challenge is to harness the collaborative power of networks (networks of people, of knowledge, and of sensors) and use the resulting data and collective intelligence to implement better-informed decision-making processes and empower citizens, through participation and interaction, to adopt more sustainable individual and collective behaviors and lifestyles. For example, the collection of mobile phone data and the discovery of mobility models is one of the most challenging issues in urban computing and can help achieve several smart city objectives. Transportation analysis based on mobile phone data can be applied for \hbox{estimating} road traffic volume and transport demands, to forecast the public's future demand for taxis, or infer the origins of tourists and visitors. \hbox{Analysis} tasks can identify collective movement patterns to support city managers in transport planning, intelligent traffic management, route recommendations, and smart parking services. We can also mention infotainment applications that provide community services (e.g., point-of-interest notifications, electronic and financial services, media downloading, and parking zone management). These applications are built by the transmission, collection, and analysis of a huge amount of data exchanged among people and road units and are very useful for an efficient provisioning of such services.

The array of business and scientific activities that currently use very large amounts of data is very numerous and every day new applications are added to the list. For example, banking and securities are among the top business sectors using Big Data. The securities industries rely on Big Data for risk analytics, anti-money laundering, demand enterprise risk management, and fraud mitigation. Retail traders, banks, hedge funds, and other companies operating in the financial markets use Big Data for high-frequency trading, card fraud detection, archival tracking of audit trails, enterprise credit risk reporting, pre-trade decision-support analytics, sentiment measurement, and predictive analytics.

If we move to the scientific field, one of the latest extreme data applications is provided by the James Webb Space Telescope(JWST), which is already changing humanity's knowledge of the universe. The JWST is a large infrared telescope launched on an Ariane 5 rocket from French Guiana on December 25, 2021. It is a major optical telescope in space, and it has an approximately 6.5-meter primary mirror. The JWST will be the chief observatory of the next decade, providing data to thousands of astronomers worldwide. It will be used to study every phase of the history of the universe, ranging from the first luminous glows after the Big Bang to the formation of solar systems like our own, with a planet capable of supporting life.

The telescope is parked in a point of gravitational equilibrium located about 1.5 million kilometers beyond Earth. Being so far away from our planet means that the data it sends has a long way to travel to reach us correctly. It also means that the communications subsystem needs to be reliable and fast. The data collection and transmission rates of the JWST overshadow those of the older Hubble telescope. The JWST can produce up to 57 gigabytes daily, collecting up to 1.7 terabytes each month. Any data it collects need to be stored on board because the spacecraft doesn't maintain nonstop contact with Earth. Gathered data are stored within the spacecraft's 68 gigabyte solid-state drive and every day they are transmitted to Earth and removed from the local disk to create space for new data. By the end of the JWST's mission life, estimated at ten years, it is expected to have collected and sent to Earth about 210 terabytes of data. This huge amount of data will represent an extraordinary source of information that may change astronomy and for the first time allow scientists and ordinary people to observe some of the first galaxies created just after the universe began.


\begin{thebibliography}{}

\bibitem[Berners-Lee(1990)]{chap:3:Berners-Lee:1990} R. C. T. Berners-Lee. 12 November. 1990. WorldWideWeb: Proposal for a HyperText project. [Online]. Retrieved July 25, 2022. \href{https://www.w3.org/Proposal.html}{https://{\allowbreak}www.{\allowbreak}w3.{\allowbreak}org/{\allowbreak}Proposal.{\allowbreak}html}.

\bibitem[Cao(2017)]{chap:3:Cao:2017} L. Cao. 2017. Data science: A comprehensive overview. \textit{ACM Comput. Surv.} 50, 3, 1--42. DOI:~\href{https://doi.org/10.1145/3076253}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}3076253}.

\bibitem[Codd(1970)]{chap:3:Codd:1970} E. F. Codd. 1970. A relational model of data for large shared data banks. \textit{Commun. ACM} 13, 6, 377--387. DOI:~\href{https://doi.org/10.1145/362384.362685}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}362384.{\allowbreak}362685}.

\bibitem[Hey et~al.(2009)]{chap:3:Heyetal:2009} T. Hey, S. Tansley, and K. Tolle (Eds.). 2009. \textit{The Fourth Paradigm: Data-Intensive Scientific Discovery}. Microsoft Research.

\bibitem[Laney(2001)]{chap:3:Laney:2001} D. Laney. 2001. 3D data management: Controlling data volume, velocity, and variety. META Group Research Note 6, Stamford, CT.

\bibitem[Leavitt(2010)]{chap:3:Leavitt:2010} N. Leavitt. 2010. Will NoSQL databases live up to their promise? \textit{Computer} 43, 2, 12--14. DOI:~\href{https://doi.org/10.1109/MC.2010.58}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}MC.{\allowbreak}2010.58}.

\bibitem[NIST Big Data Public Working Group(2015)]{chap:3:NIST:2015} NIST Big Data Public Working Group. 2015. \textit{NIST Big Data Interoperability Framework: Volume 1, Definitions}. NIST, Gaithersburg, MD.

\bibitem[Sotheby's(2021)]{chap:3:Sotheby:2021} Sotheby's. June. 2021. Sir Tim Berners-Lee Source Code for the WWW. [Online]. Retrieved July 25, 2022. \href{https://www.sothebys.com/en/buy/auction/2021/this-changed-everything-source-code-for-www-x-tim-berners-lee-an-nft/source-code-for-the-www}{https://{\allowbreak}www.{\allowbreak}sothebys.{\allowbreak}com/{\allowbreak}en/{\allowbreak}buy/{\allowbreak}auction/{\allowbreak}2021/{\allowbreak}this-{\allowbreak}changed-{\allowbreak}everything-{\allowbreak}source-{\allowbreak}code-{\allowbreak}for-{\allowbreak}www-{\allowbreak}x-{\allowbreak}tim-{\allowbreak}berners-{\allowbreak}lee-{\allowbreak}an-{\allowbreak}nft/{\allowbreak}source-{\allowbreak}code-{\allowbreak}for-{\allowbreak}the-www}.

\end{thebibliography}

%\end{document} 