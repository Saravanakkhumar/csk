%\documentclass{acm-book-v2}
%\RequirePackage[errorshow]{tracefnt}
%%\newcommand{\mpage}[1]{}
%%\newcommand{\indexfn}[1]{}

%%%\usepackage{showframe}

%\usepackage{custom-tooltip}
%\usepackage{custom-tooltip-Alt-Text-View}



%\begin{document}

\setcounter{chapter}{3}

\chapter{\label{chap:4}The Age of Machine Intelligence}


\noindent {This chapter discusses Turing's intuition on how machines can show \hbox{intelligent} behavior and how machines became ``intelligent'' by running learning algorithms on large datasets. After introducing how artificial intelligence was born, the \hbox{chapter} discusses the concepts of data-driven induction, data analysis, machine learning algorithms, and deep learning models. Artificial intelligence models aimed at implementing thinking machines are discussed and some real-world examples are described.}

\section{\label{sec:4.1}Machine Intelligence}

In 1983, \textit{Time} Magazine, in a break with its traditional feature known as ``Man of the Year,'' named the personal computer as ``Machine of the Year.'' The cover of that issue shows a white plaster man by sculptor George Segal contemplating a computer. In introducing the issue, \textit{Time} publisher John A. Meyers wrote, ``Several human candidates might have represented 1982, but none symbolized the past year more richly, or will be viewed by history as more significant, than a machine: the computer.'' This choice denoted a historic change that recognized the importance of computers in human life. Selecting a computer instead of a person was also a way to assign computers a new role. For the first time, it was considered by ordinary people as not only a tool for helping scientists and professionals in \hbox{number} crunching but a sort of ``intelligent'' machine that could assist humans in daily life tasks.

In the years in which personal computers began to enter homes, researchers and computer scientists were actively working in the innovative fields of artificial intelligence (AI), logic programming, and expert systems to design hardware and software systems able to mimic human intelligence. Not all the research efforts devoted to this goal were successful; however, they contributed to developing algorithms and models for advancing the machine intelligence concepts introduced by Alan Turing and John von Neumann in the late 1940s and early 1950s. Since 1983, computer science has undergone enormous development; computers have become faster and faster and their software systems increasingly sophisticated. The ways to model and develop digital intelligent systems have increased, and computers are now able to solve many problems better and faster than humans. This scenario has been made possible by the work of researchers and software designers who invented and implemented languages, tools, and hardware able to run procedures that can be considered intelligent, although they are not performed by humans but by machines (computers).

The concept of machine intelligence is used today to express the way computer systems interact with humans and the environment for solving problems that human beings typically solve using brainpower. Machine intelligence is based on techniques and algorithms created in the field of AI and in particular in the areas of data analysis, machine learning, and symbolic computing which do things that if done by humans would be considered intelligent. We may say that machine intelligence is advanced computing that through algorithms and scientific models enables a computer or a digital device to interact with its ecosystem intelligently. That is, it carries out adaptive actions that allow it to achieve its goals. For example, when a digital machine is equipped with software programmed to learn from input data, it is able to look for patterns, behaviors, and trends within the data it is given. Its software is coded for using those findings to elicit conclusions that can be similar to or better than conclusions drawn by humans. This process may increase the knowledge of machines that take those conclusions into account and may use them in similar future scenarios. Learning from data allows computer programs to become more efficient and more accurate in solving new analogous problems.

As an example of machine intelligence, Boston Dynamic's Atlas robot traverses an environment by avoiding obstacles and demonstrates human-level agility. Its advanced control systems enable highly diverse and agile locomotion, while learning algorithms reason through complex dynamic interactions involving its frame and the environment to plan movements. Atlas doesn't know what it might encounter, but it still performs impressively well without using predefined input data. Each time its machine intelligence algorithms are activated and encounter a totally new situation, they process the environmental data and make the needed movement decisions without any human interference. Atlas's algorithms are a clear example of automated instructions that result in an intelligent behavior based on machine learning techniques that use the data they receive for making decisions that are similar to those humans would make in a similar scenario.

During World War II, Alan Turing was working on decrypting projects for the Government Code and Cypher School at Bletchley Park only a few years after designing the abstract universal computing machine, known as the ``universal Turing machine.'' In 1941, he began to think computers could solve complex problems exploiting a sort of machine intelligence. In 1944, during discussions with his collaborator the engineer Donald Bayley, he spoke of his interest in building an ``electronic brain,'' and in a letter written two years later to the English pioneer in cybernetics, W. Ross Ashby, he remarked that ``In working on the ACE [the Automatic Computing Engine] I am more interested in the possibility of producing models of the action of the brain than in the practical applications to computing'' [\citealt{chap:4:Turing:1946}]. Ashby was working on scientific explanations for adaptive behavior, and to demonstrate his theories, he built a machine capable of adapting itself to the environment by exhibiting habituation, reinforcement, and learning. Ashby called the machine Homeostat. In 1946, two years before Ashby built the \hbox{Homeostat} machine, Turing had written a letter to Ashby suggesting the use of the ACE for his experiments instead of building a special machine.

In February 1947, Turing lectured on the ACE at the London Mathematical Society. This was the first public lecture where he mentioned the concept of ``machine intelligence'' [\citealt{chap:4:Turing:1995}]. After providing several details about the implementation of the ACE, Turing wrote in the final pages of his essay, ``I expect that digital computing machines will eventually stimulate a considerable interest in symbolic logic and mathematical philosophy. ... Actually, one could communicate with these machines in any language provided it was an exact language, i.e., in principle one should be able to communicate in any symbolic logic, provided that the machine were given instruction tables which would enable it to interpret that logical system.'' He was proposing a new role for computers that should go beyond complex mathematical calculations to enter the domain of adaptive behavior to solve problems that humans solve by using their intelligence. He continued ``It has been said that computing machines can only carry out the processes that they are instructed to do. ... Up till the present machines have only been used in this way. But is it necessary that they should always be used in such a manner?'' Then Turing gave an example that resembles the approach used in machine learning algorithms: ``Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations.'' And he concludes ``When this happens I feel that one is obliged to regard the machine as showing intelligence.'' This new form of intelligence for Alan Turing comes from the collection of data and the adaptation of algorithms to this data: ``What we want is a machine that can learn from experience.''

Turing's interest in machine intelligence is clearly expressed in his paper written in 1948 titled ``Intelligent machinery'' [\citealt{chap:4:Turing:1969}]. The first sentence stated, ``I propose to investigate the question as to whether it is possible for machinery to show intelligent behavior.'' He discusses in the paper some possible scientific approaches in which machines (more specifically computers) might be made to exhibit intelligent behavior. He introduces the concept of ``unorganized machines'' that can be seen as computing models of the cortex of an infant that can be organized by suitable training. This concept is a sort of very early proposal of machine learning that today is largely used for implementing many applications of AI, a term that at the time did not exist as it was only coined in 1956. Another interesting concept is that of a ``self-modifying machine'' that could modify its own instructions with the goal of improving its results: ``I have in mind a computing machine like the ACE where large parts of the storage are normally occupied in holding instruction tables. ... Whenever the content of this storage was altered by the internal operations of the machine, one would naturally speak of the machine {\textquoteleft}modifying itself.{\textquoteright}''

Most of the machine learning techniques used today are based on the adaptation of algorithms to the data they analyze. Indeed, most of the data analysis and machine learning algorithms used today improve their learning processes by adapting their computations to the features of the data they process. It is interesting to note that in his report Turing suggests five suitable application branches of thought on which to exercise an artificial ``thinking machine'': (i)~various games (chess, bridge, poker), (ii)~learning of languages, (iii)~translation of languages, (iv)~cryptography, and (v)~mathematics. In all of them, computers are able to show intelligent behavior, and in recent years some of them have achieved better results and faster responses than humans. Turing devoted the last paragraph of his paper to sketching a preliminary experiment about what he later called the ``imitation game,'' and others named the ``Turing test,'' which aims to determine how a computing machine can become indistinguishable from a human being. In the case of chess, the experiment he proposed works like this: Two players play in front of a screen in two separate rooms, one is a man and the other is a computer. If the human player at the end of the game cannot tell if the one on the other side is a machine or a man, then we may say the computer has passed the Turing test.

Turing carried out a further investigation on this theme in his seminal paper ``Computing machinery and intelligence'' published two years later in 1950 in the philosophy journal \textit{Mind} [\citealt{chap:4:Turing:1950}]. The author opens the paper with the words: ``I propose to consider the question, {\textquoteleft}Can machines think?{\textquoteright}'' To address this question, Turing describes the problem in terms of a three-person game (which he calls the ``imitation game''), in which an interrogator directs questions to a man (A) and a woman (B) staying in another room in order to determine the correct sex of the two players. Here the question is ``What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?'' These new questions replace the original one, ``Can machines think?'' Turing later summarizes them with this one: ``Are there imaginable digital computers which would do well in the imitation game?'' This question, Turing believed, was one that could actually be answered considering that ``when playing the `imitation game' the best strategy for the machine may possibly be something other than imitation of the behavior of a man.'' The practical project for research on thinking machines culminated in his ideas for ``learning machines'' that could be educated as ``child'' machines to make them aware of the context in which they operate and able to reply to questions (which today occurs with widely used applications such as the virtual assistants Alexa and Siri and generative AI chatbots such as ChatGPT and Bard).

In the remainder of his paper, Turing argued against all the major objections to the proposition that ``machines can think,'' and he titled the last section of the paper ``Learning machines.'' This heading is particularly meaningful as it can be seen as a very early proposal for a machine learning approach in programming intelligence in computers. Turing wrote ``... the problem is mainly one of programming. Advances in engineering will have to be made too, but it seems unlikely that these will not be adequate for the requirements. ... Our problem then is to find out how to programme these machines to play the game.'' In these words, and in the remainder of the section, Alan Turing made a major effort to envision a strategy based on programming computers in such a way that they may learn (concepts and facts) autonomously by leveraging input data. The term ``machine learning'' was coined a few years later, but this paper can be seen as the first one proposing that approach.

Among the nine presumed objections against his proposition, Turing also discussed the Lady Lovelace objection we mentioned in Chapter~\ExternalLink{chap:1}{1}: ``The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform.'' In this statement, Turing expressed complete agreement with \citet{chap:4:Hartree:1949} who wrote that Lovelace's opinion does not imply that it may not be possible to construct electronic equipment that will ``think for itself,'' but that it did not seem that the machines built or designed at the time had this property. Turing noticed that Hartree did not assert that the machines in question did not have the property but rather that the evidence available to Lady Lovelace did not encourage her to believe that they had it. In his paper, he clarified that ``The Analytical Engine was a universal digital computer, so that, if its storage capacity and speed were adequate, it could by suitable programming be made to mimic the machine in question. Probably this argument did not occur to the Countess or to Babbage. In any case there was no obligation on them to claim all that could be claimed.'' Turing with his \textit{Mind} paper gave rise to the field of philosophy of artificial intelligence. He continued to work on this subject, proposing a third version of the imitation game in 1952. In that new version a jury asks questions of a computer, and the role of the computer is to make a significant proportion of the jury believe that it is really a man. In the same year, Turing was prosecuted for homosexual acts. To avoid prison, he accepted a hormone treatment that was in effect chemical castration. He died in June 1954 in Wilmslow (Cheshire, UK). The established cause of death was cyanide poisoning. An investigation determined his death as a suicide, although accidental poisoning was not totally excluded. His untimely death did not allow him to continue his research, which would certainly have accelerated research in the field of AI.

\section{\label{sec:4.2}Birth and Evolution of Artificial Intelligence}

In 1956, two years after Turing's death, Dartmouth College in Hanover, NH, hosted the ``Dartmouth Summer Research Project on Artificial Intelligence,'' a workshop that is considered the founding event of the field of AI. Stretching over eight weeks, the workshop gathered 20 of the sharpest minds in computer science. John McCarthy and Marvin Minsky together with Claude Shannon and Nathaniel Rochester made a proposal to the Rockefeller Foundation to get their financial support for a two-month meeting of about ten researchers to study AI ``on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves'' [\hbox{\citealt{chap:4:McCarthyetal:1955}}].

The workshop proposal identified seven main aspects of the newborn field of AI. First, the proposers assumed that if a machine can do a job, then a digital computer can be programmed to simulate the machine, therefore new abilities must be developed to write programs that simulate the higher functions of the human brain. Second, as a large part of human thought consists of manipulating words according to the rules of reasoning and conjecture, the process of word sequences implication must be investigated. Third, there should be a study into how a set of neurons (they called them neuron nets) should be arranged so as to form a concept. On this problem, Minsky together with other colleagues (e.g., McCulloch and Pitts) had already obtained some preliminary results, but the proposers recognized that the problem needed more theoretical work. In particular, this aspect was very important for researchers working on the so-called connectionist AI. Connectionist approaches in the field of cognitive science aimed to explain rational phenomena by defining and using artificial neural networks (ANNs) for implementing AI processes and for building intelligent machines. The rationale of (artificial) neural networks is that intelligent systems can be obtained using a computational scheme somewhat similar to that of the human brain. As we will discuss later, an ANN is composed of a set of (artificial) neurons that communicate with each other through a set of connections that play the role of synapses. Like other machine learning models, an ANN learns through experience rather than through a program that specifies its behavior. This is the main difference over the symbolic approach of AI that is based on the manipulation of symbols through the use of rules that form the program to execute.

The fourth aspect listed in the workshop proposal called for the definition of a new theory of the size of a calculation on which some partial results were obtained by Shannon and McCarthy. To get a measure of the efficiency of a calculation, it is necessary to find a method of measuring the complexity of calculating devices that in turn can be designed to have a theory of the complexity of functions. After mentioning this aspect, the proposers focused on self-improvement activities that an intelligent machine must carry out, the fifth proposal. Abstract and practical studies on self-improvement were considered necessary. Self-improvement is the ability of an AI system to perform a task or a set of tasks during its training process. This idea has been exceptionally fruitful over time, and today many AI systems include self-improvement features. The last two scientific aspects listed in the proposal concerned the need to define a set of abstractions from sensory and other data, and with randomness and creativity. Concerning this latter aspect, the basic idea was to study the injection of randomness in ``artificial'' thinking processes and evaluate how it may influence unimaginative competent thinking toward creative thinking.

Today randomness is an important element in machine learning techniques, in particular in model training. It helps eliminate inherent biases and is beneficial for building generalized machine learning models. For example, the use of randomization based on the stochastic assignment (i.e., based on random probability) of a subset of weights to the connections between artificial neurons has been demonstrated to be a fruitful approach. This notion has been applied many times over the past 50 years, and it has resulted in important types of ANNs such as feedforward neural networks, recurrent networks, and random convolutional neural networks.

Each of the four proposers invited colleagues and researchers working in different areas of computing, mathematics, psychology, and related scientific fields. Twenty people attended the workshop over the eight-week period, some coming at different times. Future Nobel laureates such as John F. Nash Jr. and Herbert A. Simon attended the meeting and provided valuable contributions. In the report of the fifth and sixth weeks written by Trenchard More, it was noted that a work presented by Marvin Minsky was centered around ``A framework for an artificial intelligence'' aimed at constructing a problem-solving machine where no one of its blocks contain the intelligent part of the machine. Minsky was also interested in a machine that controls a human-like model of a simple physical environment. The machine should develop in its memory a model of the environment and, eventually, a model of its own structure for gaining a certain degree of apperception. More's notes demonstrate that in 1956 Minsky and his colleagues were thinking about intelligent and self-conscious machines. Among the various outcomes of the workshop was one that the researcher Arthur Samuel developed on IBM's 701 commercial computer. It is the so-called Samuel's Checkers-playing program, one of the world's first self-learning programs that demonstrated some of the concepts discussed during the Dartmouth workshop. In 1959 Samuel also coined the term ``machine learning.''

In the decades after the Dartmouth workshop, research activities in the new field of AI flourished and several AI models and systems were implemented, mainly in the USA and Europe. Most the of AI researchers worked on symbolic approaches for creating a machine with intelligence comparable to humans. They designed AI models and systems based on high-level symbolic representations of problems, logic, and search. Symbolic AI exploited human-readable formalisms such as logic programming, semantic nets, and production rules for developing the so-called expert systems. An expert system is a knowledge-based software system that emulates the decision-making ability of a human by reasoning through clauses of knowledge generally coded as \textit{if--then }rules. The expert systems are AI systems based on logical deduction that can draw conclusions from existing facts using various types of inference rules. For example, if we define the rule: ``If we are in the evening then the sun sets'' with the facts ``We are in the evening'' and ``The sun sets,'' deduction consists in asserting the fact ``The sun sets'' given the rule ``If we are in the evening then the sun sets'' and the fact ``We are in the evening.'' A lot of research projects on the implementation of expert systems were started in the 1960s and 1970s in Western countries and in Japan. However, the optimistic mood of AI researchers failed to recognize the difficulty of solving several AI tasks and the low performance of expert system implementations. Progress slowed, and around the mid-1970s Western governments cut off funds for AI research.

From the 1970s to the 1990s, the research field of AI was marked by so-called ``winters'' when obtaining funding for AI projects was difficult and several researchers and politicians became very skeptical about the promises of AI. The first AI winter occurred from 1974 to 1980. In the UK, the so-called Lighthill report authored by James Lighthill for the British Science Research Council and published in 1973 provided a negative evaluation of the state of AI at that time. The report affirmed that the promises of AI researchers were overstated, and its conclusion was trenchant: ``In no part of the field have discoveries made so far produced the major impact that was then promised.'' Although there was a lot of criticism of the Lighthill report at the time, after its publication the UK government cut funding for almost all research teams in the AI field, and it started a movement against AI funding throughout Europe with impact also on the USA where DARPA cut or cancelled new spending on AI research. In 1982, the Japanese Ministry of International Trade and Industry devoted \$850 million for the Fifth Generation Computer Systems initiative. The main objectives were to develop expert systems that could carry on conversations, translate languages, interpret pictures, and reason like human beings. After ten years, the remarkable list of expected goals had not been met; therefore, this project failure also contributed to discouraging new funding and research activities on AI. The second AI funding winter occurred from 1987 to 1994 when the limitations of the AI approaches based on expert systems and automatic reasoning became apparent.

After reaching its nadir in the early 1990s, interest, activities, and promising results in AI increased in the last years of the previous century, and particularly from the early 2000s, when attention on AI research and applications rose. The research field of machine learning, both from academic and private enterprise, led to a big increase in funding and investments. AI scientists and professionals progressively rebuilt the reputations of their theories and solutions by finding specialized solutions to specific problems. The narrow focus allowed researchers to produce limited but verifiable solutions. Together with simpler and more specific strategies for developing software systems able to perform tasks with intelligent behaviors, two other elements that contributed to generating new results were the availability of large sources of data stored on computers and shared through the Internet (the so-called Big Data) and the use of high-performance computing systems that exploited the power of many processors to run AI algorithms and applications in parallel. The combined use of efficient computing hardware with~the exploitation of large datasets in the first decades of the 21st century allowed the deployment of mathematical, statistical, and computer science methods for the implementation of new induction-based solutions in the fields of data mining (or data analysis) and machine learning. Old and new techniques were combined to implement successful solutions for challenging problems in computer vision, speech recognition, natural language translation and generation, digital healthcare, and business intelligence.

The basic goal of scientists who gathered at the Dartmouth workshop was to study and design systems with the ability to understand or learn any intellectual task that a human being can. This approach can be referred to as strong AI or artificial general intelligence (AGI). It aims to develop artificial intelligent systems that can have a large scope and be adaptable. Despite its influence on science fiction movies and novels,\footnote{Movies like \textit{Her} by Spike Jonze or \textit{2001: A Space Odyssey} by Stanley Kubrick and books like \textit{I, Robot} by Isaac Asimov or \textit{Accelerando} by Charles Stross are just a few examples of creative works inspired by AGI research.} AGI is still a remote possibility, although some researchers argue it can be reached in 20 or 30 years. On the contrary, the ``AI winter'' from the 1970s to the 1990s made it clear that AGI requires a longer endeavor than expected by AI researchers. In those years, funding agencies became suspicious of strong AI promises and limited or stopped funding projects that were too ambitious. This situation encouraged scholars to focus on more limited and realistic goals in designing intelligent systems. In the past 30 years, the so-called narrow AI (also called weak AI) received a lot of attention and achieved scientific recognition and business success by focusing on particular sub-problems (e.g., face recognition, natural language processing, or sentiment analysis) where they could generate limited but useful and demonstrable results.

Narrow AI defines a type of AI where learning procedures are designed to perform a single human task, and any knowledge obtained from carrying out that task or solving a problem will not automatically be applied to other different tasks. Some applications of narrow AI are mobile phone face recognition, digital personal assistants, autonomous car control systems, fraud detection, or market basket analysis applications. Often, narrow AI tasks use large-scale datasets as input sources that their learning algorithms process to extract patterns or behavior \hbox{models.} Having large amounts of data available allows research institutions and companies to use them for training narrow AI programs and obtaining useful insights. Returning to the definition of AI, narrow AI limits but does not contradict the basic definition of AI as ``the ability of a computer to perform tasks commonly associated with intelligent beings,'' and it does respect the basic goals of AI as the science of getting machines to process information and make decisions like human beings~do.

After the ``AI winter,'' narrow AI solutions slowly but surely repaired AI's reputation in the late 1990s by producing single solutions to specific problems. The limited focus allowed scientists to produce simpler but verifiable solutions that were widely used by thousands of companies or public administrations and by millions of people. Along this path, most researchers discarded symbolic approaches to implement AI systems and began to investigate sub-symbolic approaches to solve specific AI problems by exploiting available data from the problem domain. In contrast to symbolic AI paradigms that focus on the high-level and human-readable symbolic representation of problems relying on classical logic and reasoning, sub-symbolic AI approaches learn from experience by avoiding symbolic representation of rules and features. The main assumption of the sub-symbolic paradigm is the ability to extract an accurate model with limited experience. In contrast to symbolic systems, where learning comes from human supervision and explicit reasoning processes, sub-symbolic approaches find correlations between input and output data. Such associations are often expressed by mathematical functions that map the input to the target variables. Regression models, classification and clustering techniques, neural networks, Bayesian models, and support vector machines are only some of the most used sub-symbolic AI methods that make up the machine learning field. Different names are sometimes used to refer to these methods: data mining, pattern recognition, knowledge discovery, deep learning, and more. Each of these terms are used in different contexts and by different scientific or professional communities. Although there are (minor) differences among them---for example, data mining is mainly focused on finding patterns and trends to support learning processes, and deep learning refers to machine learning methods based on neural networks---we prefer to use the term machine learning to generally refer to all of them [see \citealt{chap:4:Domingos:2017}]. These methods were shown to be effective in implementing intelligent solutions to problems and tasks that in the past could only be solved or performed by humans. All the above-mentioned methods provide significant solutions to AI problems by taking advantage of the availability of very large amounts of data available in massive repositories or coming from billions of mobile devices and shared through the Internet.

\section{\label{sec:4.3}Learning from Data}

Through the use of computers, humans have generated an impressive amount of data that over time has become impossible for a human to understand and extract insights from. There are more than five billion Internet users in the world who are progressively consuming and creating data and information on web sites, social media, search engines, IoT devices, online entertainment, and news. Internet users generate thousands of terabytes of data traffic every minute, including a hundred million emails, millions of Google searches, and numerous other types of content [\citealt{chap:4:ChauhanandSood:2021}]. Although storing Big Data is a complex issue, a key challenge is analyzing and mining data to learn from them what may be useful for our goals. As we are often interested in the value of data, data discovery and learning methods are essential tools for finding it. In fact, we must consider that the very large data sources stored on computers cannot be read and understood by humans without the use of automatic procedures; that is, without the use of computers equipped with data analysis and machine learning software programs. The main approaches used today for discovery models, patterns, and trends in Big Data are provided by data analytics, data mining, and machine learning techniques. All those techniques are based on inductive processes that go from the individual to the universal.

Induction is a learning process where a learner discovers general rules by observing examples. Induction is making an inference of a generalized conclusion based on a set of observations, of facts. This means generating a generalization based on what is observed. For this reason, induction is a method of reasoning involving a factor of probability. As a very simple example, if we know that ``Meryl got pizza for lunch at Tony's Pizzeria''; ``Andrew got pizza for lunch at Tony's \hbox{Pizzeria'';} and ``Charlotte got pizza for lunch at Tony's Pizzeria,'' then we can infer that ``The pizza at Tony's Pizzeria must be good.'' In machine learning algorithms, the input data are analyzed to find hidden patterns or trends and learn the rules that produced the large set of data items. That is, the methods process large sets of data instances to induce a general rule (or hypothesis) from a set of observed instances. For example, in inductive learning methods, given a collection of sample input data \textit{d} and a set of output samples \textit{f}(\textit{d}), a learning procedure returns a function \textit{f'} (the hypothesis) that approximates the target function \textit{f}. The hypothesis produced is also called the ``concept description'' that is implemented by a program that can be used to classify subsequent instances. Applying a machine learning method to a set of data, a new program is then produced that adapts (learns) its behavior---that is, its sequence of instructions that corresponds to the program semantics---to the data it analyzed. Inductive learning, also called supervised learning, is one of the four main classes of machine learning strategies:

\begin{itemize}
\item \textit{Supervised or inductive learning}: Other than input data representing facts, training data include the corresponding outputs. That means that the learning process is supervised by a human being or by a program that provides the desired output for each input tuple.
\item \textit{Unsupervised learning}: Training data does not include correct or desired outputs. Then the learning algorithm must be able to find correlation, regularities, or patterns in the input datasets.
\item \textit{Semi-supervised learning}: Training data includes a small set of desired outputs together with a large number\vadjust{\vspace*{12pt}\pagebreak} of unlabeled outputs.
\item \textit{Reinforcement learning}: By means of a sequence of trial-and-error actions, a learning method receive rewards when it finds a right answer and figures out how to get them again when new input data are provided.
\end{itemize}

Learning based on supervision is obviously easier to implement and use than unsupervised learning, which works without guidance and needs to dig into data to find hidden correlations. However, we must note that supervised learning can only be used when training data output values are associated with input data. As mentioned previously, inductive learning is where we are given examples of a function in the form of input data (\textit{d}) and the output of the function (\textit{f}(\textit{d})), thus the inductive learning algorithm has to learn the function \textit{f} that will be used for new data (\textit{d'}). In supervised learning, as a child is taught by example, the machine program is taught by data. The analysis provides the learning algorithm with a known dataset that includes inputs and desired outputs, and the algorithm will find a model that defines the general way to associate those inputs with outputs. While the human being knows the single correct answer for each input data value or tuple, the learning algorithm identifies patterns in data that can be used to make predictions for future input data. The larger the datasets, the more accurate the learning model will be. The learning procedure exploits all the data items until it achieves a high level of accuracy. The learning process in supervised methods is composed of two main steps: training and testing. In the training step, a set of samples of training data containing for each data item the corresponding output values are taken as input. Data features are analyzed by the learning algorithm and used to build the learning model. In the testing step, the learned model (expressed as a classification/{\allowbreak}regression/{\allowbreak}estimation program) is used to make the prediction for the test data or production data, and prediction accuracy is assessed. Labeled data compose the output of the learning model that if accurate will be used for prediction on new data.

The three main strategies used in supervised/inductive learning are classification, regression, and forecasting. In classification tasks, the machine learning algorithm must draw a conclusion from observed values and determine what class the new observations belong to. For example, when classifying the clients of a store on the basis of their monthly or annual purchases as ``high spending'' or ``low \hbox{spending,''} the learning algorithm looks at new client data and classifies each new client accordingly. The same can be done with three or more classes. Like other supervised algorithms, classes are predefined; they are determined by a human expert as a finite set on the basis of existing information\vadjust{\vspace*{12pt}\pagebreak} or experience. In \hbox{practice} this means that an existing portion of input data (the so-called training set) is labeled with the class values. The task of a classification algorithm is to find a general way to build a mathematical model that, on the basis of the training dataset, will be able to correctly classify a test dataset and new datasets that will be analyzed when needed.

While a classifier maps input data into predefined classes that represent its output, a regressor maps input data into a continuous numerical domain. Regression is a statistical technique for studying the relationship between independent variables or features and a dependent variable that represents the outcome. In regression tasks, the machine learning algorithm must find or estimate the relationships among the input numeric variables. The goal of a regression algorithm is to calculate a best-fit line or a curve between the data. Simple, but significant, examples of regression tasks are predicting the price of gas given the production costs, market demand, and so on, or predicting the achievement of a marketing campaign on the basis of product prizes, campaign spending, and number of potential clients involved. Like other supervised techniques, a regression task to train a learning model requires labeled input and output data. In this way, learning regression models may understand the relationship between input features and outcome variables. The availability of correctly labeled training data is vital to obtaining an accurate regressor that will be used to predict outcomes from new datasets.

Different approaches for performing regression have been designed. They can be grouped into three subclasses: linear, multiple linear, and logistic regression. In linear regression, the relationship between the independent and dependent variables is assumed to be linear; therefore, this is a technique that plots a straight line within data points to minimize errors between that line and the data points. When more than one independent variable is used, multiple linear regression, such as polynomial regression, is used. The obtained model would be plotted on two dimensions as a curved line fitted to the data points. Finally, logistic regression can be used for binary classification. It is used when the dependent variable can have one of two values, such as high or low, true or false, win or lose. The goal is to model the probability of a random variable being 0 or 1 given experimental input variables. A sigmoid curve with values between 0 and 1 can be used to map the relationship between the independent variables and the dependent one (see Figure~\ref{fig:4.1}).


\begin{figure}[t!]
\tooltip{\includegraphics{graphics/Chapter_04/Figure1.\image}}{Representation of a sigmoid function with values ranging from 0 to 1 (values on the \textit{x}-axis range from $-$10.0 to 10.0).}[-240pt,3pt]
\caption{\label{fig:4.1}A sigmoid function that may result from logistic regression. The predicted values \textit{f}(\textit{x}) are probabilities and, therefore, are restricted in the range 0 to 1.}
\end{figure}

The predicted values \textit{f}(\textit{x}) are probabilities, therefore restricted to (0,1), of particular outcomes rather than the outcomes themselves. For example, a sigmoid curve can show the probability for a political candidate to win an election (binary dependent variable) versus the number of positive tweets on them posted during the electoral campaign (independent variable).

The last supervised learning method to discuss is forecasting, which is the process of making predictions about the future based on past and present data. It is a sort of time-based prediction that is more appropriate when dealing with time series data. Forecasting models are commonly used to analyze trends. Forecasting models may be generated by using time series (based on the assumption that history is a good indicator for the future), relational methods (that take account of past relationships between variables to generalize such a relationship into the future), ANNs, and other learning methods. They often use cross-sectional data, which are achieved by reiterated observations of the same variables. As forecasting models are used to estimate future data as a function of past data, they are appropriate for use when past data are available and when it is reasonable to assume that some of the patterns in the data are expected to continue into the future. Forecasting models can be applied to a wide range of problems where estimates of future conditions are useful. To estimate the error of forecasting models, residuals must be calculated. A residual is given by the difference between the real value and the forecast value for the corresponding time period $(R{}_{t} =V{}_{t} - F{}_{t})$, where $R{}_{t}, V{}_{t}$, and $F{}_{t}$ are the residual, the real value, and the forecast for period \textit{t}, respectively.

When labeled examples are not available, data analysis must use unsupervised learning algorithms. Methods in this class analyze data to identify homogeneous groups, clusters, or regular patterns. Unsupervised learning strategies do not benefit from a human operator who can provide suggestions, class labels, or instructions. Instead, the algorithm determines the correlations and relationships by analyzing the data features/attributes. For this reason, unsupervised learning processes are more time consuming and more complex than supervised strategies. The learning algorithm is left to interpret any potentially interesting correlation in large datasets and group those data accordingly. In summary, we can say that unsupervised algorithms attempt to arrange input data in some way to describe its basic structure. This might mean grouping the data into clusters or displaying them in a manner that shows some organized structure. Unsupervised learning methods include dimensionality reduction, clustering strategies, and association discovery. Dimensionality reduction methods are used when the number of data features or dimensions of the input dataset are too high. These approaches reduce the number of data attributes being considered for each data item to better discriminate among data. They eliminate or transform data inputs to a manageable size while also maintaining the integrity of the dataset. Dimensionality reduction algorithms include singular value decomposition, neural network-based autoencoders, and principal component analysis. These methods transform the original matrix of data items into a low-rank matrix that is more significant and can also be better visualized. Data clustering strategies involve grouping sets of unlabeled data based on similarity criteria. They are used for segmenting data into several groups on the basis of data item similarities and differences. Clustering algorithms are generally used to process raw data items by assigning them to different groups on the basis of hidden regularities that can be found by the learning algorithm. Some clustering algorithms are asked to group data in a given number of clusters while others will work to find different cluster sets or suggest the best number of clusters to split a given dataset. Well-known clustering algorithms are \textit{k}-means, hierarchical clustering and probabilistic strategies such as Gaussian mixture models, and expectation--maximization (EM).

Semi-supervised learning methods are in some ways similar to supervised learning methods, but they use both labeled and unlabeled data. Labeled data contain class information for each data item whilst unlabeled data lack that information. Therefore, the goal of this class of methods is learning to label unlabeled data. In several application domains and tasks, there is a scarcity of labeled data or the labels may be difficult to obtain because they require time-consuming human intervention or expensive trials. In these cases, semi-supervised learning becomes very useful. Regression methods using labeled and unlabeled data are part of these semi-supervised strategies, together with semi-supervised classification and constrained clustering. Constrained clustering is an extension of unsupervised clustering that aims to obtain better clustering than unsupervised clustering, which only uses unlabeled data. For this class of algorithms, the training data consists of unlabeled instances \textit{\{}$x{}_{i}$\textit{\}}, as in unsupervised clustering, plus some additional information that represents constraints about the data and/or the clusters. Constraints can be defined as the size of the clusters or the relationships among clusters. At the same time, constraints may state that data items $x{}_{i}$, $x{}_{j}$ cannot belong to the same cluster (the so-called cannot-link constraint) or that a subset of data items \textit{\{}$x{}_{k}$\textit{\}} must be in the same cluster. Semi-supervised classification is also known as classification with partially labeled data. The goal of this type of learning strategy is to train a classifier from both the labeled data containing the class tag for each input data item and the unlabeled data that miss the output class. Semi-supervised classification aims to enhance supervised classification by minimizing errors in the labeled data. It is based on the idea that including unlabeled data helps to obtain a better classifier than the supervised one trained only on the labeled data. This type of learning strategy is an extension of the supervised classification problem assuming that there is much more unlabeled data than labeled data. For example, in speech recognition, labeled audio is a very time-consuming task; semi-supervised learning can be used to speed up the process and provide better performance. The same approach can be used in web content annotation and classification.

The last class of machine learning methods is reinforcement learning, which works to find the best model by a trial-and-error cycle that explores different solutions and evaluates each result to determine which one is optimal [\citealt{chap:4:BartoandSutton:2014}]. Reinforcement learning methods are inspired by the idea that humans learn by interacting with the environment in which they live. They are different both from supervised learning (because they do not use labeled data samples provided by an instructor) and from unsupervised methods (because they try to maximize a reward rather than find hidden patterns in input data). Reinforcement learning programs learn from past answers and begin to adapt their strategy in response to the current state to achieve the best possible result, that is, to act so as to maximize a numerical reward value. The program employs trial and error to come up with a solution to the problem. It gets either rewards or penalties for the actions it performs with the goal of maximizing the total reward. Reinforcement learning systems basically include a set of environment states and agent states, a set of possible actions of the agent, the probability of transition at time \textit{t} from a state to a new state under a given action, and the expected reward after transition from a state to a new one achieved with a given action. An agent selects actions as a function of state according to a policy that is a stochastic rule based on probabilistic choices. The agent's objective is to maximize the amount of reward it receives over time.

An example of a reinforcement learning system is a robot that must choose whether it should continue its work collecting postal parcels in a warehouse or stop collecting and try to find its way back to the recharging station. The decision will be based on how far away a charger is and how easily it was able to find the charger in the past and the current charge level of its battery. Moreover, board games such as chess or the child's game of tic-tac-toe or games of strategy such as Go are good examples of reinforcement learning problems where the reward or punishment comes at the end like a win or a loss. Significant problems solved by reinforcement learning approaches are trajectory control and adaptation in self-driving cars, advertising recommendation systems, personalized chatbot answers, data center cooling, scheduling of space shuttle cargo loading, stock price prediction, dynamic treatment regimes in chronic diseases, and news recommendations.

After this long presentation of machine learning strategies, and before presenting a few details about some of the main algorithms used for Big Data analysis and machine learning, we can conclude that the machine learning approach used to make machines intelligent is getting computers to program themselves. Through the execution of learning algorithms on large quantities of data, new programs are generated semi-automatically. These programs are able to operate on new input data that partially share features with the training data used by the original machine learning program to generate them (as a new learning model). Therefore, machine learning can be considered as programming new software programs whose behavior is based on the training data and the learning algorithm. This new way of programming comes from data, and it generates (induces) codes that emulate human intelligence. It generates automatic knowledge that in scenarios such as autonomous cars, smart cameras, chess playing, and many more can be used as an acceptable alternative to human intelligence. Figure~\ref{fig:4.2} shows the main difference between imperative programming and machine learning programming that generates new programs instead of simple output results.


\begin{figure}[!b]
\tooltip{\includegraphics{graphics/Chapter_04/Figure2.\image}}{Scheme with two boxes, with one box representing a program labeled Imperative Programming and another box labeled Machine Learning. Input Data and a right-pointing arrow is on the left side of each box. The right side of each box has an arrow pointing, respectively, to Output and New Program.}[-270pt,-290pt]
\caption{\label{fig:4.2}In traditional programming paradigms, such as imperative programming, a program is executed on input data to produce output results that typically are numbers, texts, images, or other information. In machine learning, programs take input data (typically training and test data) and generate a new program that can be used to classify new data.}
\end{figure}

In the next sections, we will discuss a few significant algorithms used for machine learning. These will hopefully clarify the way algorithms learn from data and produce a model (expressed through a new program) that is able to discover association rules, identify hidden patterns, or find clusters or classes in large data repositories.

\section{\label{sec:4.4}Machine Learning Algorithms}

\noindent Since the birth of machine learning, a large variety of learning algorithms have been designed and implemented. Listing all of them is not possible in a single book; our goal here is not to give a complete catalogue but to discuss some of the most significant learning algorithms belonging to the four classes, discussed previously, that are used in most AI applications based on inductive processes. Among the supervised learning strategies, decision trees and \textit{k}-nearest neighbors are two classification algorithms that have been demonstrated to be efficient and effective in predicting the class of a given unlabeled data item. A classification algorithm can build a classifier that is a model \textit{M} that computes the class label $C_{i}$ for a given input item \textit{d} composed of a set of attributes, that is, $C{}_{i} = M(d)$, where $C{}_{i} \in \{C{}_{1}, C{}_{2}, \ldots , C{}_{n}\}$ assuming that data must be assigned to \textit{n} different classes. As discussed, to build a model the algorithm requires a training set, that is, a set of existing data items together with their correct class label. The model \textit{M} is implemented by a classifier program that can automatically predict the class for any new data item that is in the same domain as the training set that will be given to it as input.

\subsection{\label{sec:4.4.1}Decision Trees}

\noindent Decision
trees were introduced in Quinlan's ID3 system [\citealt{chap:4:Quinlan:1986}], in which a classifier is expressed by a tree structure where internal nodes are labeled with the names of attributes, the arcs originating from a node are labeled with the possible values of the corresponding attribute, and the leaves are labeled with the different classes. The paths from root to leaf represent classification rules. Therefore, a data item is classified by following a path along the tree formed by the arcs corresponding to the values of its attributes. Suppose we have a training set consisting of data items that describe a set of vehicles that are cars, motorcycles, and trucks. Each data item contains two attributes: the weight in tons and the number of wheels of a vehicle. In addition, each item contains an additional attribute that classifies it as belonging to the class \textit{car}, \textit{truck}, and \textit{motorcycle} on the basis of the values of the other attributes. The goal of classification is to build a simple tree that classifies all vehicles correctly. We could, for example, build the tree shown in Figure~\ref{fig:4.3}.

Quinlan developed C4.5 as a descendant of ID3 that is largely used for building decision trees. Given a set \textit{D} of data items, C4.5 first grows a decision tree using a test based on a single attribute with two or more outcomes. Make this test the root of the tree with one branch for each outcome of the test, then partition \textit{D} into a collection of subsets $D{}_{1}, D{}_{2}, \ldots , D{}_{n}$ according to the outcome for each item, and apply the same procedure recursively to each subset. Several tests could be used; for example, C4.5 uses two heuristic criteria to rank possible tests: information gain, which minimizes the total entropy of the subsets $\{D_{i}\}$, and the default gain ratio, which divides information gain by the information provided by the test outcomes. Attributes that compose a data item can either be numeric or nominal, and this defines the format of the test outcomes. For a numeric attribute \textit{A}, they are $\{A \leq t, A > t\}$, where the threshold \textit{t} is found by sorting the dataset \textit{D} on the values of \textit{A} and choosing the split between successive values that maximizes the criterion above. An attribute with discrete values has by default one outcome for each value, but an option allows the values to be grouped into two or more subsets with one outcome for each subset.


\begin{figure}[t!]
\tooltip{\includegraphics{graphics/Chapter_04/Figure3.\image}}{Scheme of a tree composed of a root node labeled Weight, three intermediate nodes labeled Wheels, and six leaves labeled Truck (three of them), Car (two of them), and Motorcycle.}[-240pt,3pt]
\caption{\label{fig:4.3}A decision tree for classifying vehicles in four different classes on the basis of two attributes (weight and number of wheels).}
\end{figure}

An advantage in using decision trees for data classification is related to model explainability. Although in some real-world applications decision trees can be too big and deep, a decision tree is a machine learning model that is easily explained by visualizing the tree. This is a benefit for end users, and it makes decision trees more acceptable than deep learning models, which are ``black boxes'' that can be difficult to explain. Moreover, there are available algorithms to generate production rules from decision trees. Production rules are easily interpreted by users because of their high modularity. Each rule can be understood without reference to other rules. The classification accuracy of a decision tree can be improved by transforming the tree into a set of production rules that contain a smaller number of attribute-value conditions. Sometimes there are conditions that may be redundant, and they can be removed to reduce the rate of misclassification on the examples that are not part of the training set. The set of classification rules for the decision tree shown in Figure~\ref{fig:4.3} is as follows:

\medskip
\begin{tabular}{P{\textwidth}}
 \textbf{\texttt{\MonoBold if}} \texttt{Weight > 15} \textbf{\texttt{\MonoBold and}} \texttt{Wheels = 6 \textbf{\MonoBold or} Wheels = 4 \textbf{\MonoBold then}} \texttt{class :}\texttt{= Truck}\\
\textbf{\texttt{\MonoBold if}} \texttt{Weight < 15 \textbf{\MonoBold and} Weight > 2 \textbf{\MonoBold and} Wheels = 6 \textbf{\MonoBold then}} \texttt{class :}\texttt{= Truck}\\
\textbf{\texttt{\MonoBold if}} \texttt{Weight < 15 \textbf{\MonoBold and} Weight > 2 \textbf{\MonoBold and} Wheels = 4 \textbf{\MonoBold then}} \texttt{class :}\texttt{= Car}\\
\textbf{\texttt{\MonoBold if}} \texttt{Weight < 2 \textbf{\MonoBold and} Wheels = 4 \textbf{\MonoBold then}} \texttt{class :}\texttt{= Car}\\
\textbf{\texttt{\MonoBold if}} \texttt{Weight < 2 \textbf{\MonoBold and} Wheels < 4 \textbf{\MonoBold then}} \texttt{class :}\texttt{= Motorcycle}
\end{tabular}



\subsection{\label{sec:4.4.2}\textit{k}-nearest Neighbors}

\noindent Another effective learning algorithm for data classification is \textit{k-nearest neighbors} classification [\citealt{chap:4:FixandHodges:1951}], which finds a group of \textit{k} data items in the training set that are closest to the test item and bases the assignment of a label on the predominance of a particular class in this neighborhood. This algorithm has three main elements: a set of labeled data items, a distance or similarity metric to calculate the distance among items, and the value of \textit{k}, that is, the number of nearest neighbors. To classify an unlabeled data item, the distance of this item to the labeled items is calculated, its \textit{k}-nearest neighbors are identified, and then the class labels of these nearest neighbors are used to select the class label of the item. The nearest-neighbor classification algorithm operates as follows. Given a training set \textit{T} and a test item $i=(d,C)$, where \textit{d} is the data attributes of the test item and \textit{C} is the item class, the algorithm computes the similarity between \textit{i} and all the training items $(d{}_{i}, C{}_{i}) \in T$ to determine its nearest-neighbor list $T{}_{i}$. Once the nearest-neighbor list is obtained, the test item is classified based on the majority class of its nearest neighbors.

\subsection{\label{sec:4.4.3}Clustering Algorithms}

\noindent When predefined classes, to which items of a dataset must be assigned, are not available, classification algorithms cannot be used to generate a model for the data. In these cases, clustering-based approaches can be applied. As mentioned, clustering is an unsupervised learning technique that separates data items into a number of groups or clusters such that data items in the same cluster are more similar to each other and data items in different clusters tend to be dissimilar, according to some measure of similarity or proximity. In contrast to supervised learning, where training examples are associated with a class label that expresses the membership of every example to a class, clustering assumes no information about the distribution of input data, and it has the task of both discovering the classes present in the dataset and assigning the items to the classes in the best way possible. One of the most used clustering algorithms is \textit{k}-means, which implements a partitional strategy to find homogeneous groups of data. \textit{k}-means is an iterative clustering algorithm to partition a given dataset into a user-specified number of clusters \textit{k}. It minimizes the squared error of values from their respective cluster means. In this way, \textit{k}-means implements hard clustering, where each item is assigned to only one cluster [\citealt{chap:4:KaufmanandRousseeuw:1990}]. The algorithm works on a set of data items having \textit{d} attributes, and it starts by picking \textit{k} points in a \textit{d}-dimensional space as the initial representatives or ``centroids'' of the \textit{k} clusters. These initial seeds can be selected by sampling at random from the dataset or setting them as the solution of clustering a small subset of the data or perturbing the global mean of the data \textit{k} times. When the centroids are generated, each data item is assigned to its nearest centroid (whose mean yields the least within-cluster sum of squares), with ties broken arbitrarily. This results in a partitioning of the dataset in \textit{k} groups or clusters. The new means of the centroids of the observations in the new clusters are calculated. Each cluster representative is relocated to the center (mean) of all data items assigned to it. If the data items have an associated probability measure (weights), then the relocation is to the expectations (weighted mean) of the data partitions. These two steps are iterated until the assignments (and the centroid values) no longer change.

While \textit{k}-means assigns each data item to only one cluster, Bayesian algorithms, like expectation-maximization, implement a soft clustering approach that returns the probability a data item belongs to each cluster. The Bayesian approach to unsupervised learning provides a probabilistic method to inductive inference. Therefore, class membership is expressed probabilistically, that is, an item is not assigned to a unique class, it instead has a probability of belonging to each of the possible classes. The classes provide probabilities for all attribute values of each item. Class membership probabilities are then defined by combining all these probabilities. Class membership probabilities of each item must sum to 1; thus there are no precise boundaries for classes: every item must be a member of some class, even though we do not know which one. When every item has a probability of no more than 0.5 in any class, the classification is not well defined because it means that classes are abundantly overlapped. Conversely, when the probability of each instance is about 0.99 in its most probable class, the classes are well separated. Bayesian clustering aims to determine the best class description (hypothesis) \textit{h} from some space \textit{H} that predicts data \textit{D}. The term ``best'' can be interpreted as the most probable hypothesis given the observed data \textit{D} and some prior knowledge on the hypotheses of \textit{H} in the absence of \textit{D}, that is, the prior probabilities of the various hypotheses in \textit{H} when no data have been observed. Bayes' theorem provides a way to compute the probabilities of the best hypothesis, given the prior probabilities, the probabilities of observing the data given the various hypotheses, and the observed data. Let $P(h)$ denote the \textit{prior probability} that the hypothesis \textit{h} holds before the data have been observed. Analogously, let $P(D)$ denote the prior probability that the data will be observed, that is, the probability of \textit{D} with no knowledge of which hypothesis holds. $P(D|h)$ denotes the probability of observing \textit{D} in some world where the hypothesis \textit{h} is valid. In Bayesian clustering, the main problem is finding the probability $P(h|D)$, that is, the probability that the hypothesis \textit{h} is valid, given the observed data \textit{D}. $P(h|D)$ is called the \textit{posterior probability} of \textit{h} and expresses the degree of belief in \textit{h} after the data have been seen. Thus, the set of data items biases the posterior probability, while the prior probability is independent of \textit{D}. Bayes' theorem provides a method to compute the posterior probability:
\begin{equation*}
P(h|D)=\frac{P(D|h)P(h)}{P(D)}
\end{equation*}

We can say that this theorem measures the probability of validity of a cause (\textit{h}) given a set of data that describe an effect (\textit{D}). This probability, which is very useful in learning a model analyzing a set of data (the larger the data, the more accurate the hypothesis), is proportional to the probability of observing the data when the hypothesis is valid times the probability of the hypothesis/cause. The value of this multiplication must be divided by the probability that the data are observed without assuming any hypothesis. The simplest version of learning algorithms based on the Bayes theorem is Na\"{\i}ve Bayes, which simplifies probability calculations by assuming that the attributes of data items are independent. Although in general they are not really independent, this simple approach allows the implementation of machine learning applications that in many cases work well and may also get results in reduced time on high-dimensional datasets. Na\"{\i}ve Bayes learning is used in spam filtering, fraud detection, biological and medical analysis, supply chain stock management, and financial predictions.

\subsection{\label{sec:4.4.4}Association Rules}

In many application domains, it is useful to discover how often two or more events co-occur. This holds, for example, when we wish to know what goods customers buy together or which pages of a web site users access in the same session. Learning frequent patterns is the basic task in all these cases. Once the frequent groups of items are found in a dataset, it is also possible to discover the association rules that hold for the frequent sets of items (\textit{itemsets} for short). Association rule discovery has been proposed by \mbox{\citeauthor{chap:4:Agrawaletal:1993}} [\mbox{\citeyear{chap:4:Agrawaletal:1993}}] as a method for discovering interesting associations among variables in large datasets.

The problem of association rule mining is defined as follows: Let $I = \{i{}_{1},\break i{}_{2}, \dots , i{}_{n}\}$ be a set of \textit{n} binary attributes called \textit{items}. Let $D = \{t{}_{1}, t{}_{2}, \dots , t{}_{m}\}$ be a set of transactions called the \textit{dataset}. Each transaction in \textit{D} has a unique transaction \textit{id} and contains a subset of the items in \textit{I}. A \textit{rule} is defined as an implication of the form $X \Rightarrow Y$, where $X, Y \subseteq I$ and $X \cap Y = \text{\O}$. The rule specifies that if \textit{X }is true, then \textit{Y} will also be true. The expression $X, Y \subseteq I$ means that \textit{X} and \textit{Y} are subsets of set of items \textit{I}, and $X \cap Y = \text{\O}$ means that the intersection of the two sets, \textit{X }and \textit{Y}, that is, the set of elements that are common to both \textit{X }and \textit{Y}, is empty (the two sets \textit{X} and \textit{Y} are disjoint). The \textit{itemsets} \textit{X} and \textit{Y} are called \textit{antecedent} and \textit{consequent} of the rule, respectively. To illustrate the concepts, we use a very simple example from the supermarket domain. An example rule for the supermarket could be (\textit{pasta}, \textit{tomatoes}) $\Rightarrow$ (\textit{olive oil}), meaning that if customers buy pasta and tomatoes, they also buy olive oil.

In addition to the antecedent and the consequent, an association rule has two numbers that express the degree of uncertainty about the rule. In association rule learning, the antecedent and consequent are sets of data items that are disjoint. The first number is called the \textit{support} for the rule. The support is the number of transactions that include all items in the antecedent and consequent parts of the rule. The support can be expressed as a percentage of the total number of records in the dataset. For example, if an itemset \{\textit{pasta}, \textit{tomatoes}, \textit{olive oil}\} occurs in 20\% of all transactions (one out of five transactions), it has a support of 1/5 = 0.2. The second number is known as the confidence of the rule. Confidence is the ratio of the number of transactions that include all items in the consequent as well as the antecedent (the support) to the number of transactions that include all items in the antecedent: \textit{confidence}$(X \Rightarrow Y)$ $= support(X \cup Y)/support(X)$. For example, if a supermarket database has 200,000 transactions, out of which 4,000 include both items \textit{X} and \textit{Y} and 1,000 of these include item \textit{Z}, the association rule ``If \textit{X} and \textit{Y} are purchased, then \textit{Z} is purchased at the same time'' has a support of 1,000 transactions (alternatively 0.2\% = 1,000 / 200,000) and a confidence of 25\% (= 1,000 / 4,000).

Association rules are usually required to satisfy a user-specified \textit{minimum support} and a user-specified \textit{minimum confidence} at the same time. Association rule generation is usually split up into two separate steps. First, minimum support is applied to find all \textit{frequent itemsets} in a dataset. Then these \textit{frequent itemsets} and the minimum confidence constraint are used to form rules. While the second step is straightforward, finding all frequent itemsets in a dataset is difficult as it involves searching all possible itemsets (item combinations). The set of possible itemsets is the power set over \textit{I} and has size $2{}^{n-1}$ (excluding the empty set which is not a valid itemset). Although the size of the power set grows exponentially in the number of items \textit{n} in \textit{I}, an efficient search is possible using the \textit{downward-closure property} of support (also called \textit{anti-monotonicity}), which guarantees that for a frequent \hbox{itemset,} all its subsets are also frequent, therefore for an infrequent itemset, all its supersets must also be infrequent. Exploiting this property, efficient algorithms such as Apriori and Eclat are used to find all frequent itemsets. For example, the Apriori algorithm uses a ``bottom-up'' approach that extends frequent subsets one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found.

\section{\label{sec:4.5}Neural Networks and Deep Learning Models}

ANNs (usually called neural networks) have a structure and a computational model (the so-called connectionist model) that is very different from the learning models discussed above. The idea behind neural networks is that intelligent systems can be obtained using a computational scheme somewhat similar to that of the human brain. To emulate the human brain, neural networks are composed of a large number of (artificial) neurons that communicate with each other through a set of connections that play the role of synapses. It should, however, be noted that neural networks represent only a very simplified model of the human brain and do not take into account the complex neural mechanisms. In neural networks, unlike almost all computational models, there is not a program that specifies the operations to be executed; the computation is defined through the characteristics of the processing units (artificial neurons) and their interconnections [\citealt{chap:4:FogelmanSoulie:1991}]. The main features of a neural network are a high number of simple processing units (artificial neurons), a high number of connections between neurons (synapses), a parallel and distributed control scheme, and a learning algorithm. The goal of a neural network is to associate an output \textit{y} to a set of input signals $\{x{}_{1}, x{}_{2}, \ldots, x_{n}\}$. In neural networks, there is no global controller of the computation state. Each artificial neuron computes its state \textit{y} on the basis of local information, that is, it evolves according to its own law determined by a transition function \textit{f} that specifies the dependence of state \textit{y} from the signals received through the connections. The set of states of all the neurons constitutes the global network state. In general, \textit{f} is a non-linear function of the linear combination of the inputs through some coefficients that are given by the weights $(p{}_{ij})$ associated with the neuron connections:
\begin{equation*}
y_{i} =f\left(\sum_{j}p_{ij} x_{j} \right).
\end{equation*}
Input neurons get activated through external values; other neurons get activated through weighted connections from previously active neurons. Learning in neural networks is about finding weights that make the network exhibit a desired behavior such as recognizing a cat in a photo or driving an autonomous car. Many learning algorithms reinforce the connections between elements that have the same values of activation in the coherent state to be stored. The learning rules specify an initial set of weights associated with the connections and indicate how these weights should be adjusted to improve network performance. Learning can take place, at least in part, by providing to the neural network a number of examples that constitute the training set. Only after the network has learned to respond correctly to all the inputs in the training set is it ready to be used to solve the problems for which it was designed.

The neural network models proposed so far belong to four main classes. \textit{Auto-associators} associate the stored patterns to themselves. For example, if a pattern with ``noise'' or partial information is provided, the network returns the complete pattern. \textit{Pair-associators} associate the stored patterns to other patterns. Provided with an image of a human face, the network may return another image of the same person. In \textit{supervised learners} the instructor gives the network the correct response (output) for each input pattern. According to the difference between the output calculated by the network and the correct response, the network varies the weights of the connections in order to produce an output as similar as possible to the correct responses provided by the instructor. Finally, in \textit{unsupervised learners} a network must learn on its own to classify the input patterns. Unsupervised networks classify input data into a set of internal categories or clusters based on the relationships that the network finds between data. These neural networks are used when it is not known exactly how to classify the available data. Since the first studies by McCulloch and Pitts, many models of neural networks have been proposed. Some of the most well-known models are the Hopfield networks, the perceptron, multilayer networks, the Boltzmann machine, self-organizing maps, and adaptive resonance networks.

As neural networks possess the ability to recognize patterns from sets of input data, they can perform a series of typical data mining and machine learning operations such as classification, clustering, association of information, and analysis of relationships between data. As an example, we briefly describe the way in which a multilayer neural network allows the achievement of a classification function. This neural network model is composed of three or more layers of neurons such that the output of a neuron in one layer is connected with the input of all neurons in\break the next layer (see Figure~\ref{fig:4.4}). The output of the last layer represents the result of the network. A multi-layer network is suitable for the implementation of a classification function that assigns an object (data) defined by an input vector $[x{}_{1}, x{}_{2},\ldots,x_{n}]$ to one or more classes $C{}_{1}, C{}_{2}, \ldots,C{}_{m}$ that are the output neurons of the network. If the object belongs to class $C{}_{i}$, the output neuron $C{}_{i}$ has a value of 1; \hbox{otherwise,} if the object does not belong to class $C{}_{i}$, the output neuron $C{}_{i}$ has a value of~0. Figure~\ref{fig:4.4} shows an example of a neural network with $n = 5$ and $m = 4$. Thus, by selecting an appropriate set of weights $p^{k}{}_{ij}$ for all the connections of the neural network, it can implement a set of classification tasks. The correct choice of weights can be made by providing to the network a number of examples that are represented by input values and the corresponding output values (supervised learning). Using this approach, an example is a vector of input values with the desired output value, where the set of $C{}_{i}$ has the values 1 or 0 depending on whether the input belongs or not to class \textit{i}. The network will adjust the weights of connections based on the examples given, and at the end of the learning procedure it will be ready to classify new objects, different from those of the training set.


\begin{figure}[!t]
\tooltip{\includegraphics{graphics/Chapter_04/Figure4.\image}}{Scheme of a neural network composed of three layers: the first layer includes five nodes that are connected to the three nodes of the intermediate layer; the third layer is composed of four nodes connected to the nodes of the intermediate layer.}[-260pt,3pt]
\caption{\label{fig:4.4}An example of a multilayer neural network that can be used for data classification.}
\end{figure}

While the neural network concept was originally proposed in 1943, the deep learning model is more recent. It was introduced by Rina Dechter in 1986, and it implements an extension of neural networks. Indeed, a neural network with many internal (hidden) layers and multiple nodes in each hidden layer is known as a deep neural network. The term ``deep'' refers to the number of internal layers, that is, the depth of the neural network. Basically, we can say that every neural network with more than three layers can be considered a deep learning system. Due to the presence of multiple layers, deep learning networks improve the accuracy of learning tasks although they may require larger input datasets in comparison to shallow neural networks. Moreover, a shallow neural network requires less time to complete the training step as it is less complex, while plenty of time may be required for training a deep learning network. Deep learning networks can be composed of tens of layers, and one of the largest deep networks is used in the Generative Pre-trained Transformer version 4 (GPT-4) system, which employs 1.7 trillion parameters to produce human-like text. GPT-4 uses a language model that by sequence transduction can predict the likelihood of an output sequence given an input sequence. This is used to generate sentences by predicting which word makes the most sense in a given text sequence. Through the GPT-4 system, complex texts and documents can be produced that are similar to texts produced by human experts. Deep learning is largely used in many other tasks such as image recognition, news aggregation, new drug discovery and disease prediction, fraud  detection, and virtual assistants.

All the machine learning models and algorithms we discussed here are very useful tools for implementing narrow AI, but they are not sufficient for implementing general AI systems that may express similar intelligent behavior to that of humans. Machine learning and deep learning techniques must be accompanied by other techniques and must include common sense to reach AGI. However, despite these limitations, the machine learning technologies that are available  today are sufficiently clever to help us in many tasks while in others they are able to replace us with excellent results. For this reason, many AI algorithms ``live'' and ``work'' with us every day in almost all areas of human life.


\begin{thebibliography}{}

\bibitem[Agrawal et~al.(1993)]{chap:4:Agrawaletal:1993} R. Agrawal, T. Imieli\~{n}ski, and A. Swami. June 1. 1993. Mining association rules between sets of items in large databases. \textit{ACM} \textit{SIGMOD Rec}. 22, 2, 207--216. DOI:~\href{https://doi.org/10.1145/170036.170072}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1145/{\allowbreak}170036.{\allowbreak}170072}.

\bibitem[Barto and Sutton(2014)]{chap:4:BartoandSutton:2014} R. S. Barto and A. G. Sutton. 2014. \textit{Reinforcement Learning: An Introduction}. MIT Press.

\bibitem[Chauhan and Sood(2021)]{chap:4:ChauhanandSood:2021} P. Chauhan and M. Sood. April. 2021. Big data: Present and future. \textit{Computer} 54, 4, 59--65. DOI:~\href{https://doi.org/10.1109/MC.2021.3057442}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1109/{\allowbreak}MC.{\allowbreak}2021.{\allowbreak}3057442}.

\bibitem[Domingos(2017)]{chap:4:Domingos:2017} P. Domingos. 2017. \textit{The Master Algorithm}. Penguin Books.

\bibitem[Fix and Hodges Jr(1951)]{chap:4:FixandHodges:1951} E. Fix and J. L. Hodges Jr. 1951. \textit{Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties}. Report Number 4, Project Number 21-49-004. USAF School of Aviation Medicine, Randolph Field, TX.

\bibitem[Fogelman Souli\'{e}(1991)]{chap:4:FogelmanSoulie:1991} F. Fogelman Souli\'{e}. 1991. Neural networks and computing. \textit{Future Gener. Comput. Syst.} 7, 1, 69--77. DOI:~\href{https://doi.org/10.1016/0167-739X(91)90017-R}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1016/{\allowbreak}0167-{\allowbreak}739X{\allowbreak}(91){\allowbreak}90017-R}.

\bibitem[Hartree(1949)]{chap:4:Hartree:1949} D. R. Hartree. 1949. \textit{Calculating Instruments and Machines}. University of Illinois Press, Urbana, pp. 138.

\bibitem[Kaufman and Rousseeuw(1990)]{chap:4:KaufmanandRousseeuw:1990} L. Kaufman and P. J. Rousseeuw. 1990. \textit{Finding Groups in Data: An Introduction to Cluster Analysis}. John Wiley \& Sons, New York. DOI:~\href{https://doi.org/10.1002/9780470316801}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1002/{\allowbreak}9780470316801}.

\bibitem[McCarthy et~al.(1955)]{chap:4:McCarthyetal:1955} J. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shan. August 31. 1955. A proposal for the Dartmouth summer research projects on artificial intelligence. [Online]. Retrieved October 2, 2022 from \href{https://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html}{https://{\allowbreak}www-{\allowbreak}formal.{\allowbreak}stanford.{\allowbreak}edu/{\allowbreak}jmc/{\allowbreak}history/{\allowbreak}dartmouth/{\allowbreak}dartmouth.{\allowbreak}html}.

\bibitem[Quinlan(1986)]{chap:4:Quinlan:1986} J. R. Quinlan. March. 1986. Induction of decision trees. \textit{Mach. Learn.} 1, 1, 81--106. DOI:~\href{https://doi.org/10.1007/BF00116251}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1007/{\allowbreak}BF00116251}.

\bibitem[Turing(1946)]{chap:4:Turing:1946} A. M. Turing. November. 1946. Letter to W. Ross Ashby of 19 November 1946. [Online]. Retrieved October 2, 2022 from \href{https://www.rossashby.info/letters/turing.html}{https://{\allowbreak}www.{\allowbreak}rossashby.{\allowbreak}info/{\allowbreak}letters/{\allowbreak}turing.{\allowbreak}html}.

\bibitem[Turing(1950)]{chap:4:Turing:1950} A. M. Turing. October. 1950. Computing machinery and intelligence. \textit{Mind} LIX, 236, 433--460. DOI:~\href{https://doi.org/10.1093/mind/LIX.236.433}{https://{\allowbreak}doi.{\allowbreak}org/{\allowbreak}10.{\allowbreak}1093/{\allowbreak}mind/{\allowbreak}LIX.{\allowbreak}236.433}.

\bibitem[Turing(1969)]{chap:4:Turing:1969} A. M. Turing. 1969. Intelligent machinery, report for National Physical Laboratory. In \textit{Machine Intelligence 5}. Edinburgh University Press, Edinburgh.

\bibitem[Turing(1995)]{chap:4:Turing:1995} A. M. Turing. 1995. Lecture to the London Mathematical Society on 20 February 1947. \textit{MD Comput. }12, 390--397.

\end{thebibliography}

%\end{document}

